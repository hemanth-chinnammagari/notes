***********************************************************************************************************************************************************

CKA : CERTIFIED KUBERNETES ADMINISTRATOR -

***********************************************************************************************************************************************************

CKA Domains in official Blueprint exam : 

1. Cluster Architecture, Installation & Configuration : 25%
2. Workloads & Scheduling : 15%
3. Services & Networking : 20%
4. Storage: 10%
5. Troubleshooting : 30%

* CKA is practical oriented exam  . in exam we will be given various scenarios related to troubleshooting which we have to practically implement/ fix issue.

GITHUB LINKS : 

- https://github.com/cncf/curriculum                                     for Official CKA Exam Blueprint

- https://github.com/zealvora/certified-kubernetes-administrator         for Zeal's CKA GitHub Repository Link

Zeal's modified domain structure : 
- Added " Core Concepts, Security, Exam "

***********************************************************************************************************************************************************

1.CORE CONCEPTS

***********************************************************************************************************************************************************

OVERVIEW OF CONTAINER ORCHESTRATION(COS): 
- Container orchestration is all about managing the life cycles of containers, especially in large,dynamic environments
Demo : i have 3 virtual machines.i have webserver container and app server container in each vm1 and vm2.if we have small env like this we can go and start the container manually. but there are other aspects apart from starting the container. one among them is monitoring. let's say the webserver container in 

VM1 went down, in order for prod env to work perfectly we  need to have a min of 2 webserver containers to work always. what happen is i installed webserver and it went down.who will monitor this web server container?.and also monitoring script should have event driven mechanism in such a way if webser goes down it also restart the container or due to some reason the container is not restarting then the script should automatically start a web container in another VM3. this can be acheived with the help of Container Orchestration.

Container Orchestration can be used to perform a lot of tasks, some of them includes:

- Provisioning and deployment of containers among VM's. let's say i have 100 VM's and i want to deploy containers across them it is diff manually, container 
   orchestration can easily do that
- Scaling up or removing containers to spread application load evenly
- Movement of containers from one host to another if there is a shortage of resources  EX : vm1 has 2 containers.but vm1 has short resource. there is only like 50mb of ram which is available.container orchestration tool can move the container from VM1 to vm which is much more empty.
- Load balancing of service discovery between containers
- Health monitoring of containers and hosts

whenwe have COS there can also be lot of mis config's that can happen.let's say that i defined a req in cos which states that a min of 2 web containers should be running all the time.now cos can launch the 2 web containers in the same Vm although we have 3 vm's, it launches web's in one vm.here both in a same vm,1st there will be a resource issue if the prod traffic increases.2nd this is not highly available.the vm1 itself goes down then both the containers goes down.
so here we need to have a good set of req's and the cos should be intelligent in such a way that if the cos tool has a defn that the 2 webservers should be running all the time it should be intelligent to split the webserver containers across the vm's so that the load is balanced and HA would be taken care of

 ex : ecs is a orchestration tool specifically for aws. it has a feature of task placement. tasks use various constraints like az or instance type to split containers across vm's.
in AWS : we have vm1 and vm2 in same AZ.in sduch case we have web contaienr1 in vm1 and web container2 in vm2 although it might be good interms of load balencing but interms of fault tolerance it is not good.both of this im same az.we need inteligence in such a way that we server container split across the instances in a diff AZ's.if a single az goes down that should not effect ur appl.this is great capability of ecs

There are many container orchestration solutions which are available, some of the popular ones include:
- Docker Swarm
- Kubernetes
- Apache Mesos
- Elastic Container Service (AWS ECS)
There are also various container orchestration platforms available like EKS.aws takes care of managing the k8s and we take care of the rest.

DEMO : in cli :
- run " docker service ls" : i can see 1 service helloworld which is using nginx image and has a replica of 1/1. currently the mode is replicated.
- run "docker service ps helloworld" it basically shoiwing earlier it got launched in a node or vm called as swarm02 but due to some reason this VM stopped responding.and then what OS(orchestration) tool did : it started this specific container in diff vm named swarm03.it has the capability to monitor whether the appl or container is running or not.and if it is not running it will automatically started in a diff vm which is available.
- earlier we discussing : a webserver container stopped working in a vm1 then the os tool will automatically starts the web containr in diff vm.

demo : login to swarm03 container and manually stop the container which is running in swarm03 and will look into whether the container tool (swarm) whether it is able to detect this specific container has stopped working and whether it launches a new container yet again.:
- connect swarm03 where my service is running we can see status up for 3omin. now stop the docker daemon : " systemctl stop docker " . this is the worst case.it might happen that the server itself has stopped working and it has shutdown.since we stopped docker service the appl would have gone down.

- now run " docker service ps helloworld" : now after it detected the container within the swarm03 is not running what it does was it started the container in swarm01 node automatically. if i docker ps i can see docker container is running from nginx image(service)
-COS tool can automatically detect whether the container is up or not , if it is not up the it can either restart it or it can start the specific container in the diff node alltogether.

***********************************************************************************************************************************************************

Introduction to Kubernetes : 
- we have docker swarm, apache mesos, k8s and other orchestration tools. By far Kubernetes is the most popular container orchestration engine.
- Kubernetes (K8s) is an open-source container orchestration engine developed by Google.
- It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.

ARCHITECTURE OF K8S: 
- we have k8s master and worker nodes(physical servers or VM's where the containers would be running). k8s master is the one who will be managing the container. K8S master will basically recieve the commands from the users. how will it recieve commands (it can recieve commands via API,CLI,GUI : just like we run commands in docker swarm master node to create services). then k8s master will process the things which are mentioned within the commands.

EX : i have command "i want to launch app A " we will give this command to k8s master . now k8s master will check how many nodes it has.there are 3 nodes which are associated with k8s master. it will launch the app(container) in one of the nodes. various algorithms based on which specific node is selected. it's not like that Node A is currently having 100% CPU load, in that case k8s master will not really send the appl to node A.it will send to node which has most resoirces available. k8s master will mange launching the container and managing the container in longer term.

- now due to some reason worker node A got stopped working.there might be power issue, network connectivity or kernel issue and various others.once the worker node got stopped working K8s master will autpomatically detect it. it migrate the container to diff running worker node. this also referred as record of intent. record of intent is given bu user which is "i want to launch app A ".

- k8s master not only launch container once it also performs health check.master not only  checks that the node is down it can also detect if the app is not working properly.if app container is not working properly it will restart the container, it automatically migrate the container to diff worker node.
DEMO : k8s master is 1. it can have any no of nodes from 1 to 100. in our case we have 1 worker node . run " kubectl get nodes " . it will show 1 node.here my record of intent is to launch app A container. run " kubectl run myapp --image=nginx ". run " kubectl get pods ". i can see 1 container(pod) runnig.

- here we told k8s to launch a contaienr from nginx image.this was supplied to k8s master. k8s master will look into worker nodes and it will launch that specific pod in one of the worker nodes.refer pod as a container. here container is running in cli where we are passing commands. run docker ps | grep pod_id ' i will get container ' . now stop the container " docker stop container-id". 

- now k8s will detect the container that is not working. then the k8s will automatically start a new container in a diff node. now run docker ps we can see a new container got launched.
- we already seen 3 way to supply commands to k8s (api,cli,gui). in the demo we ran kubectl command we used CLI.

***********************************************************************************************************************************************************

INSTALLATION OPTIONS FOR K8S : 
1.Use the managed K8S service generally offered by cloud providers
2.use Minikube in local system setup
3. Install and config k8s manually( Hard way )

1.Use the managed K8S service generally offered by cloud providers : Various cloud providers like AWS,IBM,GCP and others provided managed K8s clusters. in managed k8s we can run commands, here providers manage k8s clusters

2.use Minikube in local system setup : minikube is a tool that makes it easy to run k8s locally. Minikube runs a single node k8s cluster inside a VM on your laptop for users looking to try out k8s or develop.

3. Install and config k8s manually( Hard way ) : in this we install and config all k8s components individuallly.

Things to config while working with k8s : 
KubeCtl: CLI for running user commands against cluster.(in AWS just like AwsCLI)
K8S master : K8s cluster itself.
Worker node agents : k8s node agent ( agents that sits in worker nodes	.we have various agents like kubelet, kubeproxy we need to configure in every  agent)

Components to be configured in (cli,gui,api) : 
CLI :- we have to first configure k8s master. it contains multiple components. once i have master and worker nodes i have to configure CLi(kubectl).we have to send commands to master using cli.

- In managed service: we have to download Kubectl cli only. managed provides take care of master , worker nodes, and also agents in nodes
- Minikube: minikube take care of master and worker node( which is basically same workstation where master is configured), and we have to configure kubectl.
 disadv: with minikube it only provisons single node cluster

***********************************************************************************************************************************************************

USING MANAGED K8S SERVICE FROM CLOUD SERVICE PROVIDER : 
- 3 components in k8s : CLI( will install in laptop) , k8s master is referred to as control plane (  In aws k8s cluster costs 0.10USD per hour.). Worker Node Agent( where our appl will be running. worker node agent is charged in aws)

DEMO : in AWS give worker node name, region, cluster name , select no of worker nodes 1.
- once k8s cluster is up and running , in order to schedule ur appl or create various resources , the 1st thing we need is client(CLI). the client will able to communicate with k8s master. it's not like client will communicate with k8s master without , master and pswrd. this is why once the cluster is up and running we can find Cluster access config file( this contains address of k8s cluster,credentials). config file is yaml file.

- in yaml fiule we can see address of where k8s server is running. when ever we want to perform some activity we will send req to this server. we know if we just send req without credentials that req will be rejected. that's why we are also provided a token. consider this token as pswrd for k8s cluster.	

***********************************************************************************************************************************************************
OVERVIEW OF KUBECTL : 
 Install kubectl : 
- k8s command line tool which also referred to as Kubectl, it allows us to run commands against k8s cluster. let's k8scluster running.i want to create resources like pods, any other resources for that we need a command line(kubectl). through kubectl we go ahead and send istructions to k8s cluster based on our instructins, the resources will be created or deleted. in order for kubectl to connect to cluster it needs various info like DNS/Ip of cluster, authentication cred's luike username,pswrd. this is why we have one more file called kubeconfig file. it contains info of token userame,dns etc.
 path : kubeconfig resource file - Kubectl binaries - k8s master 

- to get kubectl for windows : download kubectl binary with curl on Windows. put the .exe file in a folder. and in powershell run kubectl . we can see list of commands
- run " kubectl get nodes " it will show :Unable to connect to the server. beacuse here kubectl dowsn't know the dns associated with k8s master service. and we know we can find all this info in kubeconfig file. copy the kubeconfig file that we downloaded from AWS k8s cluster and put it in  installation folder, in kubectl.exe location.
- now run kubctl.exe file and associate kubeconfig file. run ' kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes ' 
 this thrown an error " Unable to connect to the server: x509: certificate signed by unknown authority ".

IN LINUX :
 in ubuntu install kubectl binary for linux using " curl -Lo "https.." ". then make kubectls as executable file using " chmod +x ./kubectl ". now run kubectl file and associate kubeconfig file.
- run  kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port? it says connection was refused and it assumed cluster is in localhost. since k8s cluster in AWS.we need to fetch the config file that we fetched from AWS.

- so copy the config file to kubectl.exe location and there run : " kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes " we will get o/p :
 kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes
NAME                STATUS   ROLES    AGE   VERSION
worker-node-qhdi5   Ready    <none>   17h   v1.26.3

- here everytime we run kubectl we have to specify kubeconfig parameter --kubeconfig "k8s-cluster-kubeconfig.yaml" to get nodes. to avoid that we can make use of easy way. inside working directory create a file " mkdir .kube " . and there run " cp k8s-cluster-kubeconfig.yaml ~/.kube/config ".now go to"~/.kube"
 and run ls we can see cache and config. there run " cat config " . we can see all the contents of kubeconfig.yaml.
- go to location where kubectl.exe present and run " kubectl get nodes " we can see list of nodes.

***********************************************************************************************************************************************************
CONFIGURING K8S IN MINIKUBE IN WINDOWS :-

Minikube is a tool that makes it easy to run Kubernetes locally. it provides single node k8s cluster that we can install in windows linux,mac.But in this we wont be able to do everything.

Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.
***********************************************************************************************************************************************************
UNDERSTANDING PODS : 
- in docker if we want to run a docker container " docker run --name webserver nginx "
- in kubectl if we want to run a container " kubectl run webeserver --image=nginx " 
If we want to create a container in the available worker nodes in kubectl. "kubectl run webserver --image=nginx" 'pod/container created' in kubectl  is referred to as POD. run "kubectl get pods " we can see 1 pod named webserver is running
NAME        READY   STATUS    RESTARTS   AGE
webserver   1/1     Running   0          72s

If we want to connect inside a container:
in docker run " docker container exec -it container_name bash"
in Kubernetes : " kubectl exec -it webserver -- bash "

- " webserver -- bash " here -- seperates the argument that we want to run as part of container(pod). ex : i want to run "ls -l / " inside pod then :
  " kubectl exec -it webserver -- ls -l / "

- In docker if we want to remove docker container then we stop it and we remove it . EX: docker stop container_name , docker rm container_name.
- In K8s if we want to remove pods : then run " kubectl delete pod webserver " 

POD : A Pod in Kubernetes represents a group of one or more application containers and some shared resources for those containers.
EX : in pod 1 i have 1 container and it has it's(container's) ip address
     in pod 2 i have 1 container and storage volume and ip address
     in pod 3 i have multiple containers  and 1 storage volume
     in pod 4 i have multiple containers and multiple storage volumes
- there can not only a single container in pod, also has multiple containers that are part of a single pod.

Benefits of PODS : Many appl's might have more than one container which are tightly coupled and in one to one relationship.
 EX : i have 1 appl and it requires 2 containers to be up and running.1st container name is funxtion01, function02 : 2 containers	

- here " docker run -dt --name myweb01 function01 " ,same for function02. we are running a containers from function01,02. only after both the containers are up and running then only the appl will work. if one container goes down at the end the appl will go down.
 in this approach we are managng at container level. let's say we have fxn01,02 thiingsd are working fine. tmrw good amnt of load is happening.we need to know 1 to one relationship here. we need to have a sheet which states that fxn01 is dependent on fxn02 and so on. 

- One of the ways to deal with tightly coupled appl in k8s is to make use of PODS.
- in pod i have fxn01 and fxn02.we will go and create a pod , pod intern will go ahead and create multiple containers here.we don't have to create individual containers just like in docker. here things become simpler at pod level. 

EX : i have pod1 and inside it fx1 and fxn2 working fine. tmrw more load will come and things will slow down. so we can go ahead and create one more pod this pod will intern create a containers of fxn1,fxn2 that's it. we don't need to know about 1 to 1 relationship everithing k8s will deal in background.

- if something is not really working well like fxn2 is not working well in pod1 ,k8s can go ahead and automatically create 1 more pod so that our appl can go ahead and  remain available all the time.let's say 2 pods that are serving the prod traffic,however the load is increasing k8s can create more pods that can recieve the prod traffic so that the load is evenly distroibuted. just like if we work with ASG feature in aws k8s is similiar to that.

- A Pod always runs on a Node.( ex run kubectl get nodes : we have 1 node. all the pods that we are creating , they will create in this worker node)
- A Node is a worker machine within Kubernetes env.
- Each Node is managed by the Master.( we have k8s master which will manage worker nodes)
- A Node can have multiple pods( we can even create 100 pods)

***********************************************************************************************************************************************************
KUBERNETES OBJECTS :
- in earlier videos we discussed " record of intent" it is basically what we tell to k8s master and master will make sure whatever  we have informed will always be running all the time. thiis is what record of intent means. " i want to launch App A " was record of intent.k8s master will recieve and it willlaunch in nodes. master will make sure app A will always be running.if A doesn't work then Matster will move that app A to another worker node.
- Here technically record of intent is refrred to as K8S OBJECTS.

- Kubernetes Objects is basically a record of intent that you pass on to the Kubernetes cluster.
- Once you create the object, the Kubernetes system will constantly work to ensure that object exists.
- this object can be pod , it can be a namespace,itcan be a deployments etc.in k8s we can do apart from just creating a pod.those aspects are referred to as k8s objects.
There are various ways in which we can configure a Kubernetes Object.

- The first approach is through the kubectl commands. we ran kubectl run command to create a pod. that pod is referred to as object
- The second approach is through a configuration file written in YAML.( here we can create things via YAML file.we have config file written in YAML, and that config file can also be used to create K8S object. in the given yaml file (pg.no:14). in yaml file it states that it creates a pod(kind:pod means k8s should create a pod, nameof it is webserever, and it will run from image nginx. )
- save the yaml file as (pod.yaml) in one folder.if i want to create the things which are mentioned in yaml file the run " kubectl apply -f pod.yaml" (-f stands for file) in the o/p : pod/webserver created. run kubectl get pods we can see 1 pod which is running.

- YAML :
- YAML is a human-readable data-serialization language. It designed to be human friendly and works perfectly with other programming languages. there are other products which actively uses YAML.Ansible is one of the famous configuration management tools whic uses yaml.we also have kubernetes which also uses yaml.Even Cloudformation which uses yaml files.
- XML and json are a bit of pain if we write it from scratch.machine specific purposes is quite good for machines. However if we want to write things from scartch yaml ois good.
- This is why YAML is one of the supported language for writing the K8s Objects.

BENEFITS OF CONFIGURATION FILES : 
- it integrate well with change review process.in many org's we  go through a change review process, where before we apply a change we have to get it review from the peers.

- It provides the source of record on what is live within K8S cluster.: let's say there are 5 people who are managing the k8s cluster and everyone is using kubectl command to perform various write operations . if we want to look into what are the changes that are happening tdy ,ystrday,or changes thatare lying at an instant of time it is quite chalklenging to do that.if we have everything within the config file a new joine can easilyread the comfig file and he can understand on what are the things which are lying.
- it is easier to troubleshoot changes within version control. since this is config file we can even version control it, we can find out the changes applied

EX : i have pod.yaml file which has basically referenced.now some one ran this pod.yaml file and the system broke. i want to know because i tested this yaml file in dev env and everithing was working fine.when i ran it in prod things were not expected working properly. so i want to see if this file was modified or not. so we can see commit switch which were made towards this pod.yaml file and we can see changes.

- ALSO i edit one line in yaml and commit.however before this commit goes to matser i can do a curl pull req and within a pull req i can add a reviewers there. Means before this specific change goes to master branch and after it gets applied what i can do is i can go through a pull req whwre my peers were working with me they can review the change. if they find things not going expected they can reject my change, this can prevent downtime.

***********************************************************************************************************************************************************
OVERVIEW OF K8S CLUSTER ARCHITECTURE : before  we deep dive we should understand the componensts of k8s architecture.
- till now we descusses 3 componensts(1.api,cli,gui,2. cluster 3.worker nodes) but we havent gone deep into what are the components inside cluster.

- kubernetes master has API Server, Scheduler, Controller Manager etc components. worker nodes also need cretain agents which need to be running so that it can communicate with the kubernetes master and follow the  responsibility which the master assigned to it.

- Kubernetes Master consists of five major components :

  1.Kube API Server : it is a compnent in k8s cluster which exposes the k8s API.this iis api sserver where our worker nodes communicating with.so whenevr we run kubectl get nodes or pods all of those commands first reach the kube-apiserver.this is like gateway entry point for the entire k8s cluster.and this is why we see both the kubelet,kubeproxy (agents which sits in nodes they interact with kube api server)

  2. Kube Scheduler : it is a component in the master that watches newly created pods that has no node assigned.and selects the nodes for them to run.
   ex : whenever we run a command kubectl run --image_name.here what happens the data gets stored in etcd.now kube scheduler will fetch the data and now it will assign a node where the pod should run. So this is the responsibility of scheduler.because k8s master can have 100s of worker nodes so in which worker node the pod will be assigned to it is one of the imp responsibilty of scheduler.

  3. Kube-controller-manager : it is resposible for various aspects including node controller( like what happens when specific node goes down)all of these capabilities are part of control manager.it also has various things like replication controllers,endpoint controllers.service acnt and token controllers.

  4. Kube-controller-manager( for cloud environments) : earlier it is not a independent component.but now it become independent.This is sepcifically for cloud env's. so for aws we have a diff way of interacting with the aws services, the LB is diff,vol's are different.and when we run k8s in diff providers like azure ,gcp then the components thet need to be interactive those might be different.

  5. etcd : it is basically the database where all of the configuration data is stored. this compon is very imp because this is where all details are stored. etcd is a key-value store used as  k8s backing store for all cluster data.

- Kubernetes Worker Node consists of two major components

  1.Kubelet : it is basically an agent that runs on every node within the cluster.Kubelet makes sure that the containers are running in a relevant pod.if we look into diag both kubelet and kube-proxy both interacts with api-server.so lets say scheduler has decided that a specific pod should run in worker node 1, now scheduler through apiserver will send the message through the kubelet of that worker node 1 to go ahead and run the pod kubelet is responsible for doing this.

  2.Kube-Proxy : it acts as a network proxy which basically maintains the network rules on the host and also perform the connection forwarding.it might happens that various containers might want to communicate with each other in those aspects kube-pproxy proves to be imp.

  3.Container runtime(it is not a component by itself, but this is like a prerequesite) : it basically a software which is responsible for running containers . There are diff supported runtimes : Docker, Containerd, rktlet, and others.

* out of all compnentes scheduler,apiserver,control manager are the 3 imp components developed by k8s community.etcd is a component which is not developed by k8s community.etcd is a seperate product by itself.it is also used in other projects apart from k8s.
-n generally whenever we do k8s in hard way we have to pull scheduler,apiserver,control manager, etcd all these images we have to pull seperately.after downloading all the components images we have tpo configure them to connect.afyter all these we will have working k8s master.

***********************************************************************************************************************************************************
K8s Component - ETCD : 

- In a Linux environment, all the configurations are stored in the /etc directory.means in linux server we have seen that all the configurations that comes by default they are stored in /etc directory.for variouses services or apps it can be crone or ssh etc fpor all of these configs it will be stored in /etc.

- when i run " ls -l /etc" it will show many files under etc dir.these files are nothing but config files that are req for various services.if we are using ldap for this all configurations will be in etc/ldap dir, similarly for pam will be in etc/pam.
- etcd is inspired from /etc dir of linux.in etcd , d stands for distributed.for a linux server it is fine that we store all the config in etc dir which is part of that server.however for a distributed env where there can be 100's of servers , we dont want evrything to be stored in as ingle server.we need everything to bes distributed and it will be fault tolerant and highly available.

- we have seen etc used for storing configuration data, and d used for distrivuted . in page no 12(ppt). (/ which is root). we have a key called as foo and it has some value. and within /bar/this it has a vlue of 42./bar/that it has a value of take. we can create a hierarchy depending upon intent what we want to each of the key withing the hierarchy will have certain value.

- etcd is basically a distributed reliable key-value store . like above(foo(key) some value(value), this(key) 42(value)

- etcd reliably stores all of the configuration data associated with the Kubernetes cluster, representing the state of the cluster (what nodes exist in the cluster, what pods should be running, which nodes they are running on, and a whole lot more) at any given point of time.

- lets say our k8s master has 100s of nodes,1st node has 10 pods, 2nd has 30 pods . where al these data exactly will be retrieved from answer to this is etcd.ex : run kubectl get nodes.it will show list of nodes which are available in master,all of the data that we are looking is actually stored in etcd DB.

- i created one etcd single node cluster running in one env. run "./etcdctl" this is command line utility for etcd similiar to what we have for kubectl . 
- run " ./etcdctl set key value " ./etcdctl set hemanth 292528" 
- so if we want to fcetch value run " .etcdtl get hemanth it will give value associated with it.

IMP POINTS : 
- A Kubernetes cluster(including master and nodes) stores all its data in etcd.
- Anything that you read while running kubectl get pods is stored in etcd
- Any node crashing or process dying causes values in etcd to be changed.
- Whenever you create something with kubectl create / kubectl run will create an entry in the etcd.


***********************************************************************************************************************************************************
K8s Component - kube-api server ( it acts as a gateway to kubernetes master) : 
- When you interact with your Kubernetes thriugh api or cli or gui all of that data directly comes to kub-apiserver
- (if we look into pg.no 11 in ppt) we have apiserver and all of the external entities (kubelet, kubeproxy , even kubectl commands ) all of these entiroies directly connected with kapiserver.apiserver acts as a gateway inside k8s master.directlybwe can't talk with control manager ,etcd,scheduker etc.

- The API Server is the only Kubernetes component that connects to etcd; all the other components must go through the API Server to work with the cluster state.(in pg 11 diag) we can see no one can connect to etcd .(ex : if controller wants to write some data to etcd it needs to go through the kapiserver. so etcd can only communicate with apiserver)

- The API Server is also responsible for the authentication and authorization mechanism. (whenever i do k get nodes , when we run that command the data actually comes from etcd,however before the dats is delivered to us , api server actually verifiy whether we are the right person or right permission .if we dont have right permsn, we wont be able to see the data stored in etcd.( config files which has keys will actsa as authentication mechanism)

- (flow diagram in pg.14 V IMP ) :
points : 

1.Kubectl writes to the API server (kubectl run mywebserver --image=nginx)
2. API server will authenticate and authorize. Upon validation, it will write it to etcd.
3. Upon write to etcd, API Server will invoke the scheduler.
4. Scheduler decides which node the pod should run and return data to API server. API will in-turn write it back to etcd.
5. API Server will invoke the kubelet in the node decided by the scheduler.
6. Kubelet communicates to the docker daemon via Docker socket to create the container.
7. Kubelet will update the status of the POD back to the API server.
8. API Server will write the status details back to etcd.

***********************************************************************************************************************************************************
Introduction tpo API : 
- API stands for application programming interface.
- API : It is generally used for inter-communication between multiple softwares.without api also we can do it .but with api we have many benefits.

- With the API, the exact structure of request and response is documented upfront and is  likely to remain same throughout the time.
  ex : James wants to build a weather report application. Since it needs weather report for all countries, he wonders where he can get all the data from. OpenWeatherMap has all this data  and thus James decides to integrate his application fetch data from OpenWeatherMap database.
Now question is, how will he parse all this data ?   : through api. although the structure of interface changes, api structyre never changes. it will be same as always.

2n ex : You want to have list of all instances in cloud environment, along with their names, IP  addresses, OS,  kernel version region, backup windows and available snapshots. How will we go ahead with the use case ?.if we have 100s of servers, resoiurces, doing things in gui way is difficult.so through api we can fetch all data. if we dpo curl to api link we get all the data in structured way.

***********************************************************************************************************************************************************
 Kubernetes API Primitives (Analogy is the key) : 

- till now we were using cli to do various operations. so we make use of kubectl,to create pods,to get pods.so which ever we command we type through cli or gui, ultimately it goes through an appropriate api to the api server which is runnings inside k8s master.

ex : my inernet ic not working.i called the toll free no.initially whenever we call toll free no we might have seen that they ask you to dial a specific option,it might be dial 1 for billing related,2 for techincal suprt, etc. so we dial appropriate number for a specific use case 

- similarly whenever we run command through cli,cli will make an appropriate call to api depending upon what we want to do.if we want to create a pod,then it will make a specific api call,if we want to create a namespace, then that api call might be diff.in (pg.14 on diag is there)

- in diag we ca see multiple groups that are available under api.if we want to reate a pod,kubectl will actually call pod specific api which is available.if we want to create sercice,kubectl will call to servicei specific api which is available. ultimately there are multiple apis which are available,kubectls does the actiual translation.
- when we write cli commands , those cli commands will be converted to appropriate format it can be json before it is sent to api associated.
- if we want pod specific operation it will go to /api/v1/pods
- if we want namespace specific operation it will go to /apis/batch/v1/namespaces
- check page.no 15 for api levels.

***********************************************************************************************************************************************************
Creating First POD  with  YAML Conﬁguration file :

- Already discussed about multiple ways in which we can configure object withing k8s cluster.
1st : run kubectl command which automatically does lot of things for us.it can convert our command to specfic json format before it sends it to api.
2nd : we can create our own config file and that config file can be written in yaml format.
- DO the 2nd approach from scratch : we already seen how a pod can be created with pod.yaml
- what contents we need in pod.yaml : expl below

apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
  labels:
   app: demo-app
spec:
  containers:
  -  image: nginx
     name: mywebserver

  apiVersion(version of api that we want to use , we can't use all api it depends on Kind(pod or etc . we have pod and this pod isv1 core api group. in k8s   
  there are lot of api paths available.)
- after api,kind we need to define metadata. metadata uniquiely identifies a specific k8s object within the give namespace. here i given name as mywebserver
 SPec : is the desired state of object. i given containers name is mywebserver and image is nginx.

- in officlai k8s docs -https:k8.io : api overview in that there are various objects available in workloads(pods,job replicaset). click on pod we can see the yaml config for pod.if we click on replicaset it will give template(config file) for replica set.
- in kubectl run : kubectl api-resources or kubectl explain pod : it will give detailed info about pod.

DEMO : i created a newpod.yaml. this is where we will be configuring our new object through yaml file.
apiVersion: v1
kind: Pod
metadata:
  name: nginxwebserver
spec:
  containers:
  -  image: nginx
     name: democontainer

- now run: kubectl apply -f newpod.yaml : o/p- pod/nginxwebserver is created.kubectl get pods we can see 1 pod.
- to delete run : kubectl delete -f newpod.yaml ( -f is file) : o/p- pod is deleted.
***********************************************************************************************************************************************************
Working with multi container-pods : 

- A pod in k8s represents a group of 1 or more application containers,and some shared resources for those containers.A single pod can have multiple containers running. like one pod has 3 containers and 3 of them having one ip address.

Dealing with tightly coupled application : Containers within a pod share a same ip address and port space,and they can find each other via the localhost.
- in one pod 2 containers are running fx1,fx2.lets say a process is listening on port 80 in container fx1,and any other process in fx2 container can directly call via the local host to the port 80 to connect to that process which is running in container1

- write one multicontainer.yaml for :

apiVersion: v1
kind: Pod
metadata:
  name: nginxwebserver
spec:
  containers:
  -  image: nginx
     name: democontainer1
  -  image: busybox
     name: democontainer2
     command: 
      - sleep
      - "3600"

- this is an yaml file for a multicontainer pod.
run " kubectl apply -f multicontainer.yaml"  it will say pod/nginxwebserver has been created. run get pods we can see ready"2/2". 
- run kubectl  describe pod nginx webserver, within containers section we can see 2 container sections of busybox, nginx.

- How tpo connect inside container : " kubectl exec -it nginxwebserver bash ". now we login to container by default we login to container1, not container 2.
- inside container run " apt-get update && apt-get install net-tools" and run netstat -ntlp , we can see 1 port where nginx is listening(0.0.0.0:80)
- now run ifconfig we can see one private ip it got same for both containers.. now exit from container and login to container 2 by specifying " kubectl exec -it nginixwebserver -c container2 sh " . now logged into cont2. and run netstat -ntlp we can see there is one process which is listening on port 80which is running inside container1 actually, run " wget 127.0.0.1:80 we see index.html downded. run cat index we can see welcome to nginx. 
- now we understood a containers can find each other via the localhost.
- eventhough a pod has multiple containers , all the containers will hasve same pod ip.

- as we discussed we have fx1 and fx2 containers. fx1 is nginx container and it has a process which is running on port 80. we did wget from fx2 container to port 80 to local host then local host gave process of nginx as a result. we communicated with other containers which is inside pod via the localhost.

***********************************************************************************************************************************************************
K8s Component - kube-scheduler (Finding right home for a pod) : 

- main responsibly of Kube-scheduler is it watches for newly created pods that have no node assigned, and selects a node for them to run on.
EX : we have k-scheduler and we have multiple w-nodes available. what k-scheduler does is it keeps track of the nodes and its associated specs.(w-node1-fast,node-2:small,slow ; node-3 : medicore)

- whenever kapiserver tells to scheduler" a user has requested a pod, where we should we assign it, beaciuse we hav 3 nodes)this is the responsb of scheduler.scheduler will take varioyus factors into consediration and it will assign to specific node.now scheduler has told to apiserver to assign pod in node-3.Intern api server will connect with the kubelet in node-3,and the pod will be scheduled.

EX-2 : user has req for a pod,and specified that it needs to run fast, where should we assign it(asked by apiserver). scheduler will look into nodes depending upon algorithms it identifies that the worker1 has fast server.it will respond back to api server and it will go and assign in node1 kubelet.

There are several factors which are taken into consideration before a pod is scheduled to a node.Some of these includes:
- Resource Requirements ( lts say we have our appl which require min 8gb ram,we dont want scheduler to assign a node which has only 4gb ram to that specific pod,so user can say he need 8gb, so scheduler can schedule a pod to worker node which has that req)
- Hardware/Software policy constraints ( like pod can run on ubuntu only, not centos)
- Affinity & Anti-Affinity
- Data Locality

***********************************************************************************************************************************************************
Revising Dockerﬁle - CMD vs ENTRYPOINT : 

- The best use for ENTRYPOINT is to set the image’s main command. we can even set main command with cmd.whenever a image is launched whatever the things we specified in cmd will run.problem with cmd is it can be overriden. ENTRYPOINT doesn’t allow you to override the command.

run ls we see dockerfile.it is small. i has 2 instructions :
Dockerfile:
FROM busybox
CMD ["sh"]

- build " docker build -t base1 .". run " docker container run -dt --name base01 base1 ". so whenever container starts we see automatically the command sh is connected to. So whatever we specify inside cmd can be overriden.

- run " docker container run -dt --name base02 base1 ping -c 10 google.com " this container will ping google 10 times and will exit. now docker ps we can see 1 container base02, whose command is ping-cgoogle.com.

- write onemore dockerfile,  by commenting cmd
FROM busybox
ENTRYPOINT ["/bin/ping"]     specified a binary ping
#CMD ["sh"]     save and build it as base3 image. 

- run " docker run ... base3 sh " here docker will basically try to execute something like /bin/ping sh .Means "/bin/ping" this binary cannot be overriden.
-  run " docker container run -dt --name base03 base3 ping -c 10 google.com " now run ps, under smd we can see " /bin/ping c -10 google.com" it appended c google.com, however the main binary remains same.

***********************************************************************************************************************************************************
Understanding the Commands and Arguments in Kubernetes : 

- During the video of ENTRYPOINT, we discussed the difference between CMD and ENTRYPOINT instruction in Dockerfile.
- We can also refer them as Image Command and Image Entrypoint.
- In Kubernetes, we can override the default entrypoint and CMD with the help  command and args field.

Docker         Kubernetes     description 

entry point     Command       command that will be run by container 

CMD             args          with args-field we can pass argumnet to the container

EX : i connected to a linux server which has a docker installed.run ps no containers. now run " docker run --name busybox busybox" run ps no containers. run ps -a. we see one busybox container in exited state,because of "sh" command.we have one sh command, since we are not attatching to the tty, the busybox container is exiting immediatly.
- we see same thing while running image in k8s level. write an yaml file of pod launching from busybox image. run get pods, iw ill say 0/1 ready.similar to docker level.
- now launch docker busybox container by overriding busybox dockerfile command  " docker  run -d --name busybox1 busybox sleep 3600" run ps we can see container running, and command sleep 3600.login to container using sh, and we see pid 1 is sleep 3600.once this pid exits, container will also exits.

- similar to how we override default sh command for busybox in docker to sleep 3600, we can even do at pod manifest level.

apiVersion: v1
kind: Pod
metadata:
  name: override
spec:
  containers:
  -  name: override
     image: busybox
     command : ["sleep","3600"]    this we also can write (  command : ["sleep"]    , args : ["3600"] ), pid is same as sleep 3600.   or 3rd option we can 
                                   specify in args command ( args : ["sleep","3600"]  )

- run apply -f yaml, run get pods, we see override pod is running and logit to it using exec -it sh,  and inside run ps we see pid 1 proces is sleep 3600.

* Check 4 use cases in pdf file page no(19,20).

***********************************************************************************************************************************************************
CLI document for K8S resources : 

- Till now we have been going through documentation from browser to understand about fields. Ex : we looked api, gui for manifest files of k8s objects.
  There is better way through which we can achieve similar functionality via CLI .This is through the kubectl explain command.
- while writing pod manifest file, we have apivesrion,metadata, spec( under spec many things we can add).
- if we want to know about the things inside any object run " kubectl explain object ; kubectl explain pod " it will give a list of resources we can add inside pod manifest file.

***********************************************************************************************************************************************************
EXPOSE INSTRUCTION : 

- The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime.( let's say that i have a docker image and the service or the appl that is within the docker image is listening on port 5000.
- so how will a individual who is running a docker image know that a service is listening on port 5000.because the service can listen on 8000,9000.
- The EXPOSE instruction does not actually publish the port.but to inform on which port specific apll or service is running.
- if we want to actually publish a port at docker level we have to use the '-p' option.

- It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to  
  be published (generally there are 100's of images within the dockerhub.now a specific image as we are discussing it can listen the service or appl can container can listen on any fmrl port.how will a person that is running a container know on which port that appl is listening to.Anyone who builds their image, They inform with the  EXPOSE instruction at a specific service or appl.

USE CASE : we have a service container on the right side.when we perform a curl on the domain followed by the port then this service container basically returns the datablog.Now we really don't know on which port this service container is listening to.that is where EXPOSE instruction comes into picture.

DOCKERFILE : ' FROM ubuntu ;  apt-get install service ;  EXPOSE 9324 ;  the image which gets built from this dockerfile ,from the image a person who is going to to run can identify that this specific service is going to listen on 9324 and he can publish the port accordingly.

demo : launch a container from nginx image.withing the port section when we do ps. we can see '80/tcp'. so that basically means that this 'nginx -g daemon' 
       which is running inside container it is listening on a port of 80 at tcp level.this info'80/tcp' doesn't come from the container by itself.this nginx can even run on 9000.that doesn't really means that port will be updated accordingly.infact this info is coming from EXPOSE instruction which the  image creator had added in the dockerfile.

- when we do docker inspect on nginx image ' docker inspect nginx' there we can see within the "ExposedPorts : 80/tcp "
- run one more container " docker container run -d --name nginx2 -p 80:80 nginx ". when we do docker ps under ports section we can see " 0.0.0.0:80->80/tcp"
 means an traffic from (0.0.0.0) which is coming on the host port of 80  will automatically forwarded to the container port of 80. here in this case when we made use of ' -p 80:' here we are saying that any request which comes to the host port of this server on 80 will be forwarded to port 80 of the nginx container. from expose instruction it was easy to find second part.

- however there can be a lot of apploications which might listen on port 5000.then "-p 80:x" x part will change accordingly.
- i have custom dockerfile. in that i have " EXPOSE 9080 " instruction in dockerfile which , it using nginx image. in that dockerfile it is using nginx image so code : " FROM nginx:1.17-alpine ". after launching a docker image and acontainer from this cpontainer , run ps we can see two ports in ports 

section. " 80/tcp, 9080/tcp ". here we can see 2 ports because, now run docker inspect container_name : there we can see " EXPOSEDPORTS : { 80/TCP: {} , 9080/TCP : {} } . here 80/tcp comes from the image that we are using in docker file to creaste custom image.
- now what happens if we don't expose any port(9080). if we don't expose a port it will be a little diff for the person who is going to run your appl container to indentify on which port your appl is running.

- i have one more dockerfile. here we are centos as base image. and we are not giving expose port instruction. now create a image from it and run a docker container. do ps : we can see nothing in ports section.

Q. Can I create a docker image without a base image?
   The short answer is no. we need a base OS image or a base function image

***********************************************************************************************************************************************************
EXPOSING ports for Pods : 

- similiar to expose instr at docker level, we can expose a port at k8s level.

ex : 

spec:
  containers:
  -  name: override
     image: busybox
     ports:
       - containerport: 8080 
- this will say the container that is running inside a pod is exposed oin the given port no.

***********************************************************************************************************************************************************
Generating Pod Manifests Via CLI :

- till now we have been writingmanfiest file for k8s objects from absolute scratch.
- to write yaml for pod we can write it form scartch or in k8s docs take ex of pod and modify it. these both ways take good amnt of time.
- kubectl provides various cli commands,that will allow us to create the object,as well manifest associate with those object.

today we hav 3 examples : 

1. Create a Pod from Nginx Image
Syntax:  kubectl run [NAME-OF-POD] --image=[IMAGE=NAME]
Command: kubectl run nginx --image=nginx

2. Create a Pod and Expose a Port
kubectl run nginx-port --image=nginx --port=80

3. Output the Manifest File
kubectl run nginx --image=nginx --port=80 --dry-run=client -o yaml
- when we run "--dry-run=client " this will not create the object writte in the command "kubectl run nginx --image=nginx --port=80" , and "-o yaml" output of the given file is yaml.

- run"kubectl run nginx --image=nginx --dry-run=client" this will say 'pod/nginx created (dry run)' this means the object is not created. and add -o yaml(o/p of yaml).
- run "kubectl run nginx --image=nginx --dry-run=client -o yaml " under this we can see the yaml file for the pod object.


4. Delete PODS

kubectl delete pod nginx

kubectl delete pod --all

***********************************************************************************************************************************************************

DOMAIN 2 : WORKLOADS AND SCHEDULING

***********************************************************************************************************************************************************

LABELS AND INSTRUCTORS : 

- labels are key/value pairs that are attatched to objects, such as pods.
1st ex : i have 2 diff env's. 1st env : server,DB,LB  in 2nd env : LB,Server,DB.
- now if someone says to stop all the resources associated with dev env.from above it is diff to know which is dev env server, because above there is not appropriate label associated with all resources

2nd ex : we have added label to each resource like 1st server has label where key is name and value is kplabs-gateway, and second label where key is env and value is prod.\
  ex: name = kplabs-gateway
      env  = prod             same for all resources in 1st env is prod and 2nd is dev.
- in aws tags for servers similiar to tags. in aws ec2 tag : name(key)=kplabs(prod)

SELECTORS : Selecters allows us to filter objects based on labels.
EX : show all the objects which has label where : env:prod. this will show all resources where env is prod

- in aws i have 10 ec2 running. above in ec2's earch option(filters) we can consider filter as selector.i search i can see a tag optio. give dev to that tag, it will show only resources which belongs to dev.

There can be multiple objects within the k8s cluster 
 some of the objects :Pods,services,secrets,namespaces,deployments,Daemonsets. we can attatch label to objects in k8s cluster just like we did it for lb,db,servers.etc
In K8s : we have multiple pods. we can't identify each pod without labels.give label env:prod,dev for pods. we can get list of pods using sleectors.

Use case : run kubectl get pods . i can see 2 pods. from the name of pod is diff to understand which pod belongs to dev or prod.to know use -l(label).
  run : kubectl get pods -l env=dev . it will give the pod which has label env as dev.    
 or we can run : kubectl describe pod pod_name. it will give lot of info. in that we can see labels : env:dev.

IMPLEMENT LABELS AND SELECTORS FOR K8S OBJECTS :

Demo create 3 pods (names=pod-1,2,3) by : kubectl run pod-1 --image=nginx .same for pod2,3. now run kubectl get pods we can see 3 pods. to see labesl associated with pods run " kubectl get pods --show-labels " we can see each pod has a label of (run=pod-1,2,3) with this way it is diff to understand fxnlty of each pod.
- to add a label to pod-1 : run " kubectl label pod pod-1 env=dev " . env=dev is the label thatwe attatchedto pod-1. same for pod2,3( in env stage prod). run " kubectl get pods --show-labels " we can see (env=dev,stage prod for each pod)

SELECTORS : run kubectl get pods it will give all the pods.but we want to look for a pods that has env of dev.to get that we add a selector.
 run " kubectl get pods -l env=dev " . here (-l) is the selector and (env=dev) is the label we will get pod-1. same for prod,stage we will get pod2,3.
- run " kubect get pods -l env!=dev " : pod2,3. labels which doesn't have dev env are pod2,3.

- to remove labels for pod : run " kubectl label pod foo bar - " ( foo is name of pod. name of key is bar )
  run " kubectl label pod pod-1 env- " this will remove the env label associated with pod-1.

- but in prod we mostly use Yaml files for objects not kubectl.
run " kubectl run nginx --image=nginx --dry-run=client -o yaml " dry run client with the o/p of yaml.we will get a yaml file of the pod nginx. inside yaml we can see labels section under metadata

-  run " kubectl run nginx --image=nginx --dry-run=client -o yaml > label-pod.yaml " here yaml file will store to label-pod.yaml file. inside that filke under labels section we can add 1 more label (env=dev) and save file. from cli run " kubectl apply -f label-pod.yaml ". pod will be created 

- if we want to apply labels to all pods run " kubectl label pods --all status=running " this will apply label(status=running) for all the pods.
- to get more info about label run : kubectl label --help.
- to delete all pods run " kubectl delete pods --all " 

***********************************************************************************************************************************************************
OVERVIEW OF REPLICASETS :

 A ReplicaSet purpose is to maintain a stable set of replica Pods running at any given time.
EX : we have a replicaset. within replicaset we have 2 configuration settings.1.desired state = 3. 2.Image=Nginx . Now replicset will maintain 3 pods of nginx always running within our k8s cluster. this is replicaset purpose.it always maintain desired amount of the pods from the images thatwe have specified to replicasets.
DesiredState: The state of pods which is desired at any given moment of time
Current state : The actual state of pods which are currently running within k8s cluster.

Replicaset : DesiredState=3,  CurrentState=3, Image = Nginx.  Means we would like to have 3 replicas of pod from nginx image to be up and running.current state is 3, because 3 pods running. Now let's say due to some reason pod 2 stopped working.now currentstate becomes 2.however desired state is 3. Now replicaset will go ahead and it will launch a new pod so that our desired state matches the current state. Remember replicaset will always tries to maintain desired state=Currentstate.

DEMO : run kbectl get pods .we can see 3 pods running.
 run " kubectl get replicaset " we will get 1 replicaset, name is frontend.Desired is 3, current and ready is 3.
- run kubectl get pods.we cans see 3 pods from frontend replicaset. in list of 3 pods delet one pod from replicaset . run " kubectl delete pod frontend-pod-id " .  now run " kubectl get pods " we can see 1 more pod got automatically launched 10sec back.it is luanched by replicaset. becuse we deleted one pod and total count(currentstate) didn't match the current set.this is why replicaset automatically created one more pod. 

***********************************************************************************************************************************************************
CREATING REPLICASET:

 implement replicaset through yaml file. Docs : https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/  .we can find the yaml exampe for replicaste.
replicaset yaml file in k8s docs :

apiVersion: apps/v1              (replicaset is part of apps/v1. run "localhost:8080/apis/appv1" we get resourceslist in apps/v1 like controller, daemonsets   
                                  deployments,replifcasets)since replicaset is in apps/v1. so the apiversion of replicaset is apps/v1.
kind: ReplicaSet
metadata:
  name: kplabs-replicaset     ( name of replicaset)
  labesls:                    (label to replicaset)
    tier: frontend 
spec:
  replicas: 5                 ( specifying no of replicas to be up and running.)
  selector:                   ( here we are matcjing the label where the key is tier and value is frintend) 
    matchLabels:
12    tier: frontend
  template:                   ( template associated with the resource of pod. here in template we don't give apiversion and kind replicaset will take care. 
                                we start from metadata)
    metadata:
      labels:                 (metadata is empty.attatching a label to the pods which will created. we specified replicas=5. means 5 pods will be created
13      tier: frontend        ( all the 5 pods will have a label of tier frontend)
    spec:
      containers:             ( in spec we have 1 container , its name, and image)
      - name: php-redis
        image: nginx

- Save the above file as replicaset.yaml. and run " kubectl apply -f replicaset.yaml " it will say 'kplabs-replicaset has been created'
  run ' kubectl get replicaset ' we can see newe replicaset name is kplabs-replicaset, desired state and current state is 5. and ready is 0. (ready=0) becuae it can happen the containers are in creation stage. after some time pods(containers) get created. now run get replicaset, we can see ready=5.
- we alreday discussed replicaset will always try and maintain  the desired amount of pods. delete one pod from replicaset " kubectl delete pod pod_name ".
 - we deleted one pod.run " kubectl get pods ". replicaset will create 1 more pod . 

- run " kubectl get pods --show-labels ". we can see all the pods has a label of frontend. where we specified in yaml file : metadata:labels:tier:frontend
- in line 12 : tier: frontend = this label is for replicaset to see how many pods are running. replica will look into labels.in pods replicaset will check if 1 pod has tier=frontend it will count as 1..and 2nd 

- it is vry imp that Selector should match the labels that we associate it with the pods
- replicaset shortcut=rs . run " kubectl get rs " it will give list of replicasets.
- run " kubectl delete rs kplabs-replicaset" this will delete kplabs replicaset.

- if tier: backend in 12th line and tier:frontend in 13th line above , this will throw an error : label within the template doesn't matchj the label within the selector. whatever label we give in the selector should match with the label in template.

***********************************************************************************************************************************************************
Deployments : 
- Challenges with replicasets(RS): In rs we get basic fxnality like managing pods,scaling pods etc
EX : we have rs. it has desired state is 3. tmrw i want 5 pods.here we go and change the desired state of rs.and rs will launc 2 more pods so that curerent state becomes 5.for these basic fxnaloities we can use rs.

- however if we want certain imp fxnalities related to rolling out changes,rolling back changes and so on , then we need to go with deployments.
- Deployments provide replication functionality with the help of ReplicaSets, along with various additional capability like rolling out of changes, rollback 
  changes if required.

- we have a deployment set which sits at the top , now deployment set internall it makes use of replicaset to achieve replication capability related to pods. along with that deployment can also do various things like rollback, it can maintain the appl revision.

BENEFITS OF deployments : ROLLOUT CHANGES -
- We can easily roll out new updates to our application using deployments. let's say we are managing k8s within our org.it might happen that for every few days there will be new version of our appl which has been tested and we want to rollout that new version of our appl.

 use case : we have a Deployments.it uses rs.and rs have 1 pod. Now tmrw there is a new verion of our appl which has been released and we want to makesure that our prod has that new version.So now Deployment will create one more replicaset it will launch a pod with the latest version of our appl.and then it will go ahead and connevt to that replicaset. now once our new version of appl has been created then Deployment will remove the older version of our appl.

- This deply will makesure that whenever a new version of our appl is deployed the appl is not down.because appl not down is imp aspect.
- Deployments will perform an update in a rollout manner to ensure that your app is not down.

Benefits of Deployment - Rollback Changes : 
- let's say we deployed a new version of appl and due to some reason the appl is giving a lot of issues to our users.now i would like to rollback from the latest update to the previous working  update which is was working perfectly.in this case we can easily do it with deployments
- deployments behind the scenes it uses replicasets.however it provides lot of additional capabilities like rolling out our rolling back changes

DEMO : run kubectl get deployments : i see 1 deployment(kplabs-deployment) which is currently available.
- look into rollout history associated with this deployment. we can see multiple revisions which has been made to this specific appl. like we have revision 2,3,4 etc.let's say we switched to revision2. now due to some reasons there are lot of errors that are happening in this revision. now we can roll back to revison 1 or we can even change our appl revison to version 3,4 etc.this is one of the adv's of deployments that we can not only look into the rollout history to see on what of the updated changes that are been happening to our appl.we can also rollback to a specific revison incase if latest version of appl is giving errors.

CREATING OUR FIRST DEPLOYMENT : we already saw the video of replicasets we already know the structure and imp of every field within the yaml file. we will make few changesto replicasets yaml file like( kind:deployment, change name (optional) ) and save file as kplabs-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx

- now run " kubectl apply -f kplabs-deployment.yaml " it says 'deployment.apps/kplabs-deployment' created. run kubectl get deployments we can see 1 deployment created. we already know that deployment makes use of replicasets to make sure that there are always a specific no of replicas which are running.
- even in deployment.yaml file we see in spec:replicas=5.here deployment will create a rs which would have a desired state of 5.to know run " kubectl get replicaset" we will get 1 replicaset name(kplabs-deployment-9shdyujgd has a desired and current state is 5). run kubectl get pods we can see 5 pods currently running. these 5 pods are managed by repolicasets and replicasets are intern managed by deployment


deployments : ROLLOUT CHANGES : we already have 1 deployment, replicaset and 5 pods in rs. now we want to update the appl ,so now deployment will create a new rs and oit will luanch all the pods within that replicaset and that rs will be connetcted to deployment. once the pods within the rs is running perfectly then deployment will go ahead and it will remove the pods associated with the previous appl(rs).

 DEMO : in deployment.yaml at last in spec section we can see the image which the pods are launching is nginx. assume this is version1 of appl.tmrw i want to specify a custom nginx version there( in dockerhub lot of nginx versions available.) so in 2nd verison of our appl we want use 1.17.3 .

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx:1.17.3      save this file.and aplly the changes within deployments.yaml file." kubectl apply -f deployments.yaml" now run kubectl get replicasets. we can see 2 replicasets which are created.( we already discussed in order for a new rollout change to happen deployment will create a 2nd replicaset and 2nd rs will have the new version of our pods running. along with that deployment also start to remove the pods which are present within the older replicaset in a rollout fashion.) 

- now run kubectl get replicaset : we will see there are zero pods which are currently present in older rs. and new rs has all the pods which are running.
 the new rs has the latest version of the nginx image which is running.
- to know more about it : run " kubectl describe deployments kplabs-deployment " this will give info about our deployments . in events section we cansee detailed info about the rollout .
 in events we ction we can see 
- " scaled up rs kplabs-deployment-1st to 5 "
- " scaled up rs kplabs-deployment-2nd to 2 "
- " scaled down rs kplabs-deployment-1st to 4 "
- " scaled up rs kplabs-deployment-2nd to 3 "
- " scaled down rs kplabs-deployment-1st to 3 "
- " scaled up rs kplabs-deployment-2nd to 4 "
- " scaled down rs kplabs-deployment-1st to 2 "
.
.
- " scaled up rs kplabs-deployment-2nd to 5 "
- " scaled down rs kplabs-deployment-1st to 0 "

- run kubectl get replicaset : 2 replicasets(1st,2nd)we know deployment maintains bot the replicasets.
- if we want to change the scaling up and scaling down of replica would happen if we change the rolling update strategy.
- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment " : it will show 2 revisions. means deployments maintains revison associated with our changes. like earlier we had simple nginx image that was revison 1. then we chnaged to nginx1.17 this is revison 2.

- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment --revison 1" : thi will show all details including image:nginx.
- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment --revison 2" : thi will show all details including image:nginx:1.17.3

This revision is a great functionality. because it happens luke we have shifted to reviso 2 and there are lot of issues whic are happening in revsion 2 then we can go ahead and even rollback to the previuous revision which was known to be working well.
IMP:
- Deployment ensures that only a certain number of Pods are down while they are being updated.
- By default, it ensures that at least 25% of the desired number of Pods are up (25% max unavailable).
Lt's say i have a deployment which has 3 pods.now i am updating appl. so deployment can depending upon strategy and my configuration deployment can bring down all 3 pods at a time of our appl so at this time our appl lead to 100% downtime.but i don't want this. i dont want deployment to immediatly terminate all my pods of running applbecause that will lead to downtime.

- so what deployment does is we have 3 pods.deployment wull terminate 1 pod, 2 pods are still running whic will serve rtraffic.Along with that deployment will go ahead and launch one more pod of my new appl.if new pod running perfectly ,remove 1 more pod from old deployment and launch new pod in new appl. till the desired statein new appl equals to current state in new appl and current state in old deployment equals to zero.
- Deployments keep the history of revision which had been made.

***********************************************************************************************************************************************************
ROLLING BACK DEPLOYMENTS : 

- if new pod(new appl) is not working , we can rollback to old pod which is uder old RS. deployments offers revision to roll back changes.
run k rollout history deployment/kp-deployment : it will show 2 revisions.curently we are under 2nd revision. if we want to go back to revsion 1, then run "k rollout undo deployment/kp-deployment --to-revision=1 " this will go back to revision 1.it will say rolled back to 1.and it will delete old pod and create new pod like that.

CREATE DEPLOYMENTS VIA CLI : 

- to write manifest for depl is very time taking instead of that we can write command in clie and we can get it as yaml.

- run " kubectl create deployment my-deployment --image-nginx --replicas 3 --dry-run=client -o yaml " this doesnt create depl, but it will display yaml file of deply with replicas in cli. in command if we dont spicify no of replicas, this will create default of 1 replica.

- run " kubectl create deployment --help " this will give ex's of deployments command.

IMP POINTERS-DEPLOYMENTS : 

for exams we should know the below 4 pointers:
1. You should know how to set a new image to deployment as part of rolling update.
ex : currently our depl is on nginx. tmrw we should update it to apache image.
2. You should know the importance of --record instruction.
3. You should know how to rollback a deployment.
4. You should be able to scale the deployment

- Kubectl set image deployment nginx-deployment nginx=nginx:1.71 --record   : for setting the image to deployment
- kubectl scale deployment nginx-deployment --replicas 10     : for scaling deployment
- kubectl rollout undo deployment nginx-deployment    : for rollout undo the depl.

DEMO :
create a depl with non existing image :
- run "kubectl create deployment my-deployment --image-nginx2" this will say error : imagebackpulloff.

-  Kubectl set image deployment nginx-deployment nginx=nginx:latest  and run get poids we can see 1 pod terminated, 1 created. now for this run " k rollout history deployment my-deployment" it will show 2 revisions with change reason"none". so with record instriuction we can add change-revision.
 so run "Kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record"  run this, and run rollout history, in revision 3 we can see the whole command of ngnx:1.9.1 under change cause.

How to rollback to old depl : run "Kubectl rollout undo deployment nginx-deployment " run and it will say nginx:1.9.1 rolled back to latest. to check run
" kubectl get deployment nginx-depl -o yaml " under spec in yaml in containers section we can see nginx:latest.	

How to scale deployment : currently nginx-depl has 1 replica. 
run " k scale depl nginx-depl --replicas 4" run get



***********************************************************************************************************************************************************
Understanding DaemonSets : 

Use case : i have 3 worker nodes(node1,2,3).i want to run a single copy of pod(Webserver) node.
common ans: create a deployment with replicaset of 3 and will launch 3 pods in 3 nodes. But in deployment it can happen that 2 or 3 copies of pod will be launched in a single node. but we want each pod should launch in each node. to achieve this we can use Daemonset.
- A DaemonSet can ensure that all Nodes run a copy of a Pod.
As nodes are added to the cluster, Pods are added to them.

- like i can have multiple copies of pods in a single node.or each podn in each node . what is advantage:
ex : i have an antivirus pod which scans every files in node in this case we need only one pod for one node.
2nd ex: whenever a new node comes up , then that new node should contain a single copy of antivirus pod. all these ex's will be achieved with the help of daemonsets.
DEMO : daemons.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kplabs-daemonset
spec:
  selector:
    matchLabels:
      name: kplabs-all-pods
  template:                    ( in template we are launching a conrainer in spec section. also every pod(container) thats get created that pod will have 				                a label of kplabs-all-pods. we also have selectore in above spec section which will have the same value as   
                                                 labels)      
    metadata:
      labels:
        name: kplabs-all-pods
    spec:
      containers:
      - name: kplabs-pods
        image: nginx

- run kubectl get nodes. i can see 2 nodes. now run apply -f daemons.yaml  :daeomonset.apps/kplabs-daemonset created.
- run get pods : we can see 2pods launched.
- run get pods -o wide : we can see in nodes section 1st pod from 1st node and 2nd from 2nd node.
 - also anytime anybnew worker node is launched in cluster.daemon set will also launch the pod in thgat node
- run kubectl get daemonset we can see 1 daemonset.desired=current=ready=2.
- run describe daemonste name_ofdaemonset

***********************************************************************************************************************************************************
NODE SELECTOR : 

- NodeSelector allows us to add a constraint about running a pod in a specific worker node.

Use-Case:
- AppA requires a faster disk in order to be able to run effectively.
- Run AppA in nodes which has SSD.
we have 3 wnodes, and one appA pod.we have to place this pod in node which has ssd disk.

Step 1 : we can achieve this usecase by adding label to the node depending upon their disk types.
ex : wroker nodes labels: node1 = disk:hdd, node2 = disk:sdd, node3 = disk:hdd

Step2 : Create a nodeSelector configuration to run pods only on nodes which has a label of disk=ssd. we already seen selector video.here we are additionally selecting the ndoe here so it is called nodeselector.

Demo : get nodes, we have 3 nodes available,assign labels(hdd to node1,3, and ssd to node2)
- kubectl label node node1 disk=hdd
- kubectl label node node2 disk=sdd
- kubectl label node node3 disk=hdd

- run k describe node node1 : we can see disk=hdd under labels section, in labels nodes also will have pressigned labels from providers etc.
- 
apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
spec:
  containers:
  -  image: nginx
     name: mywebserver
  nodeselector:                         we just added nodeselector to pod manifest file. in nodeselector we gave label key pair.
     disk: ssd
- no a pod with the nme of mywebserver,will be created in a node which has a label of disk: ssd. run k apply -f pod.yaml. get pods we see one pod launched run kubectl get pods -o wide , we can see this pod is running in a node which has disk ssd.

Built in Node Labels : Nodes come pre-populated with the standard set of labels.
these include : 
kubernetes.io/hostname,
kubernetes.io/os
kubernetes.io/arch
failure-domain.beta.kubernetes.io/zone
failure-domain.beta.kubernetes.io/region
beta.kubernetes.io/instance-type

***********************************************************************************************************************************************************
Understanding Node Affinity : 
- For multiple reasons, there can be a need to run a pod on a specific worker node.it can be that a worker node is too fast interms of memory, cpu etc.
* Node affinity is a set of rules used by the scheduler to determine where a pod can be placed
In Kubernetes terms, it is referred to as nodeSelector, and nodeAffinity/podAffinity fields under PodSpec.

In Kubernetes, we can achieve nodeAffinity with the help of:
- nodeSelector
- nodeAffinity (more flexibility)   it will be replacing woith nodeselector in future.

Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.

HardPresence : RequiredDuringSchedulingIgnoredDuringExecution     required means hard preference
SoftPresence : PreferredDuringSchedulingIgnoredDuringExecution    preferred means soft prefer(soft pref means even if the constraint is not fulfilled,still our scheduler can go ahead and place your pod to that spoecific node) however for required our constraint must match in order for the  pod to place in node

Demo : for node-affinity-required.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: required-node-affinity
spec:                                     in spec we have affinitysection,inside we have nodeaffinity, within nodeaffinity we have req section.
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:               in req section we have, match expression,where we specify key(this key is label assigned to node, node should have
          - key: disk                     a key of disk and value as ssd.
            operator: In                    
            values:
            - ssd
  containers:                             we specifying a container name and image.
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0

run k apply node-req.yaml, pod created, getpods -o wide,we can see pod is luanched in a node which has label of key disk=ssd.

- now i want to place a pod which doesnt has value ssd.for that we just have to change operator.in k8s doc for operator we have diff values(IN,NotIN, Exists,DoesNotExist)

- so for above yaml chnage operator In to NotIn , save and apply by deleting previous pod.get pods -o wide, we can see a pod scheduled in a node which doesnt has ssd disk.
- now change yaml code againa and give value of ssd to ssd2(which doesnt exist).now get pods we can see pod in pending state, and run describe pods, below we can see failed from schduler: 0/3 nodes are available,3 nodes didnt match node selector)


- we also have preferred-scheduling-yaml file, where we have prferred-schduling,


apiVersion: v1
kind: Pod
metadata:
  name: preferred-node-affinity
spec:                                     
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: memory
            operator: In
            values:
            - high                        (none of our nodes has the memory key andvalue high or medium)
            - medium
  containers:                             
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0

- run apply, it says pod is created, run get pods we can see one pod in running state.run get pod -o wide, and descrie the node which contians this pod, in labels section nowehere we see key memory.

- So incase of prferred scheduling what happens is , if k8s found that none of the nodes has it , then it will not stop pod from being sceduled.However incase of required if none of the node has it then it will not schedule our pod.

***********************************************************************************************************************************************************
RESOURCE REQUESTS AND LIMITS (Scheduling Objects)

- If you schedule a large application in a node which has limited resources, then it will soon lead to OOM(out of memory exception) or others and it will lead to downtime.
Ex: we have a scheduler and 3 nodes,we can see size of nodes are different.node1,small,2,medium,3 large.it might happen that APPA pod scheduled in Node1.AppA might need much more resources than node1 has and this can lead to down time.to overcome this we can manually specify that one specific pod require 512mb ram,to run and occcasionally it might need a max of 1GB ram.then it might not assign in node1(doesnt have much memory), instead it can assign to Node03(which has much ram available)

- All of these can be achieved with the help of requests and limits.

- Requests and Limits are two ways in which we can control the amount of resource that can be assigned to a pod (resource like CPU and Memory)
Requests: A pod is Guaranteed to get a specific amnt of ram or cpu mentioned.
Limits: Makes sure that the container does not take node resources above a specific value(limits basically limits the amount of resources a pod can take)

ex : for one pod we give req of 300mb. now scheuler will assign a node for that pod with 300mb. we also give limit of 400 mb.so that pod can take upto 400mb in node not more than that.because if appl gets rogued it takes huge amnt of ram and cpu,so during that =time we have to restart our appl.

limits.yaml : 
apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
spec:
  containers:
  -  image: nginx
     name: mywebserver
     resources:                         in resources section we specified resources and limits
       requests:
         memory: "64Mi"
         cpu: "0.5"
       limits:
         memory: "128Mi"
         cpu: "1"

- run apply limits.yaml, and run get pod -o wide. scheduler will looks into requests sectionand checks about memry,cpu.

- Kubernetes Scheduler decides the ideal node to run the pod depending on the requests and limits.
- If your POD requires 8GB of RAM, however, there are no nodes within your cluster which has 8GB RAM, then your pod will never get scheduled.
demo : change the requests:memory: 6400mi. in above yaml file. currentlyu we dont have node which has more than 6gb ram, see how scheduler will do it.
- now apply it will say seror" must be less than or equal to memory limit because we gave limit to 128mi only. Now keep it to 1200mi save and apply.and run get pods we see pod in pending state. now run describe pod, we cansee in events " failedscheduling, 0/3 nodes are available because of insufficient resources"

- scheduler schedule based on requests, not on limits(limit is like best effport). request is consideration based on which our pod will be scheduled to a given node.

- now in yaml give request is 640mb, and limit is 120GB.save and apply, and run get pods we can see one pod scheduled in a node.means scheduler will look into request section ,schduler took request as consideration while scheduling a pod.

***********************************************************************************************************************************************************
SCHEDULING A POD WITHOUT A SCHEDULER ( Static Pods) : 
- scheduler is respons for scheduling a pod on a given worker node. when we do kubectl run which creates a deplymnt, then where will be our pod will be assigned decided by scheduler.once node is decided by scheduler,our api server communicates with kubelet and kubelet intern takes care of running the pod.

- kubelet basically takes info from apiserver related to pod specification and it creates pod accordingly.it is not necessary to kubelet to always use apiserver for instructions(imagine there is not connection b/w kubelet and apiserver. now i want to schedule a pod at particular server). now we can directly tell to that kubelet to run a specific pod, now kubelet directly take specific instructions for running a pod in that nbode and it will run it.This process is called static pods.

- Pod created directly without schedulers are also referred to as Static Pods.
Demo : i am in a server which has minikube installed, now run minikube status it show running.run get nodes, it shows one node and it has kubelet running to check (systemctl status kubelet ) it will say running.we know kbelet is responsible for communicating with apiserver taking the details and coordinating with the docker daemon and run a specific pod.No we manually tell the kubelet to serve a static pod without telling apiserver.

- we can do it by specifying yaml file in node manifest file(maniifest file contains apiserver.yaml,schduler.yaml....; if we add pod.yaml file here) then kubelet automatically lqunch that resource menioned in that yaml file under manifests file)
.but disadv with static pods is we will miss health check options, if pod =goes down it wont launch in another node.

***********************************************************************************************************************************************************
Taints & Toleration :

- Taints are used to repel the pods from a specific node.
- we have node 1 ,node 2 right and left side. pod1,pod2 right and left.
- when we apply taint(boundary) on node1. so whenever taint applied on a specific node and if a pod is scheduled on that node by default it will be blocked.
  so if a scheduler tries to schedule a pod on the node which has been tainted then the effect would be blocked. node2 is not tainted. if we schedule same pod2 on node 2 it will be success. How can we apply a pod on node which is tainted. Ans : Tolerations.

- In order to enter the taint worker node, you need a special pass.This pass is called toleration.
 same ex we have node1 which is tainted. a pod is trying to schedule itself in that tainted node.also pod got pass(specia) to enter node. since pod has special pass , the pod will be allowed to schedule in node1. pod can even scheduke on node2 because it is not tainted.

- run get nodes we have 2 nodes. run describe any one node.(kubectl describe node non=de_name): in that taints:None(no taibts applied on this node)
- to apply taint on node : run " kubectl taint nodes node_name key=value:NoSchedule" : specific node is tainted.
- run describe node : in taints section we can see taints:key=value:NoSchedule. as soon as node gets tainted it will block any pod that will launch in that node.
Syntax : 
tolerations:
 - key : "key"     ( it is giving the pass value as key)
   operator: "exists"
   effect: "NoSchedule"
- if we want to untaint any node run " kubectl taint nodes node_name key=value:NoSchedule- "

***********************************************************************************************************************************************************
COMPONENTS OF TAINTS AND TOLERATIONS : 

- A taint allows a node to refuse pod to be scheduled unless that pod has matching toleration( Pass)
components :
key : a key is any string upto 253 characters( we can give any value as key)
value : a value is any string upto 63 characters( we can give any string as value)
effect: there is 3 values only(NoSchedule, PreferNoSchedule, NoExecute)
operator : two values(eaual, exist)

EFFECTS :
1.NoSchedule: new pods that do not match the taint are not scheduled onto that node.Existing pods on the node remain.
2.PreferNoSchedule : New Pods that do not match the taint might be scheduled onto that node,but the scheduler tries not to.if we use this effect there can be a chance where the pods which doesntn have the associated taint can get scheduled there. Existing pods on the pod remain.
3.NoExecute : new pods that do not match the taint cannot be scheduled onto that node. however Existing pods on the node that do not have a matching toleration are removed.

Operator : 
1.Equal : the Key/Value/effect parameters which we mention in the toleration must match.This is the default.
2.Exists : The key/effect parameters must match.You must leave any blank value parameter, which matches any.

***********************************************************************************************************************************************************
MULTI-CONTAINER POD DESIGN PATTERNS : 

Overview of Sidecar Pattern : Sidecar pattern is nothing but running multiple containers as part of a pod in a single node. Single pod can contain multiple containers in it.

- In fig we have a circle wich is pod. it has 2 containers named 'File puller','webserver' and it also has a volume which is used by both the containers to share the resources.this patterns is referred to as sidecar pattern. we have seen 3 tyre bike when we move bike, side part also moves.similiarly a pod which has multiple containers,so whenever we deploy this pod in any node,they will go together and they are connected each other.

Ambassador Pattern: Ambassador Pattern is a type of sidecar pattern where the second container is primarily used to proxy the requests.in diag(pg 15) we have a pod and it has 2 containers.the proxy server on the right hand side acts as a side car and the main aim of proxy server is to proxy the req from appl server.lets say appl server wants to connect to db server,appl send a req to proxys server, proxy intern will send the req to db server.so any pattern of side car which is porimarly used to proxy the request is called amassedor pattern.

SideCar-Container Adapter Pattern : Adapter Pattern is generally used to transform the application output to standardize / normalize it for aggregation.

- let's say we have a container appl based on PHP and it is writing data in specific format.Tmrw we need diff format.we want to interchaneg fields and add also ewant to add new fields.so for that i dont want to modify my appl,for that i will introduce one more container,called as the adapter contaier,adapter container main job is to transform your logs.any logs which our appl contaner is storing in a volume , those logs will be accesed by rhe adapter container and it will be modified according to the rules that are within the adapter container.we generally have appl's like Flue and D which generally works in this pattern to transform your logs.

***********************************************************************************************************************************************************
ADAPTER PATTERN(MULTI-CONTAINER PATTERN) :   *** Very IMP

- Adapter Pattern is generally used to transform the application output to standardize / normalize it for aggregation.
- we have 2 pods (pod1 based on PHP, pod2 based on JAVA). if we look into output log format(pg 16) pod 1 has specific log format(date host duration) pod 2 also has same field but(host duration date). Similiarly we see in lot of orgs where there are multiple appl's and each appl has diff approach and diff format of logs.But whenever we use centralaise log monitoring system we will benefit if we have standard lo=g format across all appl's.Because we have to write regex.lets say we use log monitoring soln like ELK and even splunk. In order for we to parse the logs we have to write a regex.and if we have 10 diff appls' within our org, then we have to create 10 regex for 10 appl's.

- So haviung a standardaize log format will save our time. So it would be good if we have something like (  [PHP/JAVA] [DATE] [HOST] [DURATION]   ). so irrespective of the appl(if we have 10 appl of PHP) then all will have a format of ( [PHP] [DATE] [HOST] [DURATION] ), similarly if we have 10 appl based on jave then they will have format of ( [JAVA] [DATE] [HOST] [DURATION] ). in these approach our logs will be in stardaize format. Once these logs go to centrailaize log monitoring soln , it will be easy for us to parse them.

- Now in enterprises we already have appl which is perfectly working well tested and things are working well. So in such cases where we have to change format , there are two approaches here : one we modify the appl itself  so that the appl stores the data in a newer format, this is diff. if we have 50 appl within our org, now we have to modify all the 50 appl's.

2nd approach is introduce an Adapter Container, you can transform your logs to standardize it. we have container php, and adapter contaijer. Adapter container will take logs from where php appl is storing , it will transform the logs and it will store it in a new file altogether.
ex : php container format : [DATE] [HOST] [DURATION]     changes to adpater format : [PHP] [DATE] [HOST] [DURATION]
Since containers in PODS can share volumes, adapter containers can easily access App logs.

- So instead of changing 50 appl, add one adapter container. 

DEMO : i have one adapter.yaml file. this launches a pod from image of busybox. 
in yaml first read 2nd part which is after (***). then read 1st part.

apiVersion: v1
kind: ConfigMap            object of configmap. This object is ntng but config file associated with fulentd. fluend takes care of transformation,before it 
                           can transform the logs it needs to, know where does the appl store the logs
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    <source>
      type tail
      format none
      path /var/log/1.log           we are telling that the appl store logs here 1.log willstore here
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
    <source>
      type tail
      format none
      path /var/log/2.log             appl store 2.log here
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
    <match **>
       @type file
       path /var/log/fluent/access         the new transformed logs will be available at flent/access dir
    </match>
*******************************
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:                 ( in args it is echoing certain data($i: $(date)) to a log file which is( /var/log/1.log;) and also it is echoning one more date 
                            "$(date) INFO $i" to /var/log/2.log; this is similar to (we have 2 appl.each appl has diff format($i: $(date), $(date) INFO $i)  
                            and both of them have been stored under /var/log dir. 
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent                    ( we have one more container using fluend. same as we have 1 appl outputting the data to var/log dir, now we intro 
                                           more adapter container , this will be responsible for transforming the logs.Since php is toring under /var/log 
                                           dir, we are also referencing this volume under the adapter container.now adapter and php both are appl's, we want 
                                            appl to store a data in volume in such a way that the data gets accessed by adapter container. that is y vol req
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log                volume mount
    - name: config-volume        we mounting config map to a path called as /etc/flunetd-config.
      mountPath: /etc/fluentd-config
  volumes:                                we are creating a volume based on empty Dir.this volume we mounted on container called as mount.we also mounted 
                                          this volume under /var/log dir of the adapter container
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config


- runa apply, it says configmap/flentd-config created and pod/counter created. run get pods we see 2 containers(php,adapr]ter) running.similiar to pod has 2 containers (php,adapter(fluentd))
- login to 1st conytaijer " kubectl exec -it counter sh " and go to " cd /var/log " and run ls we see 1.log and 2.log 3.fluent files will be available here.

- in var/log : run "less 1.log " run " less 2.log " we see 2 diff formats of logs.so both files has diff formats of logs.

- we have referenced the transformed logs to store under the /fluent directory. so run /var/log/fluent and run ls we see 1 more file. and run less on that file we see this file has transformed log format(PHP,JAVA,) 

- in exam if we get this type of quest we just have to give config map( not the data inside it) , mount this configmap to the fluentD containers,
- above we mount 

***********************************************************************************************************************************************************

DOMAIN 3: SERVICES AND NETWORKING

***********************************************************************************************************************************************************
 KUBERNETES SERVICE( networking aspect ) :

- whenever we create a pod we will have a corresponding ip address associated with pod.
- ex : there are 2 pods : frontend will have 10.2.0.1 ip address,backend will have 10.2.0.2 ip address. if frontend wants to communicatw with backend it can do it with the help of ip address associated with backend pod.

issue1:
- in org we will have frontend appl and backend appl.frontend appl will connect to backend appl to execute certain logic.so in order for both of them to communicate the frontend will need a ip of backennd. assume within frontend config file we sepcified backend_url=10.2.0.2 which is ip address associated with backend pod.frontend appl will refer to the given config paraeter(backen_url)to connect to backend to send the appropriate data which would be computed

 by the backend servers. asume due to some reason the backend pod has stopped working and it doesn't respond. so now if we have deployment it will llaunch a new pod and this new pod will have a diff set of ip address(10.2.0.8). the appl running in frontend and its config paramter still has the ip address associated with the old pod. now frontend appl will give error(http 500). means frontend although it is working it will give error to client eventhoiugh the working backend. 

- the config file of frentend doens't have the updated ip address associated with the new backend pod which got created. we will face these type of issue whenw e hardcode the ip or dns name associated with the pod.
Issue2 :  we have 1 frontend pod and multiple backend pod. it can happen that there are 3 backend pods.since tmrw lot of req's the backend might scale to 10.Q is how will we update the ip addresses of the backedn pods in frontend config file.if we are using deployments then replicasets can scale a lot.

- to overcome these type of issues we can use K8s Serviuces : in this we create a backend-gateway( this will connect to all the backend pods). if 5 pods in backend are reated then allk the pods associated with backend gateway. now only thing that frontend has to do is it has to send the req to backend-gateway.

- assume one pod among multiple pods in backend stopped working , we don't have to worry.becuase backend gateway will not send req to that pod.however the req will still be sent to other 2 pods which are running or if new pod gets greated then it will autoimatically will be associated with backend-gateway.

- the backend-gateway which acts as a intermediatory layer is called as Service .K8s service can acts as an abstraction which can provide a single IP address and DNS through which pods can be accessed. means backend-gateway will have an ip address or dns name.the ip address of backend can configured in configuration file of frontend appl. backend acts as a single point through which all the req's would be routed.

- like serviuce associated with 3 pods. we can perform lot of things at backend(service) level like load balancing, scaling pods without having to change the frontend

- run kubectl gfet pods: we have 4 pods.2 pods are associated with nginx webserver( both pods running as appl)
- run " kubectl get pods -o wide " it will give ip address associated with all pods. connect to 1stnginx webservr pod " kubectl exec -it 1stnginxpod curl localhost " o/p : it will say it is nginx container 1. run the same for another nginxpod using curl local host . it will say nginx container 2. 

- above we seen 2 pods of nnginx webvserver an dresponding well for curl. 1 among 4 pods is frontend(name=newcurl as frointend appl).
  frontendappl= newcurl    backend = 1stnginx, 2ndnginx (2 pods in backend)

- now login to frontend appl and do curl to 1 of the ip associated with backend appl pod " kubectl exec -it newcurl curl 10.2.0.2 " assume the ip is of backend : it will say contanienr1 ( means frontend able to communicate with backend). simliar way do the same login to frontend and do curl to 2nd backend pod it will give o/p: container2. if backends got stopped the frontend will fail. thats why we will use backend.

- run kubectl get service : i can see 1 service which is cretaed( name=servicek , it has a specific ip address , and it is conneted with all the backendpods 
- tp verify run " kubectl describe servicek " : o/p : within endpoints we can see the ip addresses of the multiple pods(1stnginxip, 2ndnginxip). means this service is conncted to the 2 pods of backend appl.

- now frontend instead of directly communicating with bckend pod it can always send a req to service ip.and service ip will deal with backend pod communication.
- connect to newcurl and do curl to service ip " kubectl exec -it newcurl curl ip_address_ofservice" will get o/p : this is nginx container1. when we run the same command it will change to container 2. again run changes to container1...

- means whenevver a frontend appl is calling a service , thenthe req is routed to pod 1,it is also been routed to pod 2.in this case if pod 1 fails then service can detect and it can only route traffic to the pod which is currently running.or if there is a new pod which is created that pod will automatically 
associated with serviece.

- service can also do load balancing so that a single pod can't have a huge load.
There are several types of Kubernetes Services which are available:
- NodePort
- ClusterIP
- LoadBalancer
- ExternalName
depending on use case we can use one among them.

***********************************************************************************************************************************************************
CREATING FIRST SERVICE AND ENDPOINT :   path : frontend(pod)- backend(service)-endpoints(pods)
- we have a frontend which is a pod
- Service(backend) is a gateway that distributes the incoming traffic b/w endpoints.
- Endpoints are the underlying PODS to which the traffic will be routed to.
DEMO :  create a frontend pod and backend pod. create servuce to route traffic coming from frontend to service endpooint.
-github link: https://github.com/zealvora/certified-kubernetes-administrator/blob/master/Domain%203%20-%20Services%20and%20Networking/serviceandendpoints.md

Create 2 backend pods :
kubectl run backend-pod-1 --image=nginx
kubectl run backend-pod-2 --image=nginx

Create frontend pod from ubuntu :
kubectl run frontend-pod --image=ubuntu --command -- sleep 3600

- now login to frontend pod and try to connect to one of the backend pod from frontend pod by doing curl.
 kubectl get pods -o wide  : we will get pods with ip's.

- now create a service and associtae the backend pods with service.
- create a file service.yaml and write inside : 

apiVersion: v1
kind: Service
metadata:
   name: kplabs-service            name of service.
spec:
   ports:
   - port: 8080                      port is where service listens to 
     targetPort: 80                  target port is where the endpoint will listen to.
- run kubectl apply -f service.yaml
- run kubectl get service : we can see kplabs service, it's ip, and its port: 8080/TCP
- run kubectl describe service kplabs-service : there we can see endpoints : none ( no endpoint connected)

PORTS: i have 2 worker nodes and one pod in each node. in one pod there is appl(nginx) is running which listens on port 80.if we want to connect to this specific appl and retriev the o/p of the appl we have to specify the ip follwed by the port 80. also we have kpolabs service and 2 endpoints (2pods) and also there is a port for service which is 8080 (the port of service). when we send the request to the ip of service with its port 8080 the service will send the traffic to the target port of the endpoint( here the target port is 80)

- now copy ip of service and connect to frontend pod.do curl on service (ip_service:8080) we won't get any respinse, because currently no endpoints that are connected to it.
- connecte endpoints to service , so that service can send the traffic to endpoint and retrieve the response and send the response back to frontend pod.

ASSOCIATE ENDPOINTS WITH SERVICE(KPLABS) :
Create a file endpoint.yaml :

apiVersion: v1
kind: Endpoints
metadata:
  name: kplabs-service
subsets:
  - addresses:
      - ip: 10.244.0.23        ip address of backend pod1.
    ports:
      - port: 80
- run kubectl apply -f endpoint.yaml.
- run kubectl describe service kplabs-service : now we can see 1 endpoint. which is endpoints:10.244.0.23
- connect to frontend pod and run curl on ip of service with service port . curl 10.24.33.2:8080  : here i will get html [page of nginx(backend appl)
- if we don't send the req to port where the serviuce is listening to 80 instead of 8080 : we won't get any response.

***********************************************************************************************************************************************************
Service Type - ClusterIP :
- Whenever service type is ClusterIP, an internal cluster IP address is assigned to the service
- since an internal cluster ip is assigned to the service , that specific service can only be reachable from within the cluster
- ClusterIP is default service type. means when we create a service and if we dont define ty of service then ClusterIP service will be cretaed.

EX ; we wlready know backend-gateway(service). and it has 3 endpoints(ip addresses of backend pods). here the service also called as ip address.- we have already done curl on ip address of service. this ip address is internal ip address of cluster.CLUSTERIP: because this ip address is among the ip addresses of cluster which is internal.
- Objects within the cluster are able to access this ip address.
- above in yaml we didn't mention type of service. so clusterip service is created. run kubectl get service . we can see kplabs-service type:ClusterIP

***********************************************************************************************************************************************************
USING SELECTORS IN SERVICE ENDPOINTS: 
- How to integarte labels and selectors while configuring k8s service : till now we manually defined the ip addresses of pods to servuce. for 1 pod is fine. but what if we have 100's of pods. if we use manual we have to add 100's of ip's to service. this way is not recommended and difficult.

- So whenever client creates a service they can define that (Add all the pods with label of app=nginx) . means when service cretaes the endpoint of that service will be associated with the ip address associated with all the pods that has specific label(app=nginx)
- so by adding only appropriate selector as part of service we can select all the pods .
DEMO : 
 create a file labels.yaml :  for deployment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3             ( 3 pods using nginx image. all 3 will have label of app:nginx)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

Create a service in service.yaml:

apiVersion: v1
kind: Service
metadata:
   name: kplabs-service-selector
spec:
   selector:
     app: nginx     ( we adding label here under selector. it matches the label that has been added to pods)
   ports:
   - port: 80
     targetPort: 80

- run kubectl apply -f labels.yaml.
- run kubectl get pods --show-labels : we can see labels associated with pods.
- run kubectl apply -f service.yaml .
- run kubectl describe service kplabs-service-selector : we can see 3 endpoints  ( 3 pods that are created)

Now suddenly lot of traffic will come to our website.we want 10 pods instead of 3 pods.so scale deployment of nginx-deployment.
run "kubectl scale deployment/nginx-deployment --replicas=10" 
- now run kubectl get pods : we can see 10 pods. run get pods --show-labels : all 10 pods will have label app=nginx
- now we want to know whether all these pods are part of service or not : 
 run " kubectl describe service kplabs-service-selector ". in endpounstwe can see 10 ip addresses.
- want to see list of ip addresses : run kubectl describe endpoints kplabs-service-selector" we can see 10 ip addresses.

- i created a new pod manually and i added same label(app=nginx) to that pod.so whenever k8s sees that a new pod is created with that specific label that is part of our given selector in creating service , the  service endpointv will automatically add this endpoint.
- kubectl run manual-pod --image=nginx   ( creating new pod name=manual pod from nginx image)
- kubectl label pods manual-pod app=nginx  ( adding label app=nginx to the pod name manual pod)

- now run get pods --show-labels. we can see all 11 pods has label app=nginx.
- run descrivbe service: w ecan see 11 endpoints associated witrh kplabs-selector.
- delete resoucres:
kubectl delete service kplabs-service-selector
kubectl delete -f demo-deployment.yaml
kubectl delete pod manual-pod

***********************************************************************************************************************************************************
SERVICE TYPE : NODE_PORT 
- NodePort exposes the service on each node's ip at a static port.
- we will be able to contact the nodeport service, from outside the cluster, by requesting <WorkerIP>:<NodePort>
- we have a service kplabs-service and it has 1endpoint(pod) in worker node1. since this ervice has clusterIP, whic ic internal Ip. so we won't be able to connect to this service from outside world.But we want outsid eworld to visit our website.Since clusterUp won't allow the external communication it.
- now i want outsiders9clients) to connect to service. this is where nodeport comes.

- now i created a service of type NodePort (service name=lplabs-service , it has 1 endpoint, the service is cretaed in worker node2). now on the worker node 
a new nodeport will be created. now anyone externally wants to service they have to connect toworker ndoe2 public ip followed by NodePort. and the req will now be routed to NodePortservice(kplabs-service)

DEMO :  create a pod, name NodePort-pod and label=publicpod from nginx image.
- kubectl run nodeport-pod --labels="type=publicpod" --image=nginx
- kubectl get pods --show-labels. we can see pod has label.

create a service of type nodeport in file Nodeport.yaml:

apiVersion: v1
kind: Service
metadata:
   name: kplabs-nodeport
spec:
   selector:
     type: publicpod
   type: NodePort           ( explicitly specifying the type of service as nodeport type )
   ports:
   - port: 80
     targetPort: 80
- kubectl apply -f nodeport.yaml. 
- kubectl get service  we can see our service and it has type: NodePort  , in ports: 80:30946/TCP ( but in cluster ip serviec we only see 80/tcp in port), but in nodeport we also see nodeport in nodeport section.

- so if we want to connect to this nodeport service we have to get the ip address of the workernode followed by the nodeport 
- get the public ip od worker node : kubectl get nodes -o wide : in exiternal_ip:12.39.30.3 this ip is associated with worker node.
- in broweser "12.39.30.3 " run this we wont get anything. here we also have to mention nodepport then only the req will transfer to the serviuce iunsid node. in browser " 12.39.30.3:30946 " we will see wlecome to nginx.
- this nginx page is coming from the pod which is nodeport-pod that is running from image of nginx

***********************************************************************************************************************************************************
SERVICE TYPE: LOAD BALANCER 

- LB makesuse of nodeport service as part of its overall integration.
- We know that NodePort ServiceType will assign a port in all the worker node which can forward the traffic to the underlying service.
- Challenge in NodePort: We need to access it via IP/DNS:Port
 whenever we want to open a website or appl always we have to specify either the Ip of worker node followed by node port. ex : google is making use of nodeport service ,so if we want to open google.com appl, then not only we have to specify dns name of google or ip addr,we have to mention nodeport also(31514) : google.com:31514. nobody can remember this. This is not suitable for prod env.To overcome this we have another service Load Balancer.

- In load balancer service type a new external load balancer gets created automatically.This Lb takes care of routing req's to the underlying service.

EX : client sent req to gogle.com. it goes to LB.now this LB is integrated with nodeport service.LB knows that any req it recieves it needs to forward it to any specific nodeport which is(any port ex: 31515) and this node port behind associated with the service , and service integrated with pod.(page no : 6)

- so when client makes request on domain it goes to LB and data is fetched from pod and send back to client.We can say LB acts as a gateway to the nodeport of service and tat of a client. This is recommended way for prod env.

Demo : 1st we create a pod(lb-pod), and it has a label(type=lb) , image is nginx.
- we create a lb service , and we giv eselector is type=lb, this will select our pod(lb-pod). we give service type=loadbalancer.

- kubectl run lb-pod --labels="type=loadbalanced" --image=nginx

-nano elb-service.yaml :

apiVersion: v1
kind: Service
metadata:
  name: kplabs-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    type: loadbalanced

- apply now "kubectl apply -f elb-service.yaml" .before applying this in aws , check load balancers in ec2 section zero lb. now apply the selb-service.yaml     
  and run get service, we can see kplabs-lb running.
- get service 

  NAME            TYPE            CLUSTER-IP                     EXTERNAL IP                PORT(S)          AGE
 kplabs-lb     Loadbalacer        10.243.23.10                   68.262.829.780            80:30268/TCP       8 sec

- Npow in aws an external LB automatically gets created.and this LB will automatically integrated with nodeport which is(30268) and the req will be routed accordingly. now copy external ip of lb and paste in chrome, we see welcome to nginx page.

- open lb settings in aws.we see forwarding rule " TCP on port 80 -> TCP on port 30268 ".it directs from port 80 to port 30268(which is nodeport associated with kplabs-lb).Means any traffic that comes to port 80 of LB , direct it to specific nodeport within the worker node.
- instead of specifying ip, we can map dns to the ip , so whenever a user enters domain name,he will automatically see our website.

- The overall implementation of LoadBalancer depends on your Cloud Provider.if we are using managed-k8s service, and if we define managed service type of LB , automatically the integration is already there.(look into k8s architecture in D1 Coreconcepts page 11).in master we have a cloud controller manager,and it is basically connected to specific cloud env.since we are using managed k8s cluster based on AWS , and integration to AWS LB is already there.when ever we create a new type of service type of LB , an external LB will be created in cloud provider AWS

- If you plan to use it in bare-metal, if we are using a plane server , in lb service it wont create lb ,then you will have to provide your own load balancer  implementation incase if we are running k8s in bare metal env.

- finally delete all resources :
kubectl delete pod lb-pod
kubectl delete -f elb-service.yaml

***********************************************************************************************************************************************************
GENERATING SERVICE MANIFESTS VIA CL :

- in cka not ny we have to solve qstions, but we have to solve them within time.Time management is primary challenge in CKA.so thats y we have to find quicker solns to solve Q's.
- in exam if we get a Q related to service, we go to docs, we find yaml ex's of services, we copy and modify it. but this process takes time.so now we check how to generate manifest files via CLI.

Generate Service manifest for pod : 
1. kubectl expose pod nginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml     : we exposing a pod nginx,this nginx pod will be added as a service endpoint.then we gavce name of service as nginx-service, gave target and port,if we dont have dry run and o/p, this will immideatly create the objects mentioned in command.( dry run and o/p generally used to show the manifest that will be created)
the above command will create a service,we have selector of run:nginx.

2. i want to create a service but of type nodeport, the above command remains same, but additionally we have to specify --type-NodePort.
command : kubectl expose pod nginx --name nginx-nodeport-service --port=80 --target-port=80 --type=NodePort --dry-run=client -o yaml

3. i want to expose a deployment : we say k expose depl followed by name of depl(earlier we want to expose pod so we gave pod) and remain same as above.
- kubectl expose deployment kplabs-deployment --name nginx-deployment-service --port=80 --target-port=8000

DEMo : create a pod, and deployments via yaml files, then we see how to generate manifests via cli for services.

-kubectl run nginx --image=nginx,run it says pod created.
- also create a deployment with nginx image.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend     we gave label as tier:frnt for pods in depl
    spec: 
      containers:
      - name: nginx
        image: nginx

- save anmd apply, get deply we see one deplyoment.also get pods we see 4 pods.but for above nginx pod label(run: nginx), for depl pods tier=frnd label.
- create a  service to add nginx pod of run:nginx.
- expose nginx pod to the service nginx-service port and target
 kubectl expose pod nginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml  --dry-run=client -o yaml , ther we can see the selector which is (run : nginx). here we didnt specify type of service we want, so default type of cluster ip will be created.

- kubectl expose pod nginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml  --dry-run=client -o yaml > service-o1.yaml. here in yaml we can modify and apply based on our needs.run describe service we can see nginx pod as endpoint.

- create service type of nodeport 
kubectl expose pod nginx --name nginx-nodeport-service --port=80 --target-port=80 --type=NodePort --dry-run=client -o yaml : here w ecan see service type nodeport,and save this to any yaml file., and applky. run get service we cans ee one nodepprt service.

- run et deployment, we have kp-depl. now wxpose thius depl to service.
kubectl expose deployment kplabs-deployment --name nginx-deployment-service --port=80 --target-port=80  --dry-run=client -o yaml   : below we cansee seector tier :frontend  , create it and describe ng-depl, we see it has 3 endpoints(pods)

***********************************************************************************************************************************************************
OVERVIEW OF INGRESS : 

Challenges with LB servivce deployment : Whenever making use of the LoadBalancer Service Type, out of the box, you can make use it for a single website.
we have LB, lb directs traffic to service,our service intern will direct traffic to prod.if we have one dns(example.com) we pointed this to our ip of LB , then whenevr user types example.com in browser , he see appl running in pod.

- Challenge happens when we have multiple websites or multiple env's within k8s cluster.
- use case : we have LB.however we have 2 services(ex-service, kp-service), one pod associated with each service. so 2 diff appl running.it might happen that these are 2 completky seperate env's within same k8s cluster. Now we have 1 LB only. what we want is whenevewr a user typoes example.com , then the req from LB should be routed to example service which open up the appl associated with it.same for kp-labs service, whenever user types kp.com, the our lb should redirect the traffic to kp-labs service which is running completly diff appl.

- So in short  we want a single LB , and that LB should be able to support N number of websites or appl's which are configured within k8s cluster.the problem is if we work with layer 4 LB , this is something that can't be avhieved out of box.so org has to create multiple Lb's.
- ex : we have 2 lbs.1 for example.com and another for kplabs.com.( which will forward to thise services acc=pordingly). This is the way we have to work if we want to have multiple websites within the same k8s cluster provided by using a service type called LB.Here pricing is concern.each lb costs more. also if our cluster has 10 appl, so we need 10 diff LB's.

- So this is why in k8s there is an intermediate gateway , this gateway has a intelligence to do layer 7(application LB) related routing.(PGE NO : 7)	
ex : we have 2 domains(ex.com, kp.com). client will send req to lb from one of the domain ,now lb send req to gateway layer.so client req ex.com , kb send to gateway layer,gateway layer will look into host,now depending upon the rule that we have set at this gateway , thew req will be directed to ex.com service.same for kp.com, it goe to lb, lb directs it to gateway layer,gateway wiull forward that req to kplabs service.the intelligence on layer 7 is present on the gateway.
* this is referred to as INGRESS.

DEMO :
run "kubectl get ingress " we see one ingress name(k-ex-ingress) and it has hosts (website1.com and website2.com)
run describe ingress k-ex-ingress : inside we can see rules where (if host has web1.com then it will forward the traffic to kube-1-service,  if host has web2.com then it will forward to kube-2-service which is different). we can see each service has multiple pods associated with it in baceknd.
- means 2 diff env running inside cluster, depending upon dns name which the user enters,our ingress will route the traffic to right env.

- with ingress even if we have 10's of environments we can make use of single load balancer.LB will forward the traffic to ingress controller.

*** Kubernetes Ingress is a collection of routing rules which governs how external users access the services running within the Kubernetes cluster.
  Ingress can provides various features which includes:
- Load Balancing
- SSL Termination
- Named-based virtual hosting

***********************************************************************************************************************************************************
UNDERSTANDING INGRESS RESOURCE AND INGRESS CONTROLLERS : 

There are two sub-components when we discuss about Ingress:
- Ingress Resource(ingress rules)
- Ingress Controllers

- Ingress Resource contains set of routing rules based on which traffic is routed to a service.
  ex : earlier voideo we saw when we search web1.com it directs to a specific service.same when i do for web2.com it directs to diff service.So all of these rules that governs on how the request packet would be routed is part of ingress resource.

- Ingress Controller takes care of the Layer 7 proxy to implement ingress rules.
  You must have an ingress controller to satisfy an Ingress. Only creating an Ingress resource hascno effect . means when we just create a rule,it will not really have any effect,beacuse we need someone to implement that specific ruleset.
 ex : we have a firewall rule which blocks ssh traffic from internet,so just having the rule is not enough,we need to have a firewall where that rule will be implemented.in a simliar way ingress resource is nothing but a rule,however ingress controller is the actual impelemntation which implements the rules that governs and define within the ingress resources.

- PAGE NO(8) : we have ingress controller in b/w LB and other services.lb will forward the traffic to ingress controller,and we saw how controller route traffic to example service when evern name is ex.com, when kp.com it routes to kp serivce.

How does controller knows that ex.com should be routed to ex service, and kp.com should e routed to kp-service.So this we rules thatwe set are part of within the inress controller are part of ingress resurce(ingress rules)

-controller has rules :
 rule              Host           service          
1             example.com     example-service
2             kp.com          kp-service.

(rule1 is ex.com, it is redirectijg to ex service, whenever controller recieves a req which has a host of ex.com within rule 1 it already knows that it should forward it to ex-service.  same for kp. this is fxn of controller.
- controller should be able to read the packets.when we talk about layer 7(appl layer)(.com, .in are part of host header within the http req packet)so controller should understand this, and it works on layer 7.

DEMO : open web1.com (when we search google for our web1.com using lb with ingress). inspect on that website.there under req headers we see (host header host:web1.com) . Typically since LB do not operate at layer 7,we do have lb's lik ALB in aws which operates at layer 7 however when we talk about  CLB or NLB they dont really operate at layer 7 hence they can't really look into headers filed field.So what they do is they outrightly send all the req that the LB recieves to the ingress controlelr.nowingress controller intern can work at layer 7, so it can look into http headers. then it will look into rules which are specified in the ingress.Now depending upon rule ingress controller will send the traffic to appropriate service.

- K8S currently supports and maintains GCE and nginx ingress controllers. So there can be lot of ingress controllers from diff org's.But many org's uses nginx controller exstensively.
- You must have an ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. like we just create ingress rules it will have no effect,we need to have a ingress controller which can read our ingress rules and then it can forward the traffic accordingly.

-run get ingress, and we get one, and describe it. insoide that we see 2 rules  , and any req that goes to web01 host header, and itneeds to be directed to a service called web1 service.same for second also.

- run get pods we see ingres controller runing. and get service we see one nginx controller service of type LB and it has external IP.So generally whenever we deploy ingress controller of nginx in AWS , that inginx controller will automatically create a LB and it will ask LB to forward the traffic to the port where the ingress controller is running.
we can see controller is running on 80:302254/tcp(nodeport). means any req that comes to LB on port 80 , will be forwarded to 30254 and so on.this config can be achieved automatically with the help of ingress controlller,depending upon the platform that we use.

***********************************************************************************************************************************************************
OVERVIEW OF HELM (package manager for kubernetes) :

- Helm is one of the package manager for Kubernetes. till now we have been working m=with multiple k8s objects(deplymnts,services,ingress,config) individually. when we work at individual level it is fine.However in prod env when we discuss about appl, appl will use a lot of k8s objects.It can make use of depl,service,controller,volume etc.

- Lets say we have appl A , and it is using lot of k8s objects(depl,service,controller,volume etc), now we wont go and deploy objects individually it is a bit of pain. so what we do at prod env we combine all of the objects that we will be creating as part of the app A together and this is referred to as Helm Chart. So we have helm chart of App A and this chart has all of the k8s objects required to deploy , and makesure appl A up and running.So we go and deploy helm chart in k8s cluster , now we can have multiple cluster but it doesnt matter, once we have k8s objects defined in helm chart, using our helm client we can deploy it to x amntof clusters.this is how easy to deploy our entire appl in prod env.

- one great thing about helm chart is repository.the appl that we discussing , it can be created by us or diff org's.lets say i want to deploy a wordpress based app on k8s cluster,now if we look into various repo available for helm,tere might be a wordpress app that is already available.we can use that app and deploy in our cluster, and now our wordpress is ready.similiarly there is jenkins chart available, and we can deploy , we can have jenkins running.

- in artifacthub.io here we can browse various helm packages that are available in various public repo's.i want to deploy mysql and search forit,we can see multiple sql packages available from diff providers.i can also do for jekins,we can see official jenkins helm chart package provided by jenkins.so at this stage if jenkins create deply's,secrets,config maps, vol's we dont really need to worry. all we have to take that jenkins chart , and using helm deploy on cluster.

Demo : in artifact take wordpress chart and deploy in k8s cluster.
- in cli run "helm install demo-release bitnami/wordpress"  demo release is local name,we using wordpress chart that is available and the provider is bitnami.now run it , it shows "status:deployed", at also says it takes g=few minutes for the LB ip to be avaialable. because wordpress chart might be creating various k8s objects behind scenes to check go to (github/bitnamicharts/tree/master/bitnami/wordpress check inside templates we cans ee deply,serv,configmps have been used by wordpress). now if want to launch wordpress we can create yaml for every objects,or we can launch it using available helm chart. 

- run get pods we can see 2 pods running.one is mariadb, it is deployed a DB also , along with another pod wordpress which is appl itself.in aws also check a LB has automatically got created , copy the ip of LB and paste it in chrom we can see wordpress inchrome.
- now to delete word press and unistall wordpress helm chart run " helm uninstall demo-release". run get pods , we see no pods and all of the associated objects with wordpress will be deleted.

- All the Helm charts are stored in a central repository through which charts can be pulled and deployd.
- A Chart is a Helm package.
- It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster.
- A Repository is the place where charts can be collected and shared

INSTALL HELM : 

- we need to install helm binary within our workstation.similaryly how we created kubectl binary which we used to interact with our cluster.

***********************************************************************************************************************************************************
DEPLOY OUR FIRST HELM CHART (Deploy our appl) : 
- to install helm package in k8s cluster , first we need helm package, lest say we have appA package which containesvarious k8s objects defns.now we have helm client available in our local system.using helm we can install helm command,and we have to specify the name of the helm package, and helm will deploy that package in cluster.

Helm package locn : Helm packages can be placed in local disk or can also be stored centrally in public or private repositories. Depending on the location of the package, there is a slight change in the installation step. But helm install command remains to be same, but incase if we have a helm package available in a remote website then we also have to tell helm the url of that website,from which thishelm package can be retrieved from,

- we have a public repo it has a word press package.first we have to configure the url of the repo in our local workstation.helm repo add public repo URL.after that we can use helm install command to deploy the packages in that public-repo.

Demo : in artifacthub search jenkins,as we discussed 1st we have to add repo in cli so run "helm repo add jenkins https://charts.jenkins.io" and run "helm repo update". then run " helm install any_name jenkins/jenkins" . we see the status : deployed.when we run "helm list" we see we have one helm(any_name)
and status is deployed. to unistall run "helm uninstall any_name"  . now our entire jenkins appl is uninstalled.

- also install wordpress from bitnamiinartifacthub. run " helm install my-release oci://registry-1.docker.io/bitnamicharts/wordpress", now run " helm install wordpress-name bitnami/wordpress " run it we see wordpress deployed. So depending upon the appl we deployed it takes diff time.
-we can even change the parameters in that helm packages. run get pod we see 2 pods (db is running but wordpress is not running). now due tosome reason wordpress pod is in pending state.now we think that bitnami might have written packages incorrectly.however before we think like that we have to check logs also. run " k describe pod wordpress" : ther we see failked scheduling in node, 0/1 node available, but due to insufficient CPU". in aws LB is created but is down.

- whenever we dpoloyng appl within the cluster , the cluster node must have enough CPU and memory for the appl to be deployed. so change nodes to big size and install wpordpress helm chart, we can see the appl got deploeyed and wordpress is up and running".

- Helm Commands :
helm repo add : to add a chart repo
helm install package : to install any helm package
helm uninstall package : to uninstall any helm release
helm repo list : list of repo's names along with the repo URL.
helm list : list the releases.

***********************************************************************************************************************************************************
CREATING INGRESS RESOURCE :
- how to deploy ingress resource on k8s cluster :
we alreadyseen how ingress resource contains set of rules which are asociated with ingress controller,once they both created then depending upon the hostname based on which query is made towards controller or Lb the traffic can be routed to diff serviuce an dits pod.

- when client open ex.com req will be riuted to ex-servoice, same for kp.in it will be routed to kp-service. So this k=setup in which traffic is routed towards multile services based on a host is also referred to as a Name Based Virtual Hosting 
 Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.

Demo : setup 2 services, and setup 2 pods for each service,and we need certain rules for ingress resource.
- " https://kubernetes.io/docs/concepts/services-networking/ingress/ " if we scroll down we see the diagram for multipl services under name based virtual hosting. in the yaml file they gave 2 hosts :
fie name : ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:                            under rules we have 2 rules host1,2. 
  - host: ex.com                   whenever req comes to this domain this should route to service1
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1       this is service where the req will be routed.
            port:
              number: 80
  - host: kp.in                  whenever req comes to this domain this should route to service2
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80

- Now we need 2 services, and these services should associate with set of pods.1.create pods.2.create services.after that we create above ingress resource.
Create 2 pods :
kubectl run service-pod-1 --image=nginx     this will associate with service1
kubectl run service-pod-2 --image=nginx     this will associate with service2

-create 2 services:
kubectl expose pod service-pod-1 --name service1 --port=80 --target-port=80    created 1 service for pod-1
kubectl expose pod service-pod-2 --name service2 --port=80 --target-port=80    created 1 service for pod-2
- run get services we get 2,with type cluster-IP and its address. now verify if we access serviuce will we get nginx page. fr that create one more pod frontend with image ubuntu.
- kubectl run frontend-pod --image=ubuntu --command -- sleep 36000     ; here we gae command to run in container is sleep 36000.
- login to front pod using " kubectl exec -it frontend-pod -- bash " here send a req to service-1, 2 Ip's. in front pod install curl and nano run " apt-get update && apt-get install curl nano " now "curl ip of service1"  it will show nginx page as response.do same for ip associated with service-2 we get nginx page.
- Now services are ri=unning along with pods. now we have to create ingress.

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:                            under rules we have 2 rules host1,2. 
  - host: ex.com                   whenever req comes to this domain this should route to service1
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1       this is service where the req will be routed.
            port:
              number: 80
  - host: kp.in                  whenever req comes to this domain this should route to service2
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80


- save the file and apply.run get ingress we can see 1 with name "name-virtual-host-ingress" and hosts ex.com,kp.in
- run describe ingress , there we see rules,under it hosts,we see ex.com will be routed to service01, and kp.in will be routed to service2.

- Now we have service,pod, ingress resource.  next we create actual ingress controller and we see how LB will be connected to ingress controller.

***********************************************************************************************************************************************************
DEPLOYING INGRESS CONTROLLER :   "  https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/   "

- in the web page we can see 'Kubernetes as a project supports and maintains AWS, GCE, and nginx ingress controllers'
- now we deploy ingress associated with nginx.  " https://kubernetes.github.io/ingress-nginx/deploy/  " here under quick start we can see few commands . here helm will take care of installing the appropriate resiurces req for the ingress controller.

- helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace

- make the above command as linear because in cli(\) this wont work so :
- helm upgrade --install ingress-nginx ingress-nginx  --repo https://kubernetes.github.io/ingress-nginx  --namespace ingress-nginx --create-namespace 
 run above command and check by " helm list --all-namespaces"  we can see helm chart "ingress-nginx" deployed.

- run " kubectl get ingressclass "  we see under controller "k8s.io/ingress-nginx"
- run "kubectl get service -n ingress-nginx" this will show the service in the namespace of ingress-inginx.there we can see a new service is created with the name of nginx-ingress-controller and the type is LB.Now go to aws we see one LB which is created. 
we discussed when we deploy ingress controller it will automatically deploy a new LB by createing a new service type of LB.( the LB integrated with ingress controller)

- we are using ingress controller which has a name of nginx . So in ingress resource we need to specify this.
- in ingress resource under spec section we have to add one more field 'ingressClassName' and give value as nginx. (When you create an ingress you would need that name to specify the ingressClassName field on your Ingress object )

Now go and re apply the yaml file of ingress.yaml
- run describe ingress name-virtual-host-ingress  : under evcenst we see 1 event from nginx-ingress-controller which is scheduler for sync. So now we have ingress resource and controller ,services and pods are running. No check request by quering LB endpoint which is in aws.copy ip of LB and check in chrome it says " 404 not found".

 in ingress.yaml we explicitly specified the 2 rules associated with hosts(ex.com, kp.in). now since we are not directly querying with the given hosts ex.com,kp.in,thats why we are getting that error. So connect to frontedn pod using exec -it bash : inside pod we make manually etc host entry 
 run "nano /etc/hosts" there add one more entry by
 "LB_ip ex.com kp.in "     herev ex, kp are entries for that LB.  save and exit from this hosts file. so whenever we make a query to ex.com or kp.in  this will go to LB's ip address. since this is internal it wont be resolvable and hence we have made a host entry.

- in frontend pod run curl ex.com we see nginx page, also do curl kp.in we get nginx page.
- so login to both nginx pods using exec, and got to /usr/shar/nginx/html : here change index.html byu using echo "web1" index.html same for web 2 using exec and make it as web 2 in index.html. now exit from pods and login to frontend pod, and do curl for both hosts, we cans ee web1, web2 as o/ps.

- run helm list --all-namespaces thi will show ingress nginx, unisntall using "helm uninstall ingress-nginx -n ingress-nginx"

***********************************************************************************************************************************************************
KUBERNETES NAMESPACE (security aspect) :

- Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces.
  We have a single physical cluster , we have created 3 namespaces(virtual clusters). each virtualcluster(VC) is associated with team. VC1 with team A, ..same for VC with Team C.
Benefits : there is a user belongs to team A,he wants to perform oprns related to pods. So he can be assigned permissions associated with creating,deleting..pods in VC1 associate with team A. so he can create and delete pod but only within a Namespace associated with team A.he cant do same thing in team B,C. This is like subgrouping of k8s cluster into multiple small Virtual clusters , where we can have granular levelof access control.

- Use case : provide all the users in team A to full access for pods resources only.(only for team A NS). here what i want is dev from team A should not be able to delet pods which are running by Team B vice vera.we can even set a pod in team A to not to connect from Pod in team B. so we can even set network policies using Namespaces.

Demo : run k get namespace,we can see multiple namsepaces. So whenever we do a command like kubectl run and if we dont specify which NS , by default our object get created in a Default NS.

- run get pods we see out. so this o/p only comes from default NS.If we have pods running in diff NS, then just by running get pods we wont get that o/p.
- run get pods --namespace kube-system . we see a list of pods in NS kube-system.However none of these pods were listed when we did kubectl get pods.
So by default whenever we create object like pods and we dont mention NS , Then it will take value form Default NS.

- we can create NS " kubectl create namsepace teamA " it says NS created.
- kubectl run nginx --image=nginx --namespace teamA : this will create a pod(deply) in teamA NS. run get pod --NS teamA we cans ee out nginx pod.

- this way we can have virtual clusters.and we can have granular permission .to TeamA within our org we can give all user access to manage pods only in thei NS.same for teamB.

- generally we see 4 NS available.

default : when we create k8s object within k8s cluster and that object can be associate with a NS default if we have npot explictly defined any NS.
Kube-System : the NS if K-s is for the objecte which are created by K8s system.
- Check PG.13 for diff types of NS's.

***********************************************************************************************************************************************************
OVERVIEW OF SERVICE ACCOUNTS(SA) : 

- when we discuss about cluster there are 2 types of Acnts :
1.user acnts ( meant for humans) : if someone wants to connect to k8s cluster and want to perform oprn we can classify those as user acnts.
2.Service accounts(for appls' or some processes that wants to connect to k8s cluster to perform some oprn)
- we know if we want to connect to k8s cluster ,whatever entity is connecting it needs to have some kind of auth cred's.
  ex : user want to perform oprns on cluster,he needs some token or cert. Similiarly any pod or process wants to connect to k8s cluster again some kind of k8s creds needed these comes under service acnts.
- run get pods we see o/p. now how this command get succeful, because of authentication.behind scenes i have kubeconfig that is available,which has all creds that my kubectl client uses to auth to cluster. 
- open kubeconfig there we can see a token for user,this toiken is used by kubectl to connect to cluster.

- there are diff auth cred's: tokens,certs,

- when we launch k8s cluster , it has various components(schedule,api,contrlr,etc)
- various components of k8s uses service acnts to communicate with the cluster. to check run get serviceaccounts --all-namespaces : we see a list of service acnts that has been reated.behind scenes SA are used perform lot of oprns through machibe entities.

- lets asume we have a SA  and it is associayed with pod A.inside thi pod there one appl or service.Now pod A can use the token that is associayted with the SA to perform some actions on k8s cluster.( means when SA associated with pod, this pod will recieve some kind of token , similiar to how we saw the config file that we had, it had a token) . similiarly even the pod when it gets assocuated with the SA it recieve a token,and through that pod will perform some oprns on k8s cluster depending on what permisision token has.

- create a pod name poda using nginx, run get pods we seen poda running.run describe poda.there we see a SA:Default.so whenever SA associated with pod an appropriate token will also be added as part of pod, the appl running as part of pod can use token to authenticate to k8s cluster to perform some oprn. now login to pod using exec -it . go to "cd /var/run/secrets/kubernetes.io/serviceaccount " run ls we see 'token' we use this to connect to k8s cluster.cat token it looks like ec2 key pair format.any appl running inside pod can use token and it perform auth to cluster to peform some oprns depending upon permissions it has mentioned in token.


- for simplicity run "token=$(cat token)" because when we do cat token our token gets displayed.so now when we do echo on token we can see token "echo $token"
- now in diff tab out of pod run "kubectl cluster-info" : we see k8s control plane running on https...aws.com this url" . copy url 
- now go inside pod sam in /SA locn and run " curl -k -H "Autherization: Bearer $token" https...aws.com/api/v1 "  (-h=header inside header auth= we used token, instead of mentioning all token data we just gave token$) . now we got o/p. we are getting because the token specified using that cli succefuly able to fetch cluster data.. 


- Now remove $token in above command and give somw wrong name and run, we get error saying that unautharized, not able to authenticate to k8s cluster using curl command because auth is not there.

- SO appl can create new set of pods, means whatever SA token that it is able to fetch from this dir "cd /var/run/secrets/kubernetes.io/serviceaccount" must have permissin to create pod.

SERVICE ACCOUNTS -PONTS TO NOTE : 

- Whenever we create cluster,k8s autometically creates a SA object named default for every NS in our cluster.if we have 10 NS each ns will have SA of default.
- run get sa we see one def already creatd.
- also check sa in NS of kube-system " k get sa -n kube-system " we get many list including default.whenever we create a ns associated with ns a sa will also get created."k create ns kp-tes" , k get sa -n kp-test. we get default sa.

- Each SA in k8s can be associated with certain permissions. for ex we have sa,we can assign permsiions on what the token associated with sa is allowed to do like ' whether this sa token can read the logs or full access to pods, create pods, delet pods'

- When pod uses the sa,it can inherit permissiosions what ever sa has.

- The default service acnts in each ns get no permissions by default other than the default api discovery permissiions that k8s grants to all authenticated principals if role based access control(rbac)is enabled. means we took token as part of pod to do curl on k8s cluster , when we did curl we got some o/p. this is what we referenced as "it states apart from api discovery permissions that we were able to as part of curl command no other permson is granted as default.'using token apl cant create pod, delet, or any other objects as part of cluster.'

-Assigning pods to SA : if we deploy a pod in a NS, and if we dont manually assign a SA to the pod, k8s asign a default SA for that NS to pod.
 we create a pod by run. that pod will have default SA that will be associated with it unless and until we speicfy other SA that needs to specified.
create ns kp-test,get sa kp-test, we get default. now asign a pod to ns kp-test. get pod in nkp-test we see. now describe pod of ns kp-test we see under sa : defau;lt is assigned. also when pod described we see under mounts :/var/run/secrets/k8s.io/sa here our token is assigned.

SERVICE ACCOUNTS PRACTICAL SCENARIOS :

-Mounting custom SA on pod : we know if we create a pod without specifying SA, a default sa will be attatched.what id i dont want default service acnt,and i want to add custom SA then :
- sa.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
spec: 
  serviceAccountName: custom-token

- run get sa we see 1 def only. create 1 sa " k create sa kp", run get sa we see 2 sa's. now mount custom sa on pod.
- run above yaml by giving image in container section. describe pod we see sa : customtokem

***********************************************************************************************************************************************************
NAMED PORT (Networking aspect):

- whenever we write specification(spec) for pod , wher as specific port is exposed , 
till now 
spec:
  containers:
  -  image: nginx
     name: mywebserver
     ports:
     - containerPort: 80
- along with this we can also give name for the port for container port.
spec:
  containers:
  -  image: nginx
     name: mywebserver
     ports:
     - containerPort: 80
       name: http               ( this is refered to as named-port)
- beneift of named port : it can be referencd within service file.

DEMO :
- create a pod with name nginx : run " k run nginx --image=nginx --port=80 --dry-run=client -o yaml"  we get yaml but under spec sect we see only containerport, but it doesnt have any name.now create this pod by remvig dry run.
- also create a service " k expose pod ngix --name servc --port=80 --target-port=http --type=NodePort" run it and servc sercice is created.( above servc port is 80, and pod port is http).run get servc.we see servc is running on node port "80:3227/TCP"
- also  describe servce, we see protocol is tcp,target port is http,
- run get nodes -o wide.we get ext ip's.since we have nodeport service,if we send req to ext_ip(public ip)on nodeport will see it work or not.
run " curl ext_ip:3227 " we get connection refused failed to connect.

Now we give namedport and expose serviec on that : for pod 
spec:
  containers:
  -  image: nginx
     name: mywebserver2
     ports:
     - containerPort: 80
       name: custom-http     ( now we added namedport. in above we havent added this) 
- k expose pod ngix2 --name servc2 --port=80 --target-port=custom-http --type=NodePort run it our service ois created.get service we see 80/32435/tcp  , serv listening on nodeport , now do curl on_ext_ip:nodeport : we see nginx page,working perfectly.
- describe servc2 : we see in target port instead of giving port num we gave name "custom-http/tcp"

***********************************************************************************************************************************************************

DOMAIN 4: SECURITY 

***********************************************************************************************************************************************************
UNDERSTANDING AUTHENTICATION(auth) IN K8S :

- Authentication is the process or action of verifying the identity of a user or process
 Use case : we have k8s cluster on one side and user on one side.user send a command to delete a pod name app. now k8s cluster verify whether the user is authenticated, if he is authenticated, then what are the permissions he has.
ex : in our dir go to config file of eks in .kube dir.open it  we find url of eks cluster under server. in cli do curl to that url , we get erro stating that status:failuer,fobiddetn,vant get path.
- so when we send req to k8s cluster without authentication depending upon the permissions we have in k8s cluster and overall config the user can be asked to auth first.

- Users in K8S : Kubernetes Clusters have two categories of users :  1.Normal Users 2.Service Accounts
 1.Normal Users : Kubernetes does not have objects which represent normal user accounts
  ex : new person juoined in devops team. we have to give k8s cluster access to that guy. here we cant directly create a user into k8s cluster(kubectl create user alice : not this way because Kubernetes does not manage the user accounts natively.) So how we can give access to k8s cluster : 
There are multiple ways in which we can authenticate. Some of these include:
- Usernames / Passwords.
- Client Certificates
- Bearer Tokens

- https://kubernetes.io/docs/reference/access-authn-authz/authentication/    we can see :various ways in which user can access cluster.
X509 Client Certs : certain certs used to access the cluster.
- we also provide token nane along with users and few details
- lot of methods available specifically related to tokens.

- in aws eks cluster kube config file: inside we see a user and also a token there.when we run kubectl command to connect to cluster,the token will be provided for auth process.

- dependimg upon the k8s provider that we use , it can happen that way of authentication changes.
- also some times there will be certification tokens available.

***********************************************************************************************************************************************************
AUTHENTICATING WITH K8S USING TOKENS :

DEMO :  create 1 SA name external. create 1 pod associated with that SA exter, and image nginx.
- login to pod using bash, and run "ls -l /run/secrets/k8s.io/SA/" here we see token associated with this pod.now copy this token and try to connect to k8s cluster. now we have to encode or decode the token. i chrome we find many site for it.token.dev is that. copy token here and decode it. now we can see in payload : name :pod, sa :ext, user : sa-s

- whenever we have token and we want to do curl to cluster then in command we have to add specific header "Autherization: bearer  token"
"curl -k URL_aws_EKS/api/v1 --header "Autherization: Bearer  token" "

***********************************************************************************************************************************************************
OVERVIEW OF AUTHERIZATION(auzn) : Autherization is the process of giving someone permission to perform some spec operation.
EX : AWS : in aws acnt we created a new user.Now new user will go and try to delete all servers , now aws will respond back 'you dont have permssions" (so this specific aspect related to what permission a specific user has or what permissin user doesnt have.all this is part of auzn process )

- in aws when we create a user, that user doesnt have any permissions if and only if we explicitly specify.
- in k8s there are multiple k8s auzn modules available.
  AlwaysAllow
  AlwaysDeny
  Attribute-based Access control(ABAC)
  Role-BAsed Access Control(RBAC)  : most used, we also use in prod env.
  Node 
  WebHook

RBAC : two imp components we need to know.
- A role contains rules that represents a set of permissions.
- A role binding grants the permssions defined in a role to a user or set of users.

EX : a user joined team. curently he doesnt have permsiions to perform any oprns on k8s cluster. so in order to give permissions to this user as part of RBAC we need to create a new role.we know role contains rules that represents permissions. so now create role :

 role readSome.allow : Read Pods; Read Services;REad Secrets    ; user has ready only for 3 services.After creating role we have to make role binding.So just by creating role we doesnt know whom this role applies to.so in order to defiene a user for role we have to create a role binding.

Role Binding :                 ( in role bindimg we define 2 things( role and the user (for whome the role is associated wiuth)
readsome
  - zeal

DEMO : https://kubernetes.io/docs/reference/access-authn-authz/rbac/      here take role example

- run " curl -k https://our_eks_cluster.com/api/v1 --header "Autherization: Bearer lkddksqgjksag  "    we are doing curl on eks cluster using bearer token that we fetched from pod. after running this we can see content succesfully.

- curl -k https://our_eks_cluster.com/api/v1/namespaces/default/pods --header "Autherization: Bearer lkddksqgjksag   "  here we added NS of default , and we have pods there. Now this is trying to list the pod in default NS. run it we see error " this user(system:serviceaccount: default:external\) cant be able to list the resource of pod in NS default. this user althoiugh able to perform simple oprn on the api version1, but apart from that the user is not able to do anything here.

- So now we want this user (system:serviceaccount: default:external) to be able to perform list operation on the default NS for the resouce of pods. if everything goes well when we run above curl command we should be able to see list of pods
- in role.yaml : 
apiVersion: rbac.authorization.k8s.io/v1
kind: Role      ( kind is role)
metadata:
  namespace: default     ( define NS where role will be created
  name: pod-reader       (name of role)
rules:                    ( we have rules section)
- apiGroups: [""]
  resources: ["pods"]      ( we have resources in rules section, and resources is pod)  resoirces might be secrtes,pods etc
  verbs: ["list"]         ( for the resource of pods what are the actions should be allowed)whether (get,watch.list oprsn should be alowed we can define)
                           here we just want list oprn is allowed for pod resrces
- run apply role.yaml
- run kubectl get role : we see new role pod-reader, and run describe pod-reader we can see resource is pod and verb is list.
- we discsed just by creating role things doesnt work. we also has to associate this role with user.so create role-binding.
- in above website page only we have role-binding example copy and paste it in binding.yaml
binding.yaml:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:                ( in subjects we define to which user a role should be associated with, in roleref we define what is the role that associate with)
- kind: User
  name: system:serviceaccount:default:externaln      ( this is the user that got error when we do curl on cluster for pods)
  apiGroup: rbac.authorization.k8s.io
roleRef:                 ( here we specify kind is role, and name of role that need to associate with user)
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

- apply binding.yaml, run get rolebinding we see our binding, run " describe rolebinding binding_name) this displays what is the role that is associated with , and to which user the role is attatched to)

- curl -k https://our_eks_cluster.com/api/v1/namespaces/default/pods --header "Autherization: Bearer lkddksqgjksag "
  now we able to see list of pods and its contenst.

***********************************************************************************************************************************************************
CLUSTERROLE AND CLUSTERROLEBINDING :
- A role can only be used to grant access to resources within a single NS.
- above in role.yaml we defined NS, here the role will be created.Now lets say there are 20 NS, now user want to perform list oprn on the resource of pod, on all 20 NS's.now if we just create a role and role binding in Def NS, mean user only perform oprn on that def NS only, he wont be able to perform the oprn on other NS's.

- So when we want to perform a global kind of oprn that we wnat to achieve  we dont want NS base objects.if we want to chiev list oprn on 20 NS we have to create 20 role, and rolebindings, this is difficult.we need simple approach that is ClusterRole and ClusterRoleBinding

ClusterRole : It can be used to grant the same permissions as a Role,but because they are cluster-scoped, they can also be used to grant access to: 
 cluster-scoped resources (nodes)   ( nodes are not part of NS. we can give access at a cluster scope rescourecs)
 Namespaced resources (like pods) across all NS's

- https://kubernetes.io/docs/reference/access-authn-authz/rbac/  here only we have cluster role example
clusterrole.yaml:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole       kind is clusterrole
metadata:              ( Here no NS is defined)
  name: cluster-pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]      ( this role giving access to resorces of pods)
  verbs: [list"]       (only 

ClusterBinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: list-pods-global
subjects:                   (similiar to rbinding we have subjects here, we can define usergroups etc.
- kind: User
  name: system:serviceaccount:default:externaln
  apiGroup: rbac.authorization.k8s.io
roleRef:                    ( in roleref instead of role we define crole,and name)
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io


- now delete role, rolebinding created earlier.
- Now run curl -k https://our_eks_cluster.com/api/v1/namespaces/default/pods --header "Autherization: Bearer lkddksqgjksag "  this will fail. 
- apply -f clusterrole.yaml  , get clusterrole, we see many clusterroles where k8s uses. we see our crole pod-reader. run describe crole pod-reader : similiar to role we can see resources are pods and verbal is lists.

- now asosciate crole with anyuser using cbinding.
- apply cbinding.yaml. run get clusterrolebinding list-pods-global    : we see our binding. also run describe binding : we see it is associated with a crole named pod-reader, subject is user,.
- now run curl command we are able to see content of pods.
- curl -k https://our_eks_cluster.com/api/v1/namespaces/default/pods --header "Autherization: Bearer lkddksqgjksag "
- However crole, and cbinding are global,we will be able to list resources even in diff NS.
- K create NS extra.  New NS extra created. 
- now run " curl -k https://our_eks_cluster.com/api/v1/namespaces/extra/pods --header "Autherization: Bearer lkddksqgjksag "    : we did curl for list of pods in NS extra. we are able to see list of pods but they are zero. we getting o/p.means user have permision to this NS.

* A rolebinding may also refernec a clsuterrole to grant permissions to resources defined in the ClusterRole within the rolebinding's NS. 
  ex :we have crole.it can be reference by the rolebinding.so the user defined in rolebinding will be able to perfom oprns defined in crole, biut only in the NS asspociated with rolebinding here. 
This allows admin to have set of central policy which can be attatched via rolebindings so it is applicable at a per NS level. Means once we have crole it acts as central policy, the central policy which is crole can be attatched via rolebinding so it is applicable at a per NS level where the rolebinding is created.
 ex : there are 20 NS's.and i want the use to only able to perform list oprn on 3 ns's.so we can create crole that allows read permissions,and then we can create a rolebinding in those 3 NS's, and the user will be able to perform the oprn on only those 3 namespaces.

DEMO : - create cbinding we created earlier.and run curl command  we got error again.
- create clusterrole-to-rolebinding.yaml :   copy rolebinding yaml and modify it.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding   ( since this rbinding associated with NS default , even we referinceing to crole,ultimately the permissionw pould be at NS level only)
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole       ( here we gave kind of role is clusterrole)
  name: cluster-pod-reader     ( name of crole)
  apiGroup: rbac.authorization.k8s.io
	
- run apply on croletorolebinding.yaml: now rbinding is created on NS of defau;lt.
- if we do curl on NS extra we got error. now do same curl on default NS we see contensts

***********************************************************************************************************************************************************
INTRODUCTION TO ASYMMETRIC KEY ENCRYPTION (right architeture is the key) :

- Asymmetric cryptography uses public and private keys to encrypt and decrypt data.
 ex : in asymitric key encryption there are 2 keys involved.theese 2 keys are somewhere related to each other.what happens here is whatever data we encrypt with  1stkey , can only be decrypted with the second key here.and in similiar way whatever data we encrypt with 2nd key can only be decrypted with the 1st key .that is basic concept of asymmetric key.
- so here 2 keys are referred to as public key, private key.
- one key in the pair can be shared with everyone it is called public key we can share this with across internet. The other key is kept a s secret, we dont share with anyone, it is called Private key.

- Either of the keys can be used to encrypt a message; the opposite key from the one which is used to encrypt the message is used for decryption.
ex : we have a user alice.alice generates 2 keys , one is public key,and 2nd is private key.onece keys are generated, she can go ahead and distribute the keys among the internet and she can say that anyone who wants to send me a mesage , encrypt that message with that publiuc key,and send thata cross.once that message is encrypted with public key,we know that it can only be decrypted with private key which the alice user has.
- Now bob has a mesage has "hello alice" he encrypts that message with alices publickey(to encrypt message) ,and we have resultin encrypted messqge.now this resulted encrypted message he can send across internet or across a unsafe network.No alice will get encrypted message,she will decrypt with the help of priuvate key she has and read it. this is useful to send any messages in internet( ex :we send a mail to friend in encrypted formast using public key. even any user sitting between me and reciever even he can see my encrypoted data, he wonr be able to undertand)

USE CASE 2 of asymmetrice key encryption : when a user wants tpo login to a linux server 
- User zeal wants to log in to the server. Since the server uses a public key authentication, instead of taking the password from the user, the server will verify if the User claiming to be zeal actually holds the right private key.
means we have a user,server.server basically contains the users public key.so in server zeal has its own public key( in image we see that , all the usrr who want to login to server , their pubkey is stored in server)

DEMO : zeal : i am zeal, i want to login zeal does this ssh to server. now The server creates a simple challenge, "2+3=?" and encrypts this challenge with the Public Key of the User and sends it back to the User. The challenge is sent in an encrypted format , encrypted by the public key associated with zeal user.so only the person who holds the assocuated private key will be ablt ot decrypt the ,message. If it is zeals pubkey, then it is assume zeal will hold his privatekey.Since the user zeal holds the associated private key, he will be able to decrypt the message and compute the answer, which would be 5.
Then, he will encrypt the message with the private key and send it back to the server.
- Now server decrypts the message with zeal's publick key, and checks if the answer is correct.If yes, then the server will send an Authentication Successful message and the user will be able to log in.

EX : ssh -i ~/.ssh/dp_rsa  root@public_ip( whenever iam loging through ssh with hyphen i, i am specifying the private key(dp_rsa) in /.ssh) 
- after logging to ec2 server run "cat ~/.ssh/authorized_keys " here this opened file contains public_key.this pub_key was used to compete the appropriate challenge.
- depending upon protocol we are using we can quickly generate a public and private key 
- run " ssh-keygen " this will generate pub and pvt key.
- pvt has stored in : (/root/.ssh/id_rsa)
- pub has stored in : (/root/.ssh/id_rsa.pub)
- go to /root/.ssh dir we can see both pub,pvt keys.
* in org or entpr if we want to login to server , managers ask to generate a pvt,pub key pairs and send them public key.sys admin will go and put pub key in server, and thriugh private key we will be able to login.

- Because of the advantage that it offers, Asymmetric key encryption is used by variety of protocols.
Some of these include: SSH, TLS etc.

***********************************************************************************************************************************************************
UNDERSTANDING SSL/TLS:


UNDERSTANDING HTTPS CONNECTION (SECURE COMMUNICATION) : 

- Https is Extension of HTTP.
- In HTTPS,the communication is encrypted using transport layer security. TLS is newer version of SSL.
- The protocol is also referred to as HTTP over TLS or HTTP over SSL.
* many times when we open website in browser sites like banking,or sites which contain sensitive info there we will see with green colour lock and https.
  the lock means communication is secured.

Scenarios where HTTPS is useful :

Scenario 1 : MITM(Man in the middle) Attacks : a user is sending their username and password in plaintext to a webserver for authentication over a network.
- i have a webserver(any appl) , and user. user open the website in browser and the website ask you for login and pswrd.we put username and pswrd.


- if there is an attacker who is sitting b/w them doing a MITM attack and storing all the credentials he finds over the network to a file. means 
  attacker is sniffing the network then he will be able to fetch all the crednetials which are passing this network within plaintext. he can store within 
  his databse. the longer he sniff the network the more usernames and pswrds he will be able to fetch.
- the reason he able to see the credentials is they are in plain text.THERE ARE VARIOUS PLAIN TEXT PROTOCOLS LIKE FTP,HTTP. if we use plain text protocol
  to send sensitive data , if there is mitm attack then credentials would be leaked.

Scenario 2 : MITM and Integrity Attacks : Attacker changing the payment details while packets are in transit.
- a user sending the info that send 100 rs to zeal.here attacker can modify the details.first he recieve that info and he modifies it to send 100 to attack
  er. now destination server will recievs the details and it sends the 100 to attack and it will be successful.

** to avoid the priveous 2 scenarios( and many more) , various cryptographic standards were clubbed together to establish a secure communication over an 
   untrusted network and they are known as SSL/TLS. SSL was the initial version which was basically intended for secure communication.ssl 2.0,3.0,TLS 1.0,
   1.2,1.3 various protocols which are used in ssl3.0 could be easily compromised.if we are using secure communication with ssl 3.0 were deemed to be unsaf

Understand HTTPS in an easy way : Every website has a certificate(like passport which is issued by a trusted entity) passport generally gets issued by gov
 before passport gets issued whatever contents are there within that passport it might be name,DOB,address that contents will be verified by the gov.
 similiar way certificate is like a passport it contains lot of information and that certificate is issued by a trusted entity.
- this certificate contains lot of details like domain name(zeal.com)  it is valid for, the public key, validity and others.along with that it has version
  serial number, certificate, signature algorithm,issuer,validity, public key info.

Generally when we open https website the webserver will send us its certificate. now the browser(clients) it verifies if it trusts the certificate issuer.
 if it trusts it will verify all the details of certificate. like when we gpo international there is a channel which will verify evrything. once it verify
 it accepts it bo genuine. then it will take the public key info from certificate it also initiates the negotiation.we know that tls is used for secure 
 communication.in order for secure communication it takes the public key info. Assymmetric key encryption is used to generte a new temporary symmetric key
 which will be used for secure communication. here it takes the public key info a secure pswrd is generated.that secured pawrd which is referred to as symm
 etric key will be used for encrypted and decrypted relation throughout the channel.

- if we open any website that website certificate is verified by any certified entities like comado.

***********************************************************************************************************************************************************
CREATING TLS CERTIFICATE FOR AUTHENTICATION (Certificate based auth : K8S auth) :
- we know multiple ways in which we can authenticate to k8s cluster.it can be using tokens,certs etc.
- Our goal is to set up authentication based on X509 Client certificates. at the end finally we want a User A should be able to authenticate with Kubernetes Cluster with his certificates.
- there are 4 primary steps involved in certificate phase auth :  (ppt page.no : 9)
1. here in certificate creation process we need to create key in 1st step, using openssl command to create key
2.once key is generated , from that key we generate a certificate signing request which is referred to as csr
3. from csr we create a new k8s object.here k8s object will have contents of the csr created earlier
4. the k8s object when uploaded to k8s server, we can approve or deny it. if we approve it in the end we get actual certificate
 (zeal-key,zeal.csr, k8s object,approved, zeal.cert)
- so once we have cert. we can make use of key that was generated as part of 1st step,and certificate both of them we can use along with client like (kubectl) to communicate with kube-apiserver.(* both cert,key required by kubectl command to communicate with k8s-apiserver)

DEMO : for demo we need various tools like openssl, base64 for a overall config.we have to install env variables associaste with these tools in our laptop OR we can simply create a linux server,or a linux docker container and use req tools in them.

- in AWS launch a new ec2 using ubuntu OS.
- in cli login to ec2 using ubuntu@pub_ip ; 
1. Configure kubectl in server : 
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin
- run kubectl we see working.
2. setup a kubeconfig file in ec2, so that kubectl can connect to k8s server.we have to connect to k8s object because we have to upload k8s object cretaed above in step 3.
mkdir ~/.kube      ( creted new dir .kube)
nano ~/.kube/config    ( copy kube_config file from local tpo here)
- here run get pods we see o/p.
- now we need openssl command to create a key

3.openssl genrsa -out zeal.key 2048       (using openssl command to generate a key)
openssl req -new -key zeal.key -out zeal.csr -subj "/CN=zeal/O=kplabs"       ( from key we are generating a csr) 
before running 3rd step we need to create a central dir of /root/certs, and all key,csr will be stored here 
mkdir /root/certificates
cd /root/certificates

- run open ssl command, and generate csr in cert dir.
- run ls in certs we see zeal.key there.run cat zeal.key we see it is a pvt key. generate csr from key 
openssl req -new -key zeal.key -out zeal.csr -subj "/CN=zeal/O=kplabs"     ( this comnd uses zeal.key, and o/p is zeal.csr.along with that we also specifying a subject,and within subj there is a common name call as zeal. this will be username.after running abover cmd, run ls we see zearl.csr,zeal.key in cert dir.

- we discussed we have to upload k8s object to k8s ( we have to add contents of zeal.csr in k8s object before uploading it)
-  we have to add csr here. but we cant copy contents of csr here. we havt to do base64 encode. so perform encode oprn :
cat zeal.csr | base64 | tr -d '\n'    : we will get encoded csr o/p here

-nano csr-requests.yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest     ( kind of k8s obj is certsignreq)
metadata:
  name: zeal-csr
spec:
  groups:
  - system:authenticated
  request: ADD-YOUR-CSR-HERE    heere copy the o/p of zeal.csr encoded one.
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - client auth

we added csr-req.yaml in ec2 under certs dir. now run " k8s apply-f xsr-req.yaml " : csr created. run kubectl get csr :zeal csr, but condition is pending because , we can now go and approve or deny it.
- to approve in cli run "kubectl certificate approve zeal-csr", : csr approved. run get csr we see approved for zeal csr .
- we discussed if we want to succesfully auth to k8s server,kubectl will need both key and cert. we dont have cert in workstation till now dwnld it :
kubectl get csr zeal-csr -o jsonpath='{.status.certificate}' | base64 -d > zeal.crt    : this will dwnld cert in cert dir.run ls we see zeal.crt.

- kubectl config set-credentials zeal --client-certificate=zeal.crt --client-key=zeal.key     : here we are setting up the context associated with new user zeal.before that run "kubectl config get-contexts " we see one user called as master-admin.this is actually coming from kube config file, to check run " cat ~/.kube/config " here we see user is master-admin.means whenever we run kubectl command,kubectl makes use of this ,master-admin user and its associated creds(cert or token etc for auth).Now we want to add one more user(zeal) within kubeconfig file
- kubectl config set-credentials zeal --client-certificate=zeal.crt --client-key=zeal.key  (   this will set the creds for the user zeal,where we specifying the cert and key) run it.after that 

- kubectl config set-context zeal-context --cluster [YOUR-CLUSTER-HERE like name of cluster] --user=zeal   : run it.and run " cat ~/.kube/config" we see a new user and,client cert,and key.
- now run "kubectl config get-contexts " we see 2 users, master-admin,zeal.
- run get pods we see pods, beacsue we kubectls is using master-admin to authenticate with k8s cluster. now we want zeal user to auth to k8s cluster.so thats why we have to add context as zeal 
- kubectl --context=zeal-context get pods       ( here name of context is zeal-context) " it will say forbidden error. because the zeal user doesnt have any  permissionto perform a list oprn on pods in def NS.
- so now we have to add a role and role binding to this user, so that the user has the permission.
- nano role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]

-kubectl apply -f role.yaml

-nano rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: zeal
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

- kubectl apply -f rolebinding.yaml
- kubectl --context=zeal-context get pods   : we will get o/p here.

***********************************************************************************************************************************************************
UNDERSTANDING KUBECONFIG : 
- Kubectl command uses kubeconfig files to find the information about the cluster, username, authentication mechanisms and others.
Page 14: here we have 2 clients(kubectl,curl). when we use kubectl , it refers to backend kubeconfig file which is toredin kube dir to find var info like server,user etc and it will connect to k8s cluster.
- A single kubeconfig file can have info related multiple k8s clusters.in ppt, we can see kubconfig has info related to 2 clusters.there are 2 kube-api servers which are running, these are completely diff k8s clusters.
- in config file we can see link of k8s cluster in aws.com, it has user tokens for auth (k8s auth has multiple ways, tokens,username pswrd, certs), also user name,etc.

- lets say in config file we have info related to 2 clusters.so here kubectl can look into which is the cluster that it wants to connect,it can use info from kubeconfig file to connect to k8s cluster.
- in config file we have to understand 3 parts : cluster, context,users.

1.Clusters Feild : Cluster field has details related to the URL of your Kubernetes Cluster and it’s associated information.(pg.15)
- in ppt we see 2 clusters and one kubeconfig file(config file has 2 sections, one seaction for each cluster)
- in cluster field we have url of cluster(k8smaster),name of cluster.

2.Users Field :
User field contains authentication specific information like username, passwords.
There can be different type of authentication mechanisms (user/pass, certificates, tokens etc
- so inorder for our cluster to connect to server, it will need to authenticate to it.so for kubectl to authenticate to cluster it need user name, token etc.so kubectl takes info related to cluster and user and token it will auth with cluster.

3.Context Field:
Context groups information related to cluster, user and namespace.
-we have cluster cluster1.we have user . now we have 3rd field context: it will group all the info together.context says cluster is cluster1,NS is kplabs,user is zeal.now if kubectl reads kubeconfig file,then it will connect to cluster1 of server,and NS kplabs, with the user of zeal.

* kubectl 1st read contexts field in kubeconfig file. in contexts 1st it will read clusterfield and it will connect to it, then in contect filed it will read users related infor and it will auth with cluster using users datsa.if we dont mention NS seperatly in context or user, it will connect to def NS.

-Curerent Context : means it contains name of context we have to use as of now.
- nwo we have 1 context only in config file. then current context will be the one present trhere.
- if we have multiple clusters then there will be multiplke contexts.

- run get pods : we get o/p. here kubectl look into cureent context in kube-config file.( where infront of cur_cont we mentioned context name. so under taht context we mentioned cluster,user etc). so kunbectl connected to that serve rmentioned in curent_context.
- so now create 1 new NS. and run get pods --ns new_NS. no resources found. so i dont want to mention --ns everytime to get pofds in that NS. so for that we can add NS parameter under contexts in Kube-config file. so add that NS in kubeconfig file. and run kubectl config view we cans ee our config file contains NS. so now run get pods we see no resources found. now kubetl connect to new_NS only, not to default.so if we want to see pods in def NS run " get pod --ns default"

***********************************************************************************************************************************************************
CREATING KUBECONFIG FROM SCRATCH :

- we know kconfig contains about cluster,context,user. creating this form scratch waste of time. instead we can generate using commands.
 we have set of commands for generating config file:

1. Add cluster details (in config file cluster details will be seen first so ) :
  kubectl config --kubeconfig=base-config set-cluster development --server=https://1.2.3.4     (base-config is name of config file, we have a set-cluster 
                                                                                and we called it as development, and it will have serever with rurl htps..)
- run abve cmnd it says cluster of devlpmnt has been done.and open dir we run the cmd, we can see base-config file , inside it we cansnee cluster info,and other fields.
- now kubectl need user, means somekind of token, or cert needed to connect to server.

2. Add user details : 
  kubectl config --kubeconfig=base-config set-credentials experimenter --username=dev --password=some-password  (here we are setting cred's we called it as 
                                                                                                                 expermiter with the username dev and psrd.

- run above cmd, it says user experimetter is set.
- now open base-config file we can see user info inside it. Now we need context info because this will group info of cluster, NS,user.

3. Setting Contexts
  kubectl config --kubeconfig=base-config set-context dev-frontend --cluster=development --namespace=frontend --user=experimenter
    ( here we are setting context and name of it is dev-frntd , as we know context will have(cluster,user,NS info)so here we specified cluster as devpmt,and 
       NS is frntd(means user need to use frntd NS , and atlast we specified user)
- run abv cmd, it will say context created. in confi file we can see context has all 3 info

- this is just for one cluster.we seen config file contains info of multiple clusters. so to add one more cluster in config file we need to run above commands again : 

1.kubectl config --kubeconfig=base-config set-cluster production --server=https://4.5.6.7    ( here our cluster name is prod, and it has diff server url, 
                                                                                                and we are adding in same base-config file)

- now check config file we see one mure cluster info.now add one more context for prod cluster with same user and NS.

2.kubectl config --kubeconfig=base-config set-context prod-frontend --cluster=production --namespace=frontend --user=experimenter   (context prod-frnt   
                                                                                                       created. check config file we see 2nd context file)

- kubectl config --kubeconfig=base-config view     : we can see config file here
- kubectl config --kubeconfig=base-config get-contexts  : we see 2 contexts, 1 associated with dev cluster, another with prod cluster.
- so now if we want to use specific context (to connect to specific clustera). like develper is there.when we run above cmd get contexts we see under current no context is set.so if devlpr wants to connect to dev cluster then he can make it as a default cluster to connect to it.so fo that he has to set the context associated with dev cluster as the def context.

- so in config file add : " current-context: " emty. to set any cluster to current: 
- now dev want to connect dev cluster so : kubectl config --kubeconfig=base-config use-context dev-frontend  " now we switched to dev context.
- run above get-contexts command we see * under current for dev context. means whatever kubectl command that we run , that will ran agains dev cluster.also check base config file we see  current-context: dev-frontend.

- so now if we want to connect to prod : kubectl config --kubeconfig=base-config use-context prod-frontend
- now run get-context we see * for prod cluster . and see config file we see current context:prod.  so this way we can connect to multiple clusters using same config file.

***********************************************************************************************************************************************************
Kubernetes Secrets : 

Challenge : ex : we have an App A runs on pod and it needs to connect to a Mysql DB to start working properly.
we have appA pod, and mysql.appA needs to authenticate with mysql DB. inorder for appl to start working it has to authenticate. Generally developer hardcodes the cred's.here APP A launches from a docker image and dev would hardcode the creds like(db_user,db_pass) in docker image by itself.Since trhe cred's are hardcoded once appl starts it can directly authenticate with user name and pswrd to mysql DB.

Risks with these approach : 
- Anyone having access to the container repo can easily fetch the credentials.
- Developer needs to have creds of prod systems. if mysql is prod DB , sinvc ethe dev wouldmbe building hisown docker image, the db team will have to give 
  the dev the username and pswrd in plaitext

- Update of creds will lead to new docker image being built.: like we have a compliance policy where we have to rotate db creds every 90days,so every 90days    we have to rebuild our docker image considering that the creds are hardcoded.

- INSTEAD OF HARDCODING CREDS WE CAN STORE THEM CENTRALLY : 
 here we have a central secret store. it contains our creds.now APP A doesn't have any hardcoded creds. so whenever a APP A gets launched , our appl can fetch the creds from central secret store.once it fethes creds it can use that creds to authenticate with mysql DB.
 PATH : CENTRAL SECRET STORE - fetches creds- APP A(POD) - AUTHENTICATE WITH MYSQL DB.

ADV'S : 
- we don't store hardcode cred's in pod.
- dev will not have creds.here dev will have to write a logic to pull creds from central secret store.
- even cred's are rotated the docker image will not have to rebuild.
In central secret store db team or other team will store creds and manage the creds.

CENTRAL SECRET STORE : name itself describe it is a central place where secrets will be stored. Kubernetes secrets is central secret store.
 Various other services also provide central secret store : AWS SECRETS MANAGER, AWS SYSTEMS MANAGER PARAMETER STORE , HASHICORP VAULT all these helps to store secrets centrally and the creds in these can be fetched by APP A pod to do oprns like authentication.

K8S SECRETS : A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key.
- Allows customers to store secrets centrally to reduce risk of exposure. like we have k8s secrets and we are storing username and pswrd.
- Stored in ETCD database.

Creating k8s secrets in CLI : " kubectl create secret [TYPE][NAME][DATA] "
 here TYPE : 3 types : 
1.Generic : File, directory, literal value , ((--from will be used with all these 3 generic types)
2.Docker registry
3.TLS.

DEMO : 
run " kubectl get secret " we can see 1 secret which is default secret. 
create a secret :  " kubectl create secret generic fistsecret --from-literal=dbpass=mypassword123 " this is literal value example. run kubectl get secret we can see 1 secret(firstsecret)

- run " kubectl describe secret firstsecret " it will show details. in details it only shows dbpass. it doesn't show value associated with it.if we want to see value run " kubectl get secret firstsecret -o yaml " now secret will give dbpass value in base64 encoded format.like ( dbpass = kjshdkjfagfkjfg)
 to decode it innlinux run " echo kjshdkjfagfkjfg | base64 -d "  we will gte mypassword123. this is secret interms of literal value.

Load secret from file : in notes.txt write 'dbpassword1234'. now we will add a secret with option of file. run
 " kubectl create secret generic secondsecret --from-file=./notes.txt" run get secret we can see 2nd secret.
- Create a secret from yaml file :

apiVersion: v1
kind: Secret
metadata:
  name: thirdsecret
type: Opaque
data:
  username: dbadmin
  password: myadmin   save file as secrets.yaml

run " kubectl apply -f secrets.yaml " it will throw error : bad request because data we given usernam,pswrd are in plain text.it needs the encoded version.
now encode username and pswrd : in linux terminal run " echo -n 'dbadmin' | base64 " this will give ZGJhZG1pbg== , do same for pswrd 'myadmin' it will give bXlwYXNzd29yZDEyMw== .
apiVersion: v1
kind: Secret
metadata:
  name: thirdsecret
type: Opaque
data:
  username: ZGJhZG1pbg==
  password: bXlwYXNzd29yZDEyMw==  now run this " kubectl apply -f secrets.yaml " it will say 3rd secret is created.


MOUNTING SECRETS INSIDE PODS : 
- Once a secret is created, it is necessary to make it available to containers in a pod. there are 2 approches to achieve this 
1. Volumes(primary approach)
2.Env variables

DEMO : run kubectl get secret : we will get multiple secrets thatw e created above. run kubetl get secret firstsecrte -o yaml. in o/p we can see dbpass=base64 encoded value. now we will use volume method.

apiVersion: v1
kind: Pod
metadata:
  name: secretmount
spec:
  containers:
  - name: secretmount
    image: nginx          tille here is common
    volumeMounts:           ( here we have a vol of foo. mount path is /etc/foo , and this is a read only directory.
    - name: foo
      mountPath: "/etc/foo"       we are associating first secret within this "/etc/foo" mount path. 
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: firstsecret     here we are making firstsecret should available in this "/etc/foo" directory inside the container. 
 - save the file as secretpod.yaml : run " kubectl apply -f secretpod.yaml " o/p : pod/secretmount created. now run get pods. we can see 1 pod in created.
login to pod : " kubectl exec -it secretmount bash ". inside container run " cd /etc/foo " and run ls we can see a file dbpasss.
 run cat dbpass : o/p mypassword123

2ND APPROACH : ENV VARIABLE :

apiVersion: v1
kind: Pod
metadata:
  name: secret-env
spec:
  containers:
  - name: secret-env
    image: nginx
    env:                                      this is env section
      - name: SECRET_USERNAME                name associated with env variable.
        valueFrom:
          secretKeyRef:
            name: firstsecret
            key: dbpass         whatever value associated with key of dbpass in the secret of firstsecret will be associated with SECRET_USERNAME
  restartPolicy: Never

- Save the file in secret-env.yaml . "kubectl apply -f secret-env.yaml " o/p : pod/secret-env created . cretaed 1 pod.
- login to secret-env container. so in env var approach we have a env var which has been created called as(SECRET_USERNAME) 
 inside contaienr run " echo $SECRET_USERNAME " o/p: mynewpassword123. means the secretusername env variable has value(myne..) associated with secret.

***********************************************************************************************************************************************************

STORAGE 

***********************************************************************************************************************************************************
OVERVIEW OF DOCKER VOLUMES :

Challenges with files in Container Writable Layer :
- we already discussed whatever image that we have,this image is of read only.and whatevrer writes that we make , those writes happen at the container layer (R/W layer).so whenever we create any file inside container,the file gets created in R/W layer.the problem with writing file in this layer is , it is completly dependent upon the container lifecycle. so if container is deleted, so all the files which have been written to container layer will also be removed.

- also if there is another process which is running inside container or host itself,if they want to access specific content from r/w layer it is difficlt.

-whenever we write contents into r/w layer we need to make use of some storage driver(it can be AUFS,overlay to any union filesystem base driver). the proble here is as soon as we make use of storage driver then the overall performance that we can achiev in storage layer is much more less compared to the host file system, also overall performanvce is degraded. to overcome this we make use of DOCKER VOLUME.

- DOCKER VOLUME-DV (pg.3) :  we have a container, we can mount container dir inside a filesystem,or we can mount inside a spcific area called as volume.So now the container is independent on the data which is stored on host level.so irrespective of containmer is running or not running por deleted , this data that is there within the FS or within vol will continue to exist.ADV : even contantr is deleted data is still present.Since data is there within the host , if we have another process or there are other containers which are running those processes can also access access this specific area.

- there are 3 ways in which we can store files in host machine.vo;lumes(DV,bind mounts, for linux we have tmpfs mount)
- tmpfs memory is not persistent , because it stores inside memory.for [persistent we have bind mount,DV.
- so now understand about DV.

DEMO :
- run docker ps we see 1 container running.this cont is based on overlay-2 storage driver.to check run" cd var/lib/docker". run ls -l we see overlay2.so go inside overlay2 by cd.and run ls -l we see one specific driver(dir) is for container write layer.
- so whaterver data we write in container would be stored in that specifc dir. so this dir whatever container r/w we discussed it depends upon container lifecycle.
- now stop container and rm it.now run ls -l in overlay2 dir. we see one folder in overlay has been deleted.so we cant recover that data in anyway.so this is reason for the appl which are stateless this type of arch is not really matter. But if we have a statefyul appl , lets say many org they host DB like mysql inside container,theydont want if any container gets deleted , they dont want to loose data. they have to keep data of container seperately, and container seperatly.this can be achieved with the help of bind mount and Vol's.

- run docker vol ls: no vol created.create one "docker volume create myvol", run ls we see myvol.(in pg.3 the docker area is actually dock vol). so we created that dock area.So now whatever container that we might launch we can associated that container dir with vol's dir.
- so create a container " docker run -dt --name busybox -v myvol:/etc busybox sh " here we associated one etc dir under myvol volume for this container.
run ps we see cont.
- run docker volume inspect myvolume : we see mountpoint: "/var/lib/docker/volumes/myvol/_data" : so this vol has a amountpoint in var.._data. so go to this dir "cd /var/lib/docker/volumes/myvol/_data " run ls -l we see all contents of etc dir.
- now login to container and got etc dir.we see all files. now all these files are stored in docker vol dir we see above in vol. Means the etc dir from the container is associated with the specific area within our host.so even if container stops working  or deleted whatever datsa , whatever data that presne tin dock area or vol will still be present.

- run "df -h "  to see list of file system : we see etc dir in dev/vda1 volume.
- now stop bysybox cont.and rem it.and check data here "cd /var/lib/docker/volumes/myvol/_data" the data is still presnet.this data isd indepenedent from contnr lifecycle.also one more adv : if there are any process which are running in host and if they want to access the data so since data is present within vol that process can be able to access data.

- if we dont need volme we can rm it " docker vol rm myvol" run vol ls we see vol deleted.

- I have a dockerfile.from busybos, create one dir mkdir /myvol . we are doing echo of hello world inside a file hello.txt in myvol dir.and we specifying a vol called as myvol
dockerfile:
FROM busybox
RUN mkdir /myvol
RUN echo "hello world" > /myvol/hello.txt        ( till here a busybox contr would be created.then myvol dir will be created.then there would be a file 
                                                    called hell.txt which have content of hello world.
VOLUME /myvol             here we specifying a vol whcih is myvol. so the contents which are present above (myvol/hel.txt)nwill be copied to vol called as 
                          myvol which is residing within the host

- build image dem,( earlier we created vol using create vol coimmand, and we mointed our container on vol using docker run -v . now it is not needed at all)
- now run images we see our demo image.launch container from demo im and dont specify any vol. run ps we see cont. and run volume ls we see a vol automatically got created and this vol should have hel.txt file.and this vol would reside " cd /var/lib/docker/vols" and run ls we see vol.run cd vol,and run ls we see hel.txt file.

- disadv with this approach is when we run vol ls , the vol name is very big like container UUID(not able to understand which vol). let say 100 cont, all 100 =cont wil have diff vol name.so it is recommended to use a named vol.
so run " docker run -dt --name dem1 -v mydemo2:/myvol demo sh " here we associated a dir myvol to the container vol mydemo2.now run it and run vol ls we see a dir called mydemo2.
- also check run pwd and go to /var/lib/docker/vol. and run ls we see mydemo2 dir.go inside we see hello.txt. 

- A given volume can be mounted into multiple containers simultaneously.
- When no running container is using a volume, the volume is still available to Docker and is not removed automatically.(vol lifecycle is independent of 
  container lifecycle)
- When you mount a volume, it may be named or anonymous(like UUID). Anonymous volumes are not given an explicit name when they are first mounted into a container, so  Docker gives them a random name that is guaranteed to be unique within a given Docker host.however we can also give a name to docker vol and it is referred to as named vol.

***********************************************************************************************************************************************************
CREATING VOLUMES IN KUBERNETES : https://kubernetes.io/docs/concepts/storage/volumes/

- we already seen On-disk files within a Container are ephemeral(if the container goes down or terminated  then the data becomes inaccasable)
- generally when kubelet founds that there is unhealthy pod or appl is crashed, then kubelet will basically create a new pod from clean slate.so here whatever the data present in the data will be lost.

- When there are multiple containers that want to share the same data, it becomes a challenge. means pod contains multiple containers,if all cont's wants to share common data,then we need a soln where all the conts can access a specific storage device.we already discsed diag(pg.3) where we create a container and we can do bind mount over filesystem where docker area(vol) gets creatd,so containr goi and create persistent data which will not get lost even cont terminated.we can also have tmpfs moiunt here data gets stored in memory but it is not persistent, if we restart our system then data will belost.

Vol's in K8S : One of the benefits of Kubernetes is that it supports multiple types of volumes.
ex : we have a pod and we connected it to vol.vol need not to be or necessary to be at host level.we can have various other vol's like EBS,local.nfs etc.so instead of creating local vol within host itself , we can even create persistent vol which is highly available i.e EBS( replication also happens behind secenes, so that our data doesn't lost)

- in k8s websoite under vol''s we can see multiple options available for vol's.we can use any vol which can be moiunted with our pod.

- in website select hostpath : A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. means we create a file or dir within a hostsystem,and we mount that as a vol within our pod.whenever data in pod stored, it gets stored in specific volume within host.
DEMO : we setup a minikube clustewr (because aws eks doesnt give access to underlyinghost system).
pod.yaml :
apiVersion: v1
kind: Pod
metadata:
  name: demopod-volume
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /data
      name: first-volume
  volumes:
  - name: first-volume         ( we gave name of vol)
    hostPath:                   (then we define type of vol. in doc we see multiple types of volumes.so we defined hostpath as type)
      path: /mydata            (hostpath mounts specific file or dir within host inside a container. so we sepcified path here(/mydata is path)
      type: Directory          ( we specified host path type is dir.so the path(/mydata) we want to mount inside a container is of type dir.
                              so all the above vol section would be present in a host where pods would be running.
                    after defining vol section we have to define volmounts above.volounts says that take specific vol(1st-vol)and specify the mount path where exactly this dir(/mydata) from host should be mounted inside a container. so above we mentioned it should be moiunted under /data dir.

- now run mkdir /mydata in minikube. then apply above pod.yaml. run get pods it is running. login to pod using exec and check ( in yaml we defined the dir of host should be mounted in a /data).so inside pod run cd /data. it will automatically go inside data dir. means dir exist. now create one file (touch sm.txt and do echo "hi" > sm.txt ). we knoe data dir is mounted from the host inside container.now exit from container.

*** in yaml we defined whatever data we define under /data in container will be stored in /mydata dir in host. so whatever files we store inside /data will be  store underlyng host /mydata dir. so now in hoist run cd /mydata and run ls we see sm.txt exist.

***********************************************************************************************************************************************************
PERSISTENTVOLUME AND PERSISTENTVOLUMECLAIM : 

- generally in dev or stage env , the develprs have to playa round and have to write own yaml files  depending upon the pods they want to create.
- in scenario of vols we descussed here devlpr have to do is , they have to create first vol of spec type(EBS,NFS,local etc). so devlpr should know abut type of col and its associated config.
- we took type hostpath and path of mydata nd type is dir.after creating vol of any spec type then we can mount it on pods.this approach is simple when we make use of directly vol's within our k8s.

Challenges : 
- as for devlpr perspective : a devlpr just want to write code and pod.yamlfile.he dont want to take care of storage provisioning,and its configrts.so now org is using cinder or glusterfs vol type , so devlpr since he has to mention these vols in yaml he has to have a overview of glusterfs or cinder vol and its associated configs, this is not dev wants.

- as for storage-administrator: he says he will takecare of provisioning storage and its associated config's, means if a vol need to be created in glusterfs, the storage-admin will take care of creating vol's and its associated config's.now devlpr just has to create a reference to the storage within the podspec.yaml.so storage-admin will take car of vol section in yaml , devlpr just has to reference to that spec vol . so both the things req a layer of abstraction here in b/w, because when we directly use vol's then the dev will have to do both the things within yaml file,. so this level of abstraction can be achieved with the help of PersistentVolume and PersistentVolumeClaim.

PersistentVolume (PV) : A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. mena PV is just an abstraction for the vol. in(PG.4) we have 5 vol diag's available.each vol has certain spec's.vol 1 is size small,and speed fast...etc.   so in orgs few need small vol,and fast speed, and big vol slow speed.  so storage-admin can create a wide variety of vols so that dev can use.this is refred as PV.

- each vol that created cab be diff type.we  seen in k8s diff types of vol available(ebs,nfs,cinder)each type provides diff kind of adv's.so this provisioning of vol's diff types, diff configs can be taken care by storage admin,or opsteam.

PersistentVolumeClaim (PVC) : A PersistentVolumeClaim is a request for the storage by a user. means once PVs are created, the dev has to req for those vol's, that req is referred as PVC.whenever we create claim user has to specify PVC,like size,along with the access mode for that vol.
EX : Developer: I want a volume of size 10 GB which has a speed of Fast for my pod.

(check PG.5) : 
1 Storage Administrator takes care of creating PV.
2 Developer can raise a “Claim” (I want a specific type of PV). claim contai s spec vol config and of size.
3 Reference that claim within the PodSpec file.  dev will reference that claain which contain pv config in yaml file.

DEMO : in cli run ls we see , we see 3 files. these 3 files is associated with the 3 steps written above.
1 create a PV : pv.yaml: 
apiVersion: v1
kind: PersistentVolume                     kind is pv. so a vol pv is created of 10gb mentioned below
metadata:
  name: block-pv
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi                          ths vol storage cap is 10gb
  accessModes:
    - ReadWriteOnce
  hostPath:                                vol type is host path.
    path: /tmp/data

- so in cli create a dir (mkdir /tmp/data)
- then apply pv.yaml: pv vol created.run get pv we see block-pv 

- Now dev has to raise a claim for the above spec vol or other.so that one of the vol from the pool will be assigned depending upon config that dev needs.
2 raisea PVC : pvc.yaml (raised by dev)
apiVersion: v1
kind: PersistentVolumeClaim                    kindi is pvc
metadata:
  name: pvc
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce                            access mode we needed.
  resources:
    requests:
      storage: 10Gi                           in resource , we req for storage of 10gb,
- now apply pvc.yaml : this create a claim.run get pvc we see 1 pvc created,and vol associated with this pvc is block-pv. now run get pv , we see under claim it is claimed by PVC called as pvc. means we created pv 1st,then we created a claim, depending upon the config we written in claim,the claim automatically bounded itself to one of the PV which is matched its config.

- Now dev within his podspec file is he has to reference to the above claim : 
kind: Pod
apiVersion: v1
metadata:
  name: kplabs-pvc
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:                            vol mount section is simliar to the on e wecreted for vol's in k8s
      - mountPath: "/data"
        name: my-volume
  volumes:
    - name: my-volume                   name of vol is myvol , referenced above
      persistentVolumeClaim:               additional thing here is pvc.the dev can reference the claim which is pvc.and name of pvc is given below.
        claimName: pvc

- apply podpvc.yaml. it freates pod. and exec into it,and run df -h, we see mount point(/data). and go inside it and create a file using touch. and this file will be created in PV wich is referenced by claim.now check pv.yaml , the path is /tmp/data. so go here in cli and run ls we see our file which is created in pod.

***********************************************************************************************************************************************************
STATIC VS DYNAMIC PROVISIONING OF PV : 

There are two ways PVs may be provisioned: statically or dynamically.
Static : here a cluster admin manually creates PV's.we saw in diag where admin creates a pv manually in pg 5, in demo also we did manually is called static provisioning.
Dynamic : when none of the static PVs which the admin has created,matches the PVC , the cluster may try to dynamically provision a vol specifycally for the PVC.( a s-admin has cfeated 3 PVS of type 5gb.now a dev needs a vol which is 5ogb size.so now none of the vols whiche were created in static provisioning matches 50gb size.what happens in dynamic provisioning is , a vol is automatically created of type 50gb.so for the clusters which supports dynamic provisioning,we no longer need to create PV manually.we can directly run pvc.yaml, and reference it within pod yaml.here pv.yaml not needed.So whenever we creating a claim,the vols can be created automatically or in terms of dynamic provisioning.

DEMO : modify pvc.yaml file created above.  dpvc.yaml :
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dpvc
spec:                             under spec , we removed storageclassname: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi               we gave g0gb storage

- run get pv we seen block-pv.and storage clas is manual, size is 10gb.now apply dpvc.yaml. this created a pv of size 50gb, and storageclass is standard, and status is bound. now run get pvc, we see a pvc of 50gb automatically got created.class of pvc is standard.
- run describe pv dpvc : here this pv type is hostpath,and it has automatically configured a dir(/tmp/hostpath/pvc-sjshjbdj). so this is referred to as dynamic provisioning.where we just specify the things in pvc.yaml if there is no storage of type pv which matches the pvc,automaticxaly a pv is created .
- with this d-prov : we dont want s-admin to go and create 100's of pv's which are readymadly available.

- Even in AWS also we can req if we want vol.when we run yaml file , this will create a persistent vol inAWS.
- Hostpath is something we should avoid in PROD env.

aws doc for pvc :
#pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ""
  resources:
    requests:
      storage: 5Gi

- 5gistorage gets created in aws level if we are using EKS.now apply it and run get pv we see pv created,also in aws check vol section we see 1 vol 5g crtd
and this vol is not connected to any instance.so after this we create a pod in eks and we can mount the above vol inside a pod.
-below yaml file is just for ref, not imp.
apiVersion: v1
kind: Pod
metadata:
  name: efs-app
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c",]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: efs-claim
	
***********************************************************************************************************************************************************
CONFIGMAPS: 
 
Use Case : we have an App A container. depending upon the env the config settings associated with APP A container will need to be chjanged.
EX : we have dev and prod env's.
- we have dev pod and prod pod.
- so for dev containewr of APP A we need certain env like : app.env=dev app.mem=2048m app.properties=dev.env.url. now when this container goes to prod we have to change the values. 

- like for prod container of App A : we need  app.env=prod app.mem=8096m app.properties=prod.env.url
- usually org create 2 diff container images. inthese 2 container images althoiugh the appl is same but the config file which has the above parameters are diff. Just for that single config file they have multiple config images.this is not best practice.
- let's say tmrw in dev env we want to change memory from 2048 to  app.mem=8096m. then we have to rebuild entire contaienr image.this is very tideous task.  
 and that's why configMapas will really useful.

- ConfigMaps : ConfigMaps allows us to centrally store data.what happens now is that instead of hardcoding the above parameters within the container image, we will centrally store the data. we centrally store the data associated with dev properties , same for prod. from container image we are referrencing the central data 

- in centrally storing data we can dynamically change the values.let's say i want to add one more paramater (app.cpu=1) in config file , if our config's are stored centrally then we can easily do that. If our config files are hardcoded within the container image then if we want to change even a certain parameter 
 we will have to completly rebuikd that speciifc image.

- The method of centrally storing the data is achieved with the help of ConfigMaps within K8s.
- we have ConfigMap object.within that ConfigMaps there are 2 ConfigMaps that are available 1 is associated with dev and is associated with dev pod,2nd is associated with the prod which is mounted with the prod pod.

- if we already have all of our config's which are been stored centrally then we no longer need to have a seperate contaienr image.here we can have a single container image and depending upon which env our image is deployed(say we are depoying image in dev env)then automatically we can reference the dev config map here. if we are pushing oiur container image to prod env then we can directly push this prod properties to the container image.

- CLI Syntax for creating ConfigMap : " kubectl create configmap [NAME] [DATA-SOURCE] "  here data source can be( file,directory,literal value)
- run " kubectl get ConfigMap ": currently no resources found.

- create 1 configmap : " kubectl create configmap dev-config --from-literal=app.mem=2048m " : configmap/dev-config created
- kubectl get ConfigMap : it shows 1 config map dev-config. run " kubectl get configmap -o yaml " : inside we can see this configmap has literal(app.mem it is key) and it has a value 2048m.

2ND approach reference to a file : we have 100's of configmaps. instead pof adding via literal value we can add it via file.if we have multiple config file we can add a directory.
- i have a file dev.properties : this file has 3 key value pairs:
 app.env=dev
 app.mem=2048m
 app.properties=dev.env.ur     3 key value pairs associated with dev env. since this is a file we can directly reference this to configmap.
run " kubectl create configmap dev-prop --from-file=dev.properties ".o/p: configmap/dev-properties created . means whatever configuration which was stored in the file that config is part ofthis configmap.
- run " kubectl get configmap dev-properties -o yaml " o/p : in data section we can see all 3 values which were part of dev.properties.

Q. How we can mount configmap which has been created inside a pod.it might happen that our app will be running inside pod and app intern needs this config files to start itself. so we want whenever a pod starts all of our configurations(app.env,mem) should automatically be mounted within our pod so that appl can access it.
- various ways to mount. main way is through volumes.
file : configmap.yaml :

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod         ( cretaed a pod and name is configmap-pod and launching it from nginx container)
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:           ( we using volumes, where we have volumemounts,and we have volumes)
      - name: config-volume
        mountPath: /etc/config      ( in volumemounts we are giving mountpath
  volumes:                           (in volumes we are referencing the config-maps
    - name: config-volume
      configMap:            ( the reference of volume is config map)
        name: dev-properties   ( in our case name of configmap is dev-properties. so whatever data which is present in dev-properties will e mounted inside 
                                 etc/config directory of my pod.
  restartPolicy: Never     Save the file as configmap.yaml
run " kubectl apply -f configmap.yaml " : pod/configmap-pod created. run get pods we can see 1 pod.login to that pod using " kubectl exec -it configmap-pod bash " go to "cd /etc/config ". and run ls. we can see dev.properties. run cat dev.properties . we can see 3 properties  app.env=dev,app.mem=2048m
 app.properties=dev.env.ur " this dev.properties is the file name that we have referenced in old approach.remember we have done from file and we have referred dev.prop is mounted inside the pod under etc/config

***********************************************************************************************************************************************************
SECURITY CONTEXTS(SC) : 
- When you run a container or pod , it runs with the UID 0 (uid 0 is Administrative Privilege, in linux systems concerned). risk fof running container with uid 0 is the container breakouts, In-case of container breakouts, attacker can get root privileges to your entire system.
container breakout : lets say a process running inside container, due to some vurnerability attacker was a ble to exploit that process and he is now inside container.so now attacker can get into our host system where our contr is running, in such case attacker will get same previliges as that of the process which was running in contr.
- SO thats why it is recommended to always run the POD and container with least privilege user instead of the ROOT user. Here incase container breakout happens,then attacker will have previlige as a contr or process user.here anyways process or contr doesnt have many permissions to host or serveerr where contr is running, spo he gets permission denied.(earlkiewr with admin previligies with pod, if attacker gets admin rights, he can do many things).

- In K8s there are 3 imp permission aspects(called as security contexts): 
1.runAsUser : it Specifies the user of therunning process in container  ( whateever process running inside pod,it have one user, in pg.7, we see appA user), this specific approach can be defined with the runas user security context in Pod manifest.

2.runAsGroup: specifies the primary group for all the process within container
3.fsGroup(filesystem group) : this is pecific for vol's which are mounted in container.what it does is any data that we write to the specific vol,will have the permission which is specified within fsgroup.

DEMO : run a pod from busybox image and login to it using '-it sh'. inside pod run ps to see any process, we see a user 1 is root. so go to cd/tmp and create test.txt . and run ls -l, this test file we see it has a permission of root and grpoup of root.we discussed this the contr which is running on this pod is running with root previlige.this is the 1st approach we discussed where everything withib our pod is running with root previlige.here if contr breakoutrs, attcker will get access to FS and all rightrs. to avoid that we need to run contr with least previliged user,this leats user can be dewfined with SC in k8s.
pod.yaml :
apiVersion: v1
kind: Pod
metadata:
  name: pod-context
spec:
  securityContext:              in podyaml, we just added SC.and we difined 2 parameters.
      runAsUser: 1000
      runAsGroup: 3000
  containers:
  - name: sec-ctx-demo
    image: busybox
    command: [ "sh", "-c", "sleep 1h" ]

- apply on podyaml. here login to that pod and run ps(process) we see , 1 st process is running with a user 1000, and is not running with user of root.also go to tmp dirand create a file and run ls -l.we see the user is 1000 and is part of 3000 group.because in yaml we mentioned runasuser is 1000, and group is 3000.

- now for fs group SC :
pod1.yaml :
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: root
    runAsGroup: 3000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}                         here we have vol of empty dir.
  containers:
  - name: sec-ctx-demo
    image: busybox
    command: [ "sh", "-c", "sleep 1h" ]
    volumeMounts:                        we also have volmounts.whtever dir that is created above sec-con-vol will be mounted into the path of /data/demo
    - name: sec-ctx-vol
      mountPath: /data/demo
- apply pod1 and login to it.and run ps, we see 1000 user at process 1.and tmp dir create 1 file and run ls -l we see 1000 user and 3k group for that file.
- also go to /data/demo in pod: and run touch test.txt and run ls -l, here we see user permission is 1000, and the group permission associated with test.txt  is 2000.means any user is part of 2k group,would be able to perform read only on test.txt.
- in linux we have the name of user and we have uid associted with it. run cd /etc/passwd , run ls we see root is at UID 0.also a bin user at uid 2.in passwd dir run id, we see curently we looged into as a user who has 1k uid,gid 3k, and group of 2k. so above in yaml under SC we dont specify usernames,also above yamlif we give runasuser =0 means it is root.

***********************************************************************************************************************************************************
STORAGE VOLUME EXPANSION IN K8S : 

- it can happen that our PV has become full and we need to expand the storage for additional capacity.like we have a pv of 10gb and it is full, we may have to expand this to 20gb capacity this is treferred to as Vol expansion.to expand vol certain steps involved :
1.Enable Vol expansion ( we have to makesure that a parameter"allowvolumexpansion=true"associated with the storage class where our pv is associatd with.)
2. once parameter is true we have to Resize the PVClaim like in yaml stoiareg is 10, modify it to 20gb. and asve it.
3.after modifyinmg pvc, we have to restart the pod.( we can do by deleting runningf pod and repplying pod.yaml).

DEMO : we have pvc.yaml, pod.yaml.
- run k get storageclass, we see ebs. because we are using aws eks cluster, because beside privisioner it shows aws-ebs.describe that ebs we see volume expansion is true.after verifying that create a pvc.yaml:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kplabs-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
- apply pvc.run get pvc we see it.and run get pv we see 1 pv 10gi created. now create pod.yaml :

kind: Pod
apiVersion: v1
metadata:
  name: kplabs-pvc
spec:
  containers:
    - name: my-frontend
      image: nginx
      command:
        - sleep
        - "36000"
      volumeMounts:
      - mountPath: "/data"
        name: my-volume
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: kplabs-pvc
- apply podyaml. get pods, descrie it we see it was succesfully able to attatch the vol here.and connect to pod,inside run df -h, we see size is 9.9gib.
- now expand the vol to 20gib.run get pvc we see it has 10gib cap.also get pv same cap.
- run " k edit pvc kplabs-pvc, and give storage section=20gib. now get pvc we see 20gib. and describe it we see message"waiting for user to restart the pod to finish FS resize of vol node.means inorder for actual resize to happen we need to restart pod.also in type we see FSresizepending.
- run pv we see cap is 20gb. but login to pod which is runningf and run df -h we see still cap is 9.9gb.
- so delete that pod by k delete -f pod.yaml.then applky -f again we see it is running and login to it and run df -h we se 20gb there.

***********************************************************************************************************************************************************

DOMAIN 7 : Cluster Architecture, Installation & Configuration

***********************************************************************************************************************************************************
CONFIGURING CLUSTER WITH KUBEADM (provisioning k8s cluster with kubeadm) :

- kubeadm allows us to quickly provision secure k8s cluster.
- in k8s cluster 2 components are there.
  1. k8s master : it contains 5 components, already discussed.
  2. 2nd component related to k8s worker nodes. in worker nodes again 2 components : Kubelet, Kube-proxy. depending upon tooll we use to config k8s cluster , we see either master and nodes are in 2 seperate server, or both of them at same server itself.
- when we install minikube server : it works as both master and worker node server.
- in kubeadm installation the overall approach is more diverse.kubeadm installs 1 server for master and its components, and another server for worker node and its related components.kubeadm once it installs master, node it also takes care of config's associated with all components as part of master and node.

* In CKA provisioning k8s cluster with kubeadm is very imp concept.
- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
in above link we see before installing kubeadm, we need to have H/W req's and install container runtime.
- now we install 2 servers for master and node.

- In aws launch 2 ec2s using ubuntu OS. and give hostname as kubadm-master,node.
- in ubuntu login to ec2 using "ssh root@publicip of ec2",login 2 both. so now we have 2 server for master big ec2 as per H/W req, for node small server.
- after that we have to install container runtime. however in exam if we have to install kubeadm, we dont have to install runtime, beacus eit is pre installed. we just have to install kubeadm.
- install container runtime in both master and node :
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

apt-get install -y containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml         Here we installing conter runtime. in both ec2's.

after running above commands, we just have to modify config in containerd.toml :
- nano /etc/containerd/config.toml    : here open it and search "SystemdCgroup" by pressing "ctrl + W" to search and changeit from false to true in both ec2
- after modifying restart the containerd : systemctl restart containerd

- Now we have to change Kernel Parameter Configuration : 
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system         run this command in both ec2 master and Wnode.


- Now start kubeadm Configuration : 
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

- sudo apt-get update

- Now install kubeadm related packages : run " apt-cache madison kubeadm" we see many versions of kubeads. if we want to explicitly specify any version we can do : sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00 cri-tools=1.24.2-00   : here we added 1.24 version for kubelet,adm,ctl in both ec2's.

- sudo apt-mark hold kubelet kubeadm kubectl   : here we are doing apt-mark on 3 klet,adm,ctl.means we are holding klet,adm ctl packages so that a newer version cant be install at these set of packages, and also these packages cant be removed.run it both ec2's.

-  Initialize Cluster with kubeadm (Only master node)
kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=1.24.0     : run this command in master only. after completing command we get 3 commands : mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config           we have to run these 3 commands as well
- also when we run kubeadm init command above we get a " kubeadm join Ip_of master" . this command will join the worker node in master.so this command we have to run in worker node if we want to add this node to master.

- after runnnig above 3 commands in master ,we have to Install a Pod network add-on in masteer : 
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 

- so till here overall kubeadm setup is done in master. Now we have to add node to master.run get nodes we see 1 node which is master-control-plane.
- now copy " kubeadm join Ip_of master "all this command and paste in node command.now run get nodes in master, we see 2 nodes : 1 master, 1Wnode.

- run a pod launching command in master, and get pods -o wide we see , pod is running in Wnode.

***********************************************************************************************************************************************************
UPGRADING KUBEADM CLUSTERS : 

- lets sa we have kubeadm cluster with 1.2 version, i want to upgrade it to 1.4 version.to process upgrade, we have total 7 steps.
1.find latest version to upgrade to
2.unhold kubeadm binary : when we have hold on any package , we wont be able to remove or upgrade it, for that we have to unhold
3.Dwnlod latest kubedam binary(apt-get install)
4.perform upgrade plan
5.change version in command ,while running
6.also upgrade kubelet,ctl binaries.
7.Restart service.
- https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/    here we find how to upgrade.

- for upgrade create new EC2. and run above 1st step:
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sysctl --system      : login to ec2 and run this command

- run container runtime :
  apt-get install -y containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

modify config file : nano /etc/containerd/config.toml
- SystemdCgroup = true change this from from false to true by searching using ctrl+W .

- systemctl restart containerd
- Kernel Parameter Configuration
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
- sudo sysctl --system

Configuring Repo and Installation
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

- apt-cache madison kubeadm  : we see diff versions of kubeadm.
sudo apt-get install -y kubelet=1.22.0-00 kubeadm=1.22.0-00 kubectl=1.22.0-00      Here we installed older version of adm
sudo apt-mark hold kubelet kubeadm kubectl              we holded package

- Initialize Cluster with kubeadm (Only master node)
kubeadm init --pod-network-cidr=10.244.0.0/16  : when we run it we get 3 commands, run them

-  Install Network Addon (flannel) (master node)
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- Now run above 7 steps: now we are at 1.22.0 version. upgrade to 1.23.7:
- apt-mark unhold kubelet kubectl kubeadm         : unhold binaries
- apt-get update && apt-get install -y kubelet=1.23.7-00 kubectl=1.23.7-00 kubeadm=1.23.7-00
- apt-mark hold kubelet kubectl             : hold again
- kubeadm upgrade plan                    : running upgrade plan.here we see companontes which are in current version 1.22, and target is 1.23.7. it also show run kubeadm upgrade apply command to upgrade 
- kubeadm upgrade apply 1.23.7

- systemctl daemon-reload
systemctl restart kubelet

- Now run get nodes, we see cluster in latest 1.23.7 version.

- in exam kubeadm will be already setup. we just have to find versions using apt-cache to upgared to.remove hold, and run apt-install,upgrade plan,upgrade apply.

***********************************************************************************************************************************************************
OVERVIEW OF KUBERNETES FROM SCRATCH : 
- while building k8s from scratch we understand how variius k8s components are interconnected to each other to perform the entire orchestration
imp points : 
- we will be configuring k8s master and Wnode from scratch.in k8s master 4 imp and main components are there :
Kube API Server
Kube Scheduler
Kube-controller-manager
etcd
-in worker Node 2 components req: Kubelet, Kube-Proxy.
- so we taking 4 componenst from master and 2 from node, we look into config req at each component level,also know how these components are interconnected to each other.
- In domain1 PPT page.14 : workflow is associqated with creation of pod :
- in cli with help of kctl we run the command to create a pod : what happens behind scenes :here kubectls 1st send info to apiserver,api will write info to etcd DB(etcd is primary DB where all data is stored).once data is written in etcd,apiserver will send info to scheduler,because it happens that there are 100 Wnodes and in whoch node pod needsa to be scheduled, this decision lies at scheduler level.Now schedule rwill decide in which node pod should schedule, basedon which it inform back to apiserver.again api will write that info to etcd.after that apiserver will inform the kubelet associate with node.in dia we see node has kubelet and proxy. kubelet connected to api, and it is informed by apiserver that a new pod needs to be launched in your dserver.kubelet will go and connect with the container runtime(ex:docker) , here docker run command is used to create a new set of containers.once this is done the kubelet will update the status of pod whether it is running or not to api server, and api will store data back at etcd level.

- infra req for k8s clusterto build from scratch : 2 servers, 1 for master(4comps) and other for wnode(2comps), each contai their individual components.
	
Version Constraints : k8s is very popular orchestration tool. we see new versions release frequently.
 when we design a k8s cluster from scratch,it is imp to stick with spec version, because underying config that are req to deploy a k8s cluster and its associated components these configs also changing depending upon the k8s version. EX : we deploying a cluster using 1.24 k8s version means all components are based on 1.24 version.after 1 month 1.25 will come,if we follow instructions and config's associated with 1.24 version and try to use it in 1.25 version,it can happen that configs might not work, its not like evrything will change, only 2-3 % of config will change. if we adjust that 2-3% even our cluster of 1.25 or 1.26 cluster will run.if we dont know what exactly that 2-3 % config diff , our entire cluster of 1.25 or 26 wont come up.so it is very imp to foloow the exact version when we created earlier.

- In exam they dont ask to config cluster from scratch.they just gave us cluster, and one part of fxnal(compoent) is not working.

***********************************************************************************************************************************************************
CREATING INFRA FOR K8S FROM SCRATCH : 
- Launch 2 servers for master and wnodes in aws using ubuntu OS.

Common patterns for K8S cluster from scratch : common patterns while configing compkents of k8s cluster , there are so many components , we have to config individually,and there are common steps that we perform at individual components level,so if we understand common steps te config cluster will be easy.

1st common pattern : Certificates : we will be generating a set of certificates and keys for each component as part of the cluster for secure communication.
- we have a certificat authority and entities. each entity has a certificate and key.so this is common pattern.similiarly when we configure k8s components, we generate certificate and key for each component.

2nd common pattern : KubeConfig File : we using kubecofig file to connect to k8s cluster to create various objets for taht kubeconfig is rerqured. similiarly,even within the cluster for ,multiple components to connect to central component which is api-server a kube config is req for all remaining components.so kubeconfig will contain address of api-server,and certification, keys.also kubeconfig is not required for etcd database.
- in kubeconfig we give address where api is running.

3rd common pattern : Config for components : each component of a cluster will have unique set of config options that controls the fxnlaity. ex :i want to launch apiserver, it has so many fxnalities, depending upon the config that we enable,the apiserver will also enable its fxnalty.so the config options will differ based on components, but common thing is the config options needs to define fornthe component that we run.
ex : in k8s components there are many componets. select api, and open iT we see under options many many options available. depending upon the opts we define api will provide fxnalty based on option. same for other components like scheduler etc.

4th common pattern : Systemd File : Systemd is more of linux fxnality.here we create a Systemd unit config file that contains info aboiut a process that will be controlled and supervised by systemd.
Lets say i have systemd, in Systemd i create a service file associated with each component.like service for kubelet,api,scheduler etc.onvce we have service file integrated, Systemd will take care of it.letsay i want to restart a kubeapinserver,so we can tell systemd to restart the apiserver, and Systemd will do it .if we want to stop kubelet service, we infoirm Systemd to stop kubelet service, Systemd will do it for us.
- systemd also takes care of service by itself,we are stopping server and starting again,so when our system restarts,Systemd has the capability to automatically start each and every service if we tell it to do, if we do t , service wont start automaticaslly.

- https://github.com/zealvora/certified-kubernetes-administrator/tree/master/Domain%206%20-%20Cluster%20Architecture%2C%20Installation%20%26%20Configuration
- here while configuring each component we setup systemd file. go inside api compo link : under systemd : we gave alloptions of api.systemd starts api using all option s under it.

***********************************************************************************************************************************************************
PROVISIONING RELEASE BINARIES(bins) : 

- before we start configuring cluster . we have to first download the binaries associated all the components req for k8s master.we know 6 comps for both master and nodes.
- K8s release binaries are devided into 3 categories : server binary, node bin, client bin.

Server Binary : it contains binaries associated with master comps like api,controller etc
Node Binary : it contains binaries req at Wnode level kubelet,proxy.
Client Bin : Binaries required from client side which is Kubectl.

- https://kubernetes.io/releases/
- k8s docs associated with releases it will show latest release along with the option of changelog.press on specific version changelog : we see all binaries , click on any bin and we can dwnld them.we see many versions for binary.download any binary.

- login to both servers , and run mkdir /root/binaries.go inside and run wget on https/...tar.gz file. for 1.24 version.
- uncompress above tar file by " tar -xzvf on tar file" 
- now run ls -l k8s/server/bin : wesee all components binaries.

- in node server : go toroot/bin. copy node bins link for spec version and run wget in node, and uncompress it and run ls -l for k8s we see : we have kubelet,proxy,adm .
- etcd is also part of master. biut when we run ls-l above we dont see it in bin's,because etcd is a seperate project itself, so thats why we dont see etcd bin server bin's list.dwnld etcd bin's seperatly.

- in official etcd github repo we see versions and url's for each OS.select liunux amd-64 , and in root/bin/k8s/server/bin wget here thet etcd for ubuntu os link.and uncompress here.run ls -l etcd we see 2 imp components of etcd is etcd, etcdctl.

- Now we have all bins required for master and nodes.

***********************************************************************************************************************************************************
CONFIGURING THE CERTIFICATE AUTHORITY(secure cluster communication) : 
- we know multiple components on both mastewr and node side.we know there should be an internal communication b/w these components.but if communication in plain text it will be prone to lot of attacks. it is not mandatory to have all these components have a same server, for each compont they can have individual server.when communication is taking place b/w components withing server or across the servers, the security is very imp to take care.
- how can we ensure there is secure communication b/w components : this is where certificate comes into picture. whenever we visit youtube or google we will have https certificates ppresent,thriugh that secure communication happens b/w our browser and end server.this is wwe are trying to achiove as part of k8s cluster.

- when we discuss about PKI , one of the primary components that is involved is certificate authority,it is an enetity which issues digital certificates.
- key part is that both reciever and sender trust the certificate authorty.
 EX : we have a certificate authorty and 4 users.each user have a set of private key and cert.this cert is issued by certificate authorty. lets say user1(system1) and user2(system2) communicating with each other,then if system1 is presenting a cert,system2 can either decide to trust that cert or not,why it will trust certificate if it trust the primary certificate authorty which has issued the cert.

EX : passport : passpport signifies it has been signed and stamped by the govt of our country(ind). So for me to travel from ind to USA, here the us govt should trust the passport which was signed by the govt of india.
ex : open github, above click on secure connection, open it and check cert : we see certificate authorty is digicert.we see in cert , subject : CN(common name) : github.com,means whatever cert this https based website is using, this cert is issued to only a domain called as github.com. if we try to use this cert in another domain it wont work.also this cert has a public key, which will be used to initiate a secure connection as part of https protocol.
so basically a cert to get generate, a certificate authorty should sign that cert,since this github cert is sign by certificate authorty , and the browser (chrome) trusting the certificate authorty, hence we getting connection secure message for giuthub.

- So for all components in master and node need certificates.so cert to get issued we need a certificate authorty.
- certificate authorty also contains a cert and it contains a key.so 1st we have tpo create a certificate authorty cert and key.once both these available we can issue the cert of individual entities(components of mastrr and nodes)

- in master server :
Create base directory where all the certificates and keys will be stored
mkdir /root/certificates
cd /root/certificates
- 1st we have to generate a key before a cert authority cert is generated.
Creating a private key for Certificate Authority
openssl genrsa -out ca.key 2048   run ls we see ca.key is generated.once key isgenrated we have to create cert signing req(CSR). before cert is generated, we need a cert signing req which will contain certain info that will be present as part of issued cert.
 Creating CSR
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr              here we given subj Common name as k8s-ca. run ls we see ca.csr.then self sign this CSR.we use ca key to self sign the , in the o/p of the below command we get actual cert.
Self-Sign the CSR : 
openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial  -out ca.crt -days 1000                run ls we see ca cert available.after this we no longer need CSR fiule, remove tit by " rm -f ca.csr"
- run cat on crt we see it looks like a key. to see contents of it : run
See Contents of Certificate
openssl x509 -in ca.crt -text -noout        her we see CN name, validity, public key info.

- Now we have Cert uth and its cert and key. now we can issue signed cert and key for the entities(components of master and node)

***********************************************************************************************************************************************************
CONFIGURING ETCD : 

- we already discused much about etcd. it is a distributed reliable key-value store.we can considre otbas DB for k8s cluster.etcd reliably stores the config of data of k8s cluster,representing the state of cluster at any given point of time.
- imagine 1000's of worker nodes : data of all of them is stored in etcd DB.
- in k8s master diag we see multiple comp's are communicating with apiserver, but kubeapi server is only the component which connects to etcd DB, no other component will connect to etcd.

- before configuring etcd, we already CA in place.so for etcd we have to genarte CSR for etcd ,and send that csr to CA, and tell CA to review this CSR(info inside it), after review is completed the CA will sign cert and we will be presented with cert. this final cert we use it as part of ETCD component. same provcess applied to all the components.

- Pre-Requisite: Set SERVER-IP variable
SERVER_IP=139.59.28.67 (change this to your IP)   so whenever we echo server_ip we reference to ip of this server(master)
echo $SERVER_IP

- : Configure the Certificates:
cd /root/certificates/                 in tis dir we will be storing our certs and keys.
openssl genrsa -out etcd.key 2048            before cert gets generated, we have to generate a key.

- Step1 : create a key 
- Step2 : create a CSR 
- Step3 : to get CA to sign the CSR and in o/p we get cert.            these steps same for all other components.

- now we have etcd key.

- Note: Replace the value associated with IP.1 in the below step.
below we are createoing small configuration(cnf) which have certain info related to server ip , this will be computed automatically because we set $ser_IP

cat > etcd.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = ${SERVER_IP}
IP.2 = 127.0.0.1
EOF

- run ls we see etcd.cnf, run cat on it, we see server ip compited automatically.
- so now cnf created, below we createing csr and CN is etcd, also we specifying config is ecd.cnf.so talking info from cnf, our csr will be crewated,based on CN name of etcd.it also uses key as part of cert.
openssl req -new -key etcd.key -subj "/CN=etcd" -out etcd.csr -config etcd.cnf

- run ls we see csr. now wehave cser, we  need to sign csr from CA cert and CA key.so that we can get actual cert for etcd 
openssl x509 -req -in etcd.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out etcd.crt -extensions v3_req -extfile etcd.cnf -days 1000         here for etcd cser using ca cert and key we are signing it and in o/p we are getting actual cert which is etcd crt. now run ls we see etcd.cert.

- now create a new dir of etcd. store etcd ceet and key along with ca cert here.
Copy the Certificates and Key to /etc/etcd
mkdir /etc/etcd
cp etcd.crt etcd.key ca.crt /etc/etcd
- run etcd here we get here. so we have to mv bin's of etcd from the dir where we dwnldd binaries.

 Copy the ETCD and ETCDCTL Binaries to the Path
cd /root/binaries/kubernetes/server/bin/etcd-v3.5.4-linux-amd64/
cp etcd etcdctl /usr/local/bin/          here we copying etcd and etcdtl to bin dir in usr. now run etcd anywhere we get o/p.

- now config actual etcd along with certian config options.before that when we run etcd above, in o/p we see " serving client traffic insecurly, it is discouraged , " means when we run etcd by itself, it is going to start on a localhost on port of 2379.so while we st up k8s cluster, we dont want to setup compont in insecure manner.we want comp to be secure with all the appropriate config that is suitable for prod env.

- so this is why simply runnimg etcd from binary is not recommended approach.so when we run etcd , we also want to supply certain config's over here , some of configs related to cert,


Configure the systemd File
Create a service file:

cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \\
  --name master-1 \\
  --cert-file=/etc/etcd/etcd.crt \\
  --key-file=/etc/etcd/etcd.key \\
  --peer-cert-file=/etc/etcd/etcd.crt \\
  --peer-key-file=/etc/etcd/etcd.key \\
  --trusted-ca-file=/etc/etcd/ca.crt \\                                     here all these options we got from config-flags in k8s.io
  --peer-trusted-ca-file=/etc/etcd/ca.crt \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${SERVER_IP}:2380 \\
  --listen-peer-urls https://${SERVER_IP}:2380 \\
  --listen-client-urls https://${SERVER_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${SERVER_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master-1=https://${SERVER_IP}:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd                                              var/lib/etcd where data is stored
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


- from exam point we dont have to know about above all options, we just have to know how a component will be setup in k8s master at prod level using config-flag options.
- so above we created systemd filewhich starts etcd  using config-flags component 
- when we run aboive cmd in o/p : etc/systemd/system/etcd.service , run cta on serv file we see all our config and binaries stored,so syetemd should start bin along with the given config.

-
systemctl start etcd          whenever we do systemctl start etcd, what systemd will do is(ExecStart=/usr/local/bin/etcd which is weritten insystemd file above) here it will start all the commands to start etcd. so this is the role of systemd, thats why we stored etcd config under systemd dir.
systemctl status etcd
systemctl enable etcd   (whenever our system restarts , systed automatically starts etcd comp, otherwise if we dont run this enable comd, all our comp's will be in stopped state we have to start manualkly them)

- Now etcd is up and running,now use etcdctl command to connect to etcd cluster and retrieve some data.
etcdctl --endpoints=https://127.0.0.1:2379 get foo    : thrown error: x59 cert signed by unknown authority,this is why once we have cert based cponfig in place,specifically if we have self sign cert,then when ever we run etcdctl we also have to provide appropriate ca cert along with appropriate command

- ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key put course "kplabs cka course is awesome"                     ( here we know etcd running on port 2379 on localhost,so we are setting it as endpoint,we are seupplying ca and etcd cert, etcd key,and then we are running command (put some data is kp coyurse is awesome)

- here we are retriveing data(course) from etcd :
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key get course
- above in 1st cmd we done put cmd and in 2nd cmd, we done get cmd.
- now api also need to put and retrieve data from etcd.so even api also need to config with cert and key.

***********************************************************************************************************************************************************
CONFIGURING KUBE-API SERVER : 

- API server acts as a gateway to the Kubernetes Cluster.
- When you interact with your Kubernetes cluster using the kubectl command-line interface, you are actually communicating with the master API Server component.

- The API Server is the only Kubernetes component that connects to etcd; all the other components must go through the API Server to work with the cluster state.any data that has to written to etcd, it has to go through api server only.
- we have to do same config that we have done at etcd level like generating api related key, and from key we generate csr, and if there are any config we supply that config as part of csr, csr through CA will be verified,we get final cert associated with this api component.

Pre:Requisite Step: Move the kube-apiserver binary to /usr/local/bin directory.
cd /root/binaries/kubernetes/server/bin/               (this is where our all bin's stored. here kube-api bin is there copy it and paste it in csr/local)
cp kube-apiserver /usr/local/bin/

- Ensure that the SERVER_IP variable is still set.
- Generate Configuration File for CSR Creation.
- cd /root/certificates       dir contains all our cert's.
cat <<EOF | sudo tee api.conf           this is a config file based on which csr also get generated.
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 127.0.0.1
IP.2 = ${SERVER_IP}
IP.3 = 10.32.0.1
EOF

- Generate Certificates for API Server
openssl genrsa -out kube-api.key 2048       generating a key
openssl req -new -key kube-api.key -subj "/CN=kube-apiserver" -out kube-api.csr -config api.conf             generating csr, also specified api.conf
openssl x509 -req -in kube-api.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-api.crt -extensions v3_req -extfile api.conf -days 1000      here we are signing that api csr , and in o/p we have api.crt.

- run ls we see api cert and key. also we need one more cert related to  SA.

Generate Certificate for Service Account:
openssl genrsa -out service-account.key 2048
openssl req -new -key service-account.key -subj "/CN=service-accounts" -out service-account.csr
openssl x509 -req -in service-account.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out service-account.crt -days 100
- run ls , we have SA cert and key.
- whenever we generate cert we generate a CN through which a comp is identified. once we have certs in place we have to create a dir (var/lib/k8s) and copy all cert that api needs.

- Copy the certificate files to /var/lib/kubernetes directory
mkdir /var/lib/kubernetes
cp etcd.crt etcd.key ca.crt kube-api.key kube-api.crt service-account.crt service-account.key /var/lib/kubernetes
ls /var/lib/kubernetes : we get all files req by kube-api server to start.

 Creating Encryption key and Configuration
ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)        : here we created encrypt key, run echo $encrkey : we get base64 o/p.
cat > encryption-at-rest.yaml <<EOF                  : why dis encryption file needed : ( kube-api server will be storing data in etcd like secrets which 
kind: EncryptionConfig                                 are very sensitive,we dont want to store them as plaintet in etcd.if they are in plain when we retrie
apiVersion: v1                                         data from etcd we get in plain text.so thats y we are creating an encryption key through which secret
resources:                                             wpould be encrypted.
  - resources:
      - secrets
    providers:
      - aescbc:                            here we used aesbc as provider, we ca use KMS also.wehere we can also integrate 3rd party tools.
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF

- the above is yaml file of encryconfig file,it have the encry key,we also emtion what are the resources to be consider as part of encryption, here resources are secrets.
- we used above yaml to encrypt the data asspciated with resources  secrets whenever it gets stored in etcd level.
- cp encryption-at-rest.yaml /var/lib/kubernetes/encryption-at-rest.yaml     in var/lib all other files are stored.

 Creating Systemd service file:
Systemd file:
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service           here we start apiserver with ceratin options(below). because when we run k-apiserv
[Unit]                                                                    directly we get error to specify etcd server,also SA is req flag with issuer, key.
Description=Kubernetes API Server                                  so it is not like if we run api it will run , it wont .
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \               ( here we gave 1 backslash(\) so o/p will be dirty. if we give 2(\\) o/p will be format based) 
--advertise-address=${SERVER_IP} \
--allow-privileged=true \
--authorization-mode=Node,RBAC \
--client-ca-file=/var/lib/kubernetes/ca.crt \
--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
--enable-bootstrap-token-auth=true \
--etcd-cafile=/var/lib/kubernetes/ca.crt \
--etcd-certfile=/var/lib/kubernetes/etcd.crt \
--etcd-keyfile=/var/lib/kubernetes/etcd.key \
--etcd-servers=https://127.0.0.1:2379 \                         we sepcifyng etcd runninh on LOcalost and port.also to conne t to etcd, we needcert and key
--kubelet-client-certificate=/var/lib/kubernetes/kube-api.crt \         so above we gave cert, key to connect to etcd.
--kubelet-client-key=/var/lib/kubernetes/kube-api.key \
--service-account-key-file=/var/lib/kubernetes/service-account.crt \
--service-cluster-ip-range=10.32.0.0/24 \
--tls-cert-file=/var/lib/kubernetes/kube-api.crt \
--tls-private-key-file=/var/lib/kubernetes/kube-api.key \
--requestheader-client-ca-file=/var/lib/kubernetes/ca.crt \
--service-node-port-range=30000-32767 \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/var/log/kube-api-audit.log \
--bind-address=0.0.0.0 \
--event-ttl=1h \
--service-account-key-file=/var/lib/kubernetes/service-account.crt \
--service-account-signing-key-file=/var/lib/kubernetes/service-account.key \
--service-account-issuer=https://${SERVER_IP}:6443 \
--encryption-provider-config=/var/lib/kubernetes/encryption-at-rest.yaml \
--v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

- run it and verify " cat /etc/systemd/system/kube-apiserver.service " here we see contents of api syetemd"

Start the kube-api service:
systemctl start kube-apiserver
systemctl status kube-apiserver
systemctl enable kube-apiserver

***********************************************************************************************************************************************************
TROUBLESHOOTING FAILED CLUSTER COMPONENT : 
- how to res=olve if any comp is not working . ex: api is not working. run "systemctl status kube-apiserver" it says running. but it might happen that due to some reason the api is relly not working" how to troubleshoot it .
- demo : open /etc/systemd/system/kube-apiserver.service  and add a wron syntex here.and save it and run " systemctl daemon-reload". and run "systemctl restart kube-apiserver ". and run status we see failed, in description it is loaded but failed.if we want to checl why comp is not wprking we can check logs .to check logs :
- "journal ctl -u kube-apiserver" : here in logs we see "unknown flag: --advertise-addressa"means there is a issue with this specific flag due to which api server is not really working.
- if we have any issue with journal ctl run "systemctl restart systemd-jounrald"
- so now remove what we have added wrongly in service file, when ever we modify service file we have to run "systemctl daemon-reload". systemctl restart kube-apiserver. run status we see it is running.

***********************************************************************************************************************************************************
CONFIGURING CONTROLLER MANAGER(CM) : 

- CM interacts with api server to move the current state to the desired state.CM contains multiple sub comp's(Node controller,ENdpoint controller, replication controller,SA & token controller each are respons for specific task)
- A controller tracks atleast one k8s resource type , these resource type have a spec field that represent the desired state. The controler associated with that resource is responsble for making the current state come close to that od desired state.
- current state is the state of resource that is currently running in k8s cluster.SA & Token controller : whenever a new NS is created we have seen automatically an associated SA will be created and depending upon k8s we using an appropriate token also generated to that SA.ALl these creating auotmation is taken cxare by CM.

- cd /root/certificates :
openssl genrsa -out kube-controller-manager.key 2048     even CM also need cert's and keys for secure communication.
openssl req -new -key kube-controller-manager.key -subj "/CN=system:kube-controller-manager" -out kube-controller-manager.csr
openssl x509 -req -in kube-controller-manager.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-controller-manager.crt -days 1000

- run ls we see CM cert, and its CNAMe:kube-controller-manager

-  Generating KubeConfig : we know k8s has multiple comp's like scheduler, controller etcd they interact with api server.in order for CM to interact with api it need details like ip of apiserver, also cm needs to authenticate with API server.to achieve it we creating kubeconfig file which will basically contains ip of api, also auth related details.

- 1st move the kubectl binaries to usr/bin dir : cp /root/binaries/kubernetes/server/bin/kubectl /usr/local/bin

- kubectl config set-cluster kubernetes-from-scratch \                      kctl config for kube-config
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig

- now run cat kube-controller-manager.kubeconfig : we see certificate-authorit data is populated,apiserver data is also populated.

- kubectl config set-cluster kubernetes-from-scratch \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \               here we generating cred's.and also providing cm's certs and key's.
    --client-certificate=kube-controller-manager.crt \
    --client-key=kube-controller-manager.key \
    --embed-certs=true \
    --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
    --cluster=kubernetes-from-scratch \
    --user=system:kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig

- now agin run "cat kube-controller-manager.kubeconfig " : we users as kube-controller-manager,we have client cert data and key data, which are CM key and certs actually.
- kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig

Copying the files to kubernetes directory
cp kube-controller-manager.crt kube-controller-manager.key kube-controller-manager.kubeconfig ca.key /var/lib/kubernetes/

- copy binaries of CM to usr/bin : cp /root/binaries/kubernetes/server/bin/kube-controller-manager /usr/local/bin
- Configuring SystemD service file:
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\                 we are referencing CM's binary here.
--bind-address=0.0.0.0 \\
--service-cluster-ip-range=10.32.0.0/24 \\
--cluster-cidr=10.200.0.0/16 \\
--kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
--authentication-kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\     here we are giving auth and auzn kubeconfig associated with CM.
--authorization-kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\     means KCM will use this kconfig file to find ip of apiserver and
--leader-elect=true \\                                                                auth to trha api server.
--cluster-signing-cert-file=/var/lib/kubernetes/ca.crt \\
--cluster-signing-key-file=/var/lib/kubernetes/ca.key \\
--root-ca-file=/var/lib/kubernetes/ca.crt \\
--service-account-private-key-file=/var/lib/kubernetes/service-account.key \\
--use-service-account-credentials=true \\
--v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

- systemctl start kube-controller-manager      start kcm
systemctl status kube-controller-manager       check status it is active
systemctl enable kube-controller-manager       Now enable KCM.

***********************************************************************************************************************************************************
CONFIGURING SCHEDULER : 
- a scheduler watches for newly created pods that have no nodes assigned. for every pod that the scheduler discovers , the sceduler becomes responsible for finding the node for that pod to run on.
- lets say there are 500 nodes. in which node the pod should be scheduled , this is part of scheduler.
- whenever user req for a pod, api will ask scheduler where exactly that pod should assign, in which Wnode the pod should be assigned, scheduler will assign that pod in spec node.

There are several factors which are taken into consideration before a pod is scheduled to a node.Some of these includes:
- Resource Requirements ( lts say we have our appl which require min 8gb ram,we dont want scheduler to assign a node which has only 4gb ram to that specific pod,so user can say he need 8gb, so scheduler can schedule a pod to worker node which has that req)
- Hardware/Software policy constraints ( like pod can run on ubuntu only, not centos)
- Affinity & Anti-Affinity
- Data Locality

Generate cert :
cd /root/certificates
openssl genrsa -out kube-scheduler.key 2048
openssl req -new -key kube-scheduler.key -subj "/CN=system:kube-scheduler" -out kube-scheduler.csr
openssl x509 -req -in kube-scheduler.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-scheduler.crt -days 1000
- run ls we see scheduler cert and key.

- Generate KubeConfig File :
{
  kubectl config set-cluster kubernetes-from-scratch \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \                  locn of api-server.
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.crt \
    --client-key=kube-scheduler.key \
    --embed-certs=true \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-from-scratch \
    --user=system:kube-scheduler \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
}

-  Copy the scheduler kubeconfig
cp kube-scheduler.kubeconfig /var/lib/kubernetes/

- Configuring SystemD service :
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service  
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\                                       we referencing scheduler bin
  --kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\
  --authentication-kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\       we gave cert and key for auth and auzn to apiserver.
  --authorization-kubeconfig=/var/lib/kubernetes/kube-scheduler.kubeconfig \\
  --bind-address=127.0.0.1 \\
  --leader-elect=true
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


- cp /root/binaries/kubernetes/server/bin/kube-scheduler /usr/local/bin            copy all the bin's to usr/bin dir.
systemctl start kube-scheduler
systemctl status kube-scheduler
systemctl enable kube-scheduler


- till now we have all comp's required by a master ,and we shopuld be able to connect as part of k-apiserver and perform some oprn's. since Wnode side Config is pending. now we can Create NS , secret without node as well.

***********************************************************************************************************************************************************
VALIDATING CLUSTER COMPNT STATUS :
- till now  we have all comp's required by a master(etcd, scheduler, controller,api).before creatinga ny objects like NS, Secrets, 1st check kubectl command.kubectl comman dis used to connect to master(apiserver), where auth will be needed.for that user nedd to have config file that connect to master and create objects.

- Generate Certificate for Administrator User. admin through certs(which are poart of config file below) will be able to run kctl command on master
cd /root/certificates
openssl genrsa -out admin.key 2048
openssl req -new -key admin.key -subj "/CN=admin/O=system:masters" -out admin.csr               CN: syst:masters are admin name in master.
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out admin.crt -days 10

- Create KubeConfig file for admin,here we are adding admin cert that is created above,a dn key also for auth.
{
  kubectl config set-cluster kubernetes-from-scratch \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${SERVER_IP}:6443 \                   locn where kubeapi is running
    --kubeconfig=admin.kubeconfig

  kubectl config set-credentials admin \
    --client-certificate=admin.crt \
    --client-key=admin.key \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-from-scratch \
    --user=admin \
    --kubeconfig=admin.kubeconfig

  kubectl config use-context default --kubeconfig=admin.kubeconfig
}

- in o/p we get admin.kubeconfig run cat on it , we get api server ip, clent cert,and key.

- kubectl get componentstatuses --kubeconfig=admin.kubeconfig     ( check comps statuses also give config file manually) we get all other 3 comps(etcd,scheduler,controller are working fine)
- run get nodes withoiut mentioning config file we get error(Lhost refused). So any command that we type we also have to supply kubeconfig file.
- to avoid specifying kubeconfig every time , mv this file to kubeconfig folder associated with root user.
- cp /root/certificates/admin.kubeconfig ~/.kube/config       : now run compstatus without mentioning config file we get o/p.

- Verify Kubernetes Objects Creation :
 kubectl create namespace kplabs
kubectl get namespace kplabs -o yaml
kubectl create secret generic prod-secret --from-literal=username=admin --from-literal=password=password123
kubectl get secret   : we get our secret.
- means our cluster is working fine.

***********************************************************************************************************************************************************
WORKER NODE CONFIGURATION : 

- Kubernetes Worker Node consists of two major components

  1.Kubelet : it is basically an agent that runs on every node within the cluster.Kubelet makes sure that the containers are running in a relevant pod.if we look into diag both kubelet and kube-proxy both interacts with api-server.so lets say scheduler has decided that a specific pod should run in worker node 1, now scheduler through apiserver will send the message through the kubelet of that worker node 1 to go ahead and run the pod kubelet is responsible for doing this.

  2.Kube-Proxy : it acts as a network proxy which basically maintains the network rules on the host and also perform the connection forwarding.it might happens that various containers might want to communicate with each other in those aspects kube-pproxy proves to be imp.

- Pre-Requisite 1: Configure Container Runtime. (WORKER NODE)  
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

- modprobe overlay
modprobe br_netfilter

-cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf               kernel parameters
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

- sysctl --system

- installation of containerd : 
apt-get install -y containerd
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml

- modify config file
nano /etc/containerd/config.toml
SystemdCgroup = true    make it as true in config file

- systemctl restart containerd

- cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

- sudo sysctl --system

- till now we installed the prerequisite for wnode. Now one more prerequisite is additional pckgs need to be installed,also copy binary associated with kube proxy,ctl.

- Pre-Requisites - 2: (WORKER NODE)
apt install -y socat conntrack ipset
sysctl -w net.ipv4.conf.all.forwarding=1
cd  /root/binaries/kubernetes/node/bin/
cp kube-proxy kubectl kubelet /usr/local/bin


Step 1: Generate Kubelet Certificate for Worker Node. (MASTER NODE)(we are genrtng cert for both compont and kconfig files also)
Note:
Replace the IP Address and Hostname field in the below configurations according to your enviornement.
Run this in the Kubernetes Master Node
cd /root/certificates

- cert gen req ca cert and key which are part of master node.

- cat > openssl-kplabs-cka-worker.cnf <<EOF               run this in master node
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kplabs-cka-worker          dns name of W node
IP.1 = 128.199.30.177              ip of wnode 
EOF

- generate key : openssl genrsa -out kplabs-cka-worker.key 2048           run in master
- openssl req -new -key kplabs-cka-worker.key -subj "/CN=system:node:kplabs-cka-worker/O=system:nodes" -out kplabs-cka-worker.csr -config openssl-kplabs-cka-worker.cnf
openssl x509 -req -in kplabs-cka-worker.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kplabs-cka-worker.crt -extensions v3_req -extfile openssl-kplabs-cka-worker.cnf -days 1000

- above we genrtd cert and key. run ls we see node key and cert.
- now generate cert for proxy.
 Generate kube-proxy certificate: (MASTER NODE)
openssl genrsa -out kube-proxy.key 2048
openssl req -new -key kube-proxy.key -subj "/CN=system:kube-proxy" -out kube-proxy.csr
openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out kube-proxy.crt -days 1000

- so for the communication to takes place b/w api and wnode compnts , we have to place the certs of klet and proxy in node server.because kubelet and proxy will use cert to authnticate with api.

- Copy Certificates to Worker Node:
This can either be manual approach or via SCP. Certificates: kubelet, kube-proxy and CA certificate.so now we will e cponnecting and transferring via SCP.if we are using AWS then we have to use kye-based auth as part of SCP.However we used pswrd baswd auth while creating servers,
- run " nano /etc/ssh/sshd_config" here we see pwswrd based auth=yes.
- create a user and pswrd  in node, so from master we use this user creds to login and copy files.
- useradd zeal
passwd zeal
zeal5872#      this is pswrd

- note : incase if we launched the server with pswrd based auth we dont have to run thses commands  "nano /etc/ssh/sshd_config, PasswordAuthentication yes, systemctl restart sshd ", incase if we have launched the server with key-based authentication then we have to run these 3 commands for pswrd auth to enable.

In-case you want to automate it, then following configuration can be used. In the demo, we had made used of manual way.

In-case, you want to transfer file from master to worker node, then you can make use of the following approach:

- Master Node:   scp is to copy commands
scp kube-proxy.crt kube-proxy.key kplabs-cka-worker.crt kplabs-cka-worker.key ca.crt zeal@161.35.205.5:/tmp       here ip is of Wnode.

- here we are copying files from master to node.

- now run ls in Wnode /tmp dir, we see all files.

- mkdir /root/certificates   create a dir of certs in root
cd /tmp
mv kube-proxy.crt kube-proxy.key kplabs-cka-worker.crt kplabs-cka-worker.key ca.crt /root/certificates    move certs from tmpt to ceerts in root

- Move Certificates to Specific Location. (WORKER NODE)
mkdir /var/lib/kubernetes                  we used this same locn for certs in master node
cd /root/certificates
cp ca.crt /var/lib/kubernetes             copy ca cert to var lib k8s
mkdir /var/lib/kubelet                 create a kubelet folder
mv kplabs-cka-worker.crt  kplabs-cka-worker.key  kube-proxy.crt  kube-proxy.key /var/lib/kubelet/                 mv all node, proxy certs and keys to klet.


- Generate Kubelet Configuration YAML File: (WORKER NODE)
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
      clientCAFile: "/var/lib/kubernetes/ca.crt"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
runtimeRequestTimeout: "15m"
cgroupDriver: systemd
EOF

-  Generate Systemd service file for kubelet: (WORKER NODE) :
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \                                   in systemd while running kubelet, we also have to provide path of where kubelet config
  --config=/var/lib/kubelet/kubelet-config.yaml \                     file.in this line it is which is created in above command
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
  --kubeconfig=/var/lib/kubelet/kubeconfig \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

-  Generate the Kubeconfig file for Kubelet (WORKER NODE)
cd /var/lib/kubelet
cp /var/lib/kubernetes/ca.crt .
SERVER_IP=IP-OF-API-SERVER                   server ip is of api server so kubelet kows where api is running.

{
  kubectl config set-cluster kubernetes-from-scratch \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${SERVER_IP}:6443 \                    api server addres
    --kubeconfig=kplabs-cka-worker.kubeconfig

  kubectl config set-credentials system:node:kplabs-cka-worker \
    --client-certificate=kplabs-cka-worker.crt \
    --client-key=kplabs-cka-worker.key \
    --embed-certs=true \
    --kubeconfig=kplabs-cka-worker.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-from-scratch \
    --user=system:node:kplabs-cka-worker \
    --kubeconfig=kplabs-cka-worker.kubeconfig

  kubectl config use-context default --kubeconfig=kplabs-cka-worker.kubeconfig
}

- above we setup kubeconfig.
- run cat on kplabs-cka-worker.kubeconfig : we see ip of api, certs to connect.

- rename config file : mv kplabs-cka-worker.kubeconfig kubeconfig

- till now we configured kubelet.

- Part 2 - Kube-Proxy

Step 1: Copy Kube Proxy Certificate to Directory: (WORKER NODE)
mkdir /var/lib/kube-proxy

- Step 2: Generate KubeConfig file for kube-proxy :
{
  kubectl config set-cluster kubernetes-from-scratch \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://${SERVER_IP}:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.crt \
    --client-key=kube-proxy.key \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-from-scratch \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
}

- mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig        here raname config file and move it to lib dir.

- Step 3: Generate kube-proxy configuration file for kube-proxy as well: (WORKER NODE)
cd /var/lib/kube-proxy 
cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml                   here in proxy dir we generting proxy config file
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF

Step 4: Create kube-proxy service file(systemd service file for proxy) :

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\                   here we calling proxy binary and referencing to the path associated with config file.
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


- so till now we done with node configuration so :
systemctl start kubelet
systemctl start kube-proxy
systemctl enable kubelet
systemctl enable kube-proxy

- systemctl status kubelet, systemctl status kube-proxy  both are running.

-Now run get nodes in master we see one node, but it is not ready. we will do it in next.

***********************************************************************************************************************************************************
CONFIGURING NETWORKING : earlier when we run get nodes , node was registered, but it is not ready.Now we setup network config , req for the oerall connectivity, so the status would also change.
Note: Step 1 to 3 will be on Worker node Step 4 will be on Master node.
Step 1: Download CNI Plugins:
cd /tmp

wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz

- Step 2: Configure Base Directories:
mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/run/kubernetes

Step 3: Move CNI Tar File And Extract it.
mv cni-plugins-linux-amd64-v1.1.1.tgz /opt/cni/bin               move archive file which is downloaded in step 1
cd /opt/cni/bin
tar -xzvf cni-plugins-linux-amd64-v1.1.1.tgz         extracting that file.

- run /opt/cni/bin : run ls, we see plugins dwnlded list.

Step 4: Configuring Weave (Run this step on Master Node) - IMPORTANT
The cloud.weave.works command that was used in the video will no longer work as the website is now down. Instead you can use the following command to setup networking.

- before running below command, install net-tools, and run ifconfig we see 3 interfaces(eth1,2). now run route -n we see routes that are part of server.
kubectl apply -f https://raw.githubusercontent.com/zealvora/certified-kubernetes-administrator/master/Domain%206%20-%20Cluster%20Architecture%2C%20Installation%20%26%20Configuration/weave-daemonset-k8s.yaml             herebwe are running kapply on weave.

- now run get nodes, we see status of node is ready.if it is still not ready after riunning all above commands, in Wnode run "systemctl restart kubelet"
- launch one nginx container in wnode, and get pods we see it. now in master, run exec -it on that pod we get error , to resolve in master go to "/etc/hosts" and copy the ip of public(eth0) and paste in hosts file (ip kplabs-cka-work ) 
- now do same login to nf=ginx , earlier error got resolved, now we got forbidden error due to permissions(here user is api-server, verb=create,resource nodes, and subresoure proxy)

***********************************************************************************************************************************************************
SETTING UP RBAC FOR API-SERVER : 

- above when we tried to login to pod, the user(api) dosnt have permissions.we can resolve through rbac.
- below we create clusterrole and Crolebinding.

Run in Master Node : 
cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:                 here we are giving resources list role is associated with
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF


cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User                               cluster role is applied to a user, and name is k-apiserver. because above suer of k-apiserver is getting error.
    name: kube-apiserver
EOF

- Now login to nginx pod using exec, we logged into it.
-Now we understand imp of RBAC. primarly because when we do k exec , command goes to api , and k-api will go and connect to worker node compn.SInce k-api doesnt have permissions it wont be able to perform oprn, so we agve permission to k-api user.

***********************************************************************************************************************************************************
CONFIGURING COREDNS IN CLUSTER : 

- how to configure DNS in cluster : 
- why dns is req in cluster : launch a pod from busy box image in master node.
 Create a BusyBox POD to test DNS
kubectl run busybox --image=busybox:1.28 --command -- sleep 3600
- run get pods, it is running, and login to it using sh, inside pod run " nslookup google.com" here we are not getting any response specifically to this dns name(google.com).
- nslookup google.com 8.8.8.8   : here we did nslookup on dns name along with we provided external dns server, now we get resopnse. 
- Means from cluster we are not able to perform resolution, but from an external dns server(8.8.8.8) whn we send query(ns goog.co) we get response.

- also do "ping google.com" since it is not able toperform name resolution on this record it wont work.
- ping 8.8.8.8  or ping on any public_ip : it works.

- SO what we need is we need to have DNS at cluster level.

- In cli connect to any aws managed eks cluster : and run "kubectl cluster-info" it will show k8s-controle-plane is running on "https.." also it will show coreDNS running at "https.."

- Now in our ec2 K8s master run " k cluster info " : it shows controle plane is running on "https...ipof ec2" and it wont show any dns address.

- Step 3: Install CoreDNS:
wget https://raw.githubusercontent.com/zealvora/certified-kubernetes-administrator/master/Domain%206%20-%20Cluster%20Architecture%2C%20Installation%20%26%20Configuration/coredns.yaml
- above coredns.yaml will be downloaded.
- now run "k apply coredns.yaml"

- now run get pods in NS of kube-system : we see 2 coredns pods are running.
- now login to busybox pod and run nslookup google.com we get response.also ping on google.com we get rerspoinse.

- now run cluster-info we get our coredns address which is working in our cluster.

***********************************************************************************************************************************************************
KUBELET PREFERRED ADDRESS TYPE :

- when we do k exec , the command goes to k8snmaster node.when we discuss mastere it is actually k-apiserver. at thi sstage we run k exec nginx bash, now asume this nginx pod is running on host of 'worker01.internal.com'. we have 2 ndes, but nginx running onnode1. now kube-api server to connect to this nginx pod,1st it wil need host where the pod is running( becaus eapi will connect to klet and kproxy), then it will go and perform exec permission on that node.

- So inorder for the entire exec -it to work,the k-api server should be to resolve the specific dns(node1.internal.com), if it is not able to resolve, then it wont be able to resolve.

- run get pods in master we see 2(busybox and nginx)are running. now remove hosts(nano /etc/hosts) remove the entry we added as part of hosts(ip_node kp-cka-worker)

- now login to any pod using exec : error :nslookup on 127.0.0.53;53 not working. means it is not ablt to resolve hostname associated with cka-worker.
- run get nodes. 1 ready. run describe on node : below we see internal_IP address : where it provides the ip address of worker node,and hostname of node.
- so when api tries to connectvto node, it uses hostname of Wnode.SInce this hostname is not resolvable, it will through error, unless and untill we have the appropriate entry in the etc/hosts.

- this is reaslly we dont want. imagine we have 100 Wnodes.we dont want entries of 100 WNodes to be added as part of etc/hosts in master. so instead we tell api server to instead of connecting to hostname,we say connect using internal IP.
- So this approach is referred and controlled by the "kubelet preferred address types "

- in k8s.io : within apiserver : under uptions we see :
--kubelet-preferred-address-types strings     Default: "Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP"  

- Now e tell explicitly to use internal_IP.
- Modify the Configuration file for API Service
nano /etc/systemd/system/kube-apiserver.service   : within execstart command add  "--kubelet-preferred-address-types InternalIP"
- systemctl daemon-reload
systemctl restart kube-apiserver

- now run exec -it on nginx we get o/p( logged into it)
- Here api server using internal ip to connect node.

***********************************************************************************************************************************************************
BREAKDOWN LEARNINGS ( what happens if certain comp's goes down) : 

- we discussed each comp't has a imp role in k8s cluster.
- for testing we bring down certain components and see the results. 
- now we bring down 3 comp's each at once (scheduler, kubelet,controller manager) :

Scheduler : watches for newly created pods with no assigned node,and selects a node for them to run
Kubelet : it takes care of running the container associated with pod 
COntroller manager : multi controller types : SA & token controller: creating default acnt,and API access token for new NS.

- in mastwer run get pods, we have 2.launch one more pod nginx2 from nginx image. run get pods, we see 3.
- Bring Down kube-scheduler component : 
systemctl status kube-scheduler   : running
systemctl stop kube-scheduler
systemctl status kube-scheduler   : inactive(dead)

- now launch one more pod nginx3 . run get pods, we see nginx3 pod is still in pending state.BEacuse the scheduler that is responsible for deciding in which node the pod will run that scheduler is not active.
- now describe nginx2 pod. in event types we see : default scheduler : successfully assigned nginx2 pod to the WNOde KP-CKA-WOrker.
- now for nginx3 since the scheduler has not assigned a WNode, it will be in pending state.
- now get pods, nginx3 still in pending state. systemctl start kube-scheduler. Now run get pods, we see all pods running,

- NOW KUBELET : it takes care of running the underlying containers on the worker-Nodes.
- Bring Down kube-scheduler (Worker Node)
- systemctl status kubelet  : running
systemctl stop kubelet
systemctl status kubelet   : inactive

- now launch one more pod nginx5 from nginx image in master node. now run get pods, we see nginx5 is in pending state.
- now describe nginx5 pod : in events we see : scheduler was successfully completed by assigning a node to the pod, but since the kubelet itself is not working that can create the associated containers,the pod will be in pending status.
- systemctl start kubelet. Now run get pods,now nginx5 pod is running state.

Controller MAnager : we test SA & token controllers : 
- k create NS NS1 : now run get SA in the NS of NS1. we get default one, this is the responsibility of trhe controller manager.what if CM goes down.

Bring Down kube-controller-manager (Master Node)
systemctl status kube-controller-manager  : running
systemctl stop kube-controller-manager
systemctl status kube-controller-manager 

- create 1 more NS NS2. now run " k get SA -n NS2 " : we get no SA created, because the CM associated with the SA going down.
- Also run get NS. we see all NS's. try to delet NS1. " k delete NS NS1 " we will stuck here. stop it and run get NS, we see our NS1 still in terminating state. Because the SA that are part of NS's are controlled by KCM.so it can have a diff kind of impact as well.

- systemctl start kube-controller-manager
- get NS. we see NS1 is terminated.ALso check SA of NS2. run get SA -n NS2 : we get def SA.

***********************************************************************************************************************************************************
BACKING AND RESTORING ETCD : 	

- ALl k8s onjects stored in ETCD. since everything is stoired here, it is very imp to backup ETCD if it is used as a backing store for our k8s cluster.
- till now we have been setting up cluster from scartch, we have been using etcd as a backing stoire,so we should know how to take backup contineously o ETCD DB, also should know how backup can be restored.SO in any event of disaster we should be able to backup and restore things.

- Backing of data can be accomplished in 2 ways : 
1.ETCD built-in snapsot
2.volume snapshot : we take snapshot of entire vol.IN case we are having k8s cluster in AWS,then we can directly take a backup of EBS vol. this is called Vol-snapshpot.

- BACKING AND RESTORING ETCD : 
- we will have 2 servers, each contain ETCD1,2. we store some data in etcd1, and we take backup of this, with help of etcd built in snap.once we take backup, then we restore that backup within 2nd server which is running ETCD2.then we verifybthe data that stored in etcd1 is present in etcd2

DEMO :  we have 2 servers.etcd-snapshot(1st server wherewe store some data and will be taking snapshotwith the 1st option). etcd-restore(2nd server : where we restore snap , that has been taken from 1st server)

- now connect to both ec2's in mobaxterm.
- now 1st we need to config ETCD in both servers.after that in serve1 take backup. and restore that backup in 2nd server.

Pre-Requisite: Common steps to configure etcd in both servers
- run below steps in both servers :
Step 1. Create base directories
mkdir /root/certificates
mkdir /root/binaries

Step 2. Install packages
yum -y install nano wget openssl tar gzip

Step 3. Download ETCD binary and copy them to the path
cd /root/binaries
wget https://github.com/etcd-io/etcd/releases/download/v3.4.0/etcd-v3.4.0-linux-amd64.tar.gz         here using wget we dwnlding etcd package
tar -xzvf etcd-v3.4.0-linux-amd64.tar.gz                         extracting it here
cd /root/binaries/etcd-v3.4.0-linux-amd64
cp etcd etcdctl /usr/bin/                 copying etcd, ctl binaries to usr/bin

Step 4. Turn Off SELinux and Swap    : like disabling SELinux and Swap 
setenforce 0
swapoff -a

Step 5. Generate Certificates:
cd /root/certificates
openssl genrsa -out ca.key 2048
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
openssl x509 -req -in ca.csr -signkey ca.key -CAcreateserial  -out ca.crt -days 1000
openssl genrsa -out etcd.key 2048

- run ls in certs, we see 1 cert and key for cert and etcd in both ec2's.

- Now genetrate etcd.conf :
cat > etcd.cnf <<EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = SERVER-IP-HERE         : replace ip of each ec2 when running in that particular ec2.
IP.3 = 127.0.0.1
EOF

generate cert with details CN.
openssl req -new -key etcd.key -subj "/CN=etcd" -out etcd.csr -config etcd.cnf
openssl x509 -req -in etcd.csr -CA ca.crt -CAkey ca.key -CAcreateserial  -out etcd.crt -extensions v3_req -extfile etcd.cnf -days 1000


Step 6: Copy certificates to etcd
mkdir /etc/etcd
cp etcd.crt etcd.key ca.crt /etc/etcd

- Step 7: Configure Systemd file:
cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/bin/etcd \\
  --name master-1 \\
  --cert-file=/etc/etcd/etcd.crt \\
  --key-file=/etc/etcd/etcd.key \\
  --peer-cert-file=/etc/etcd/etcd.crt \\
  --peer-key-file=/etc/etcd/etcd.key \\
  --trusted-ca-file=/etc/etcd/ca.crt \\
  --peer-trusted-ca-file=/etc/etcd/ca.crt \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${SERVER_IP}:2380 \\
  --listen-peer-urls https://${SERVER_IP}:2380 \\
  --listen-client-urls https://${SERVER_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${SERVER_IP}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster master-1=https://${SERVER_IP}:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

- Step : Start the service:

systemctl start etcd
systemctl status etcd     : running
systemctl enable etcd

- Now we config etcd in both servers with help of certs.


Steps for Server 1 (ETCD Backup)
Step 1. Add a sample data to ETCD database
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key put course "hemanth C"
- here we are storing some data , put course oprn and content is hemanth c.

Step 2. Verify if data is added successfully
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key get course
- we get o/p : hemanth c.

Step 3. Take snapshot from the primary ETCD :
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key snapshot save /tmp/snapshot.db
- above we running taking snapshot, and we are saving under the path of (/tmp/snapshot.db). so here we are using 1st capability(etcd inbuilt snapshot)
in o/p we get snapshpot, and it says it is stored at particular dir.
- in /tmp run ls we see snapshot.db.

Step 4. Check details of the snapshot
etcdctl --write-out=table snapshot status /tmp/snapshot.db
- here we checking status associatedwith snap, with the wroit e out table,in o/p we get table with all details pof snapshpt.

- till now we created a key value data in etcd and we have taken a snapshot of it.
- Now we move this snap to etcd2 server, and we restore that from that spec snapshot.

Steps for Server 2 (ETCD Restore)
Step 1. Create a user for transferring files from Server 1 to Server 2
- we creating user because we will copy snap from serve 1 to 2 using scp here.
- run in 2nd server :
useradd zeal
passwd zeal

- Step 2. Modify the SSH configuration to allow PasswordAuthentication
nano /etc/ssh/sshd_config
PasswordAuthentication yes
- systemctl restart sshd
- once we enabled PasswordAuthentication, we transfer data from server 1 to 2.

Step 4. Move the snapshot from server 1 to server 2      : run in server1
scp /tmp/snapshot.db zeal@SERVER2-IP:/tmp   : using scp command we are movinf snap to serv2, it will ask pswd, enyter it.

- in server 2 : in /tmp dir run ls we see our snap.db

Step 5. Perform the Restore Command
cd /tmp
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key snapshot restore snapshot.db
- above snapshot restore name_of_snapshot.

- snapshpot and restore command is same : snapshot save name_snap :: snapshot restore name_snap
- whenw e run above cmd it will create a default.etcd. dur, where all DB related files stores.

- ETCD basically stores ETCD in /var/lib/etcd.
- earlier we also will be having 2ndserver DB in  /var/lib/etcd here. remove it.

Step 6. Remove the files from /var/lib/etcd directory
systemctl stop etcd
rm -rf /var/lib/etcd/*

- Step 7. Copy the files from restored snapshot to /var/lib/etcd
mv /tmp/default.etcd/* /var/lib/etcd
- here we copying the snap which we restored in step 5 cmd,to var lib.

8. Start ETCD Service
systemctl start etcd

9. Verify the restore
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd.crt --key=/etc/etcd/etcd.key get course
hemanth C

- we get o/p here. so this key we stored in serv1 etcd,

***********************************************************************************************************************************************************
UNDERSTANDING NETWORK POLICIES - PART01 : 

- By default k8s pods are not isolated, they accept traffic from any source, also they can connect to any source.so at a network level tere is no restriction on them.
- A network policy is a specification of how group of pods are allowed to communicate with each other and other network endpoints.

EX : we have 3 pods.we have pod 1, i want to define a policy that pod1 should not have any internal or external communication.SO now ext endpoint should be able to connect to pod1,and pod1 shouldnt connect with any other pods or internet. How we define it, this is where network policy comes.

- interms of network policy :

Default Deny - Ingress : (Default Deny based on ingress) )
- Ingress= inbound rule, Egress= Outbound rule.

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}                     {} means all pods will be selected.
  ingress:
  - {}
  policyTypes:                   policy type is ingress. since we havent defined explicitly ingress rules, what happen is all ingress related traffic wll be 
  - Ingress                      blocked.However egress traffic will still be allowed to pass.if we have created NetworkPolicy in def NS, No new external 
                                 communication would connect to pods.How ever pod will be connected to new communication using outbound.

EX: since all ingress is blocked,New connections to pod would not be allowed. Pods would be allowed to communicate externally and return traffic will be allowed. 
- we have pod3,1,2.  pod1,2 belong to one common NS.pod3 belongs to diff NS.so if we have this kind of NetworkPolicy. if pod1 is trying to pod2,since pod1 is initiating a obound connection whic is allowed, However for Pod2 this is an Ibound connection, since we have deny ingress which is deny inbound, the NetworkPolicy will block this connection. as we discussing this NetworkPolicy applies to default NS. what if pod1 tries to communicate with pod3, for pod1 it is Obound,for pod 3 it is inbound connection. SInce we dont have a NEtwork policy into the NS of pod 3, we dont have any issue , this communication will be successful.

DEMO :
 we use 2 NS's, external and default.
- we run 2 pods(pod1,2) in def NS, and 1 pod(pod 3) in external NS.

- kubectl create ns external

- kubectl run pod-1 --image=praqma/network-multitool
- kubectl run pod-2 --image=praqma/network-multitool
- kubectl run pod-3 --image=praqma/network-multitool -n external

- run kubectl get netpol : no network policies associated with this NS(def) , same for external NS also.
  because we already discussed by def pods are non-isolated,they accept or send traffic from any source.

- kubectl get pods -o wide
- kubectl get pods -o wide -n external
now we get ip's pof each pod.

- login to pod1, and ping to ip of pod2.it will work. do the same ping to [pod 3 from pod 1. it will work. ping google.com, it will work.
kubectl exec -it pod-1 -- ping [pod-2-ip]
kubectl exec -it pod-1 -- ping [pod-3-ip]
kubectl exec -it pod-1 -- ping google.com

- Now implement default-deny ingress
- nano netpol.yaml : run it in cluster :

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

- apply netpol.yaml. now run get netpol we see a new network policy default-deny-ingress is available in def NS.
once we have deny ingress, any communication that is going to happens as Ibound to pod1 or pod2 will be regected.

kubectl exec -it pod-1 -- ping [pod2-ip]   wont work
kubectl exec -it pod-1 -- ping [pod3-ip]   it will work
kubectl exec -it pod-1 -- ping google.com  work

kubectl exec -it pod-3 -- ping [pod1-ip]  wont work
kubectl exec -it pod-3 -- ping [pod2-ip]  wont work

- incase if we want to allow ingress traffic,similiar to the podselector rule in above yaml where we havent selected anything,

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-allow-ingress
spec:
  podSelector: {}    {} for podseleftor means all pods
  ingress:              so now for all pods with in NS where netpol is cretaed for ingress we have alowed everything.
  - {}                {} for ingress means allow all ingress(allinbound rules)
  policyTypes:
  - Ingress

- delete the earlier netpol and apply this new netpol.

- kubectl exec -it pod-3 -- ping [pod1-ip]   it will work because Ibound is allowed
- kubectl exec -it pod-3 -- ping [pod2-ip]    it will work because Ibound is allowed

DEFAULT DENY EGRESS(OUTBOUND) :

nano netpol.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}                we specifying all pods, and we didnt specify any rule
  policyTypes:           policy type is egress.
  - Egress

- if we have this kind of NP then all the obound traffic from the pods will be rejected, but inbound will be allowed.
- apply above yaml, and before that delete old netpol.
- same pod1,2 in def NS, pod3 in external NS.

- kubectl exec -it pod-2 -- ping google.com              it wont work, because netpol applies, so egress not allowed from the NS where this pod is present.
- kubectl exec -n external -it pod-3 -- ping [pod1]      it work because the ingress is allowed for pod1,2, also when we ping to pod1, pod1 is replying to pod3(Q : all egress is blocked but how pod1 is responding to ping sent from pod 3, because of the stateful nature. see the above command pinging to google.com, it is new connection . if a pod3 is able to initiate a new connection to pod1 or 2 it is a new connection, but return reply is allowed here. however when we initiate a new egress connection these will be rejected.

PODSELECTOR : it will group the pods to which the policy should apply.
- so till now in above examples we are associating with all pods, now i dont want to associate with all pods, i want to associate with few only.so using PodSelector we can do it.

EX : we have pod1,2,3. in pod 1 there is some suspecious activity going on, i want to look into it in detail, but before that i dont want pod 1 to connect to any one, also i dont want any external entity should connect to pod1. So for this we can block both ingress and egress for pod1.to work podselector properly we add a label (role:suspecious) to pod1. so for podselector  instead of adding {} we add a slector of matchlabels of role:suspecious.

- delete old netpols.
- nano netpol.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: podselector-suspicous
spec:
  podSelector:
    matchLabels:
      role: suspicious
  policyTypes:
  - Ingress
  - Egress

- kubectl apply -f netpol.yaml

- kubectl label pod pod-1 role=suspicious       : we added label role to pod-1. so now above created netpol will be applied to this pod1.
kubectl get pods --show-labels

kubectl exec -it pod-2 -- ping [pod-1]    it wont work
kubectl exec -it pod-1 -- ping google.com  it wont work

- now remove label for pod1. and ping pod 1 from pod2, it works, also ping google from pod1 it will work.

***********************************************************************************************************************************************************
UNDERSTANDING NETWORK POLICIES - PART02 :-

- in network policies there are other parameters like from block, to block, NS selector.

INGRESS FROM SLECTOR : each rule allows traffic which matches both the from and ports sections.

- here under ingress we have spec rules. till now we used ingress{} norules means allowed everything.instead we can use from slector ( EX: Allow from 192.2.0.0/16  , Deny from 192.2.0.32/32.) apply this to NS where pod1 is present. So now pod1 will allow the given cidr block, and it will deny the deny IP.

- kubectl label pod pod-1 role=secure
- nano netpol.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-from-ips
spec:
  podSelector:
    matchLabels:
      role: secure             this NP will apply to pod of label role: sec. applythis label to pod1 which is in def NS.
  ingress:
  - from:
     - ipBlock:
        cidr: 192.168.0.0/16
        except:
        - 192.168.137.70/32    imagine this ip is of pod2.    so now login to pod2 and try to ping pod1, after applying this NP.yaml, it wont work.
  policyTypes:
  - Ingress

EGRESS TO SELECTOR : it allows outgoing connections only to a certain set of netwrk.

EX : we have app pod, DB pod, and xyz network. now since app pod communicating with db pod,as far as egress selector is considered we can say app pod on;ly can acommunicate with db pod and no one else. so when app pod try to connect with xyz network or any other pod it shouldn't work.

- delete earleir Npolicy.

- kubectl label pod pod-1 role=secure
nano netpol.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-to-ips
spec:
  podSelector:
    matchLabels:
      role: secure       imagine pod1 has this label.
  egress:
  - to:
     - ipBlock:
        cidr: 192.168.137.70/32                       imagine it is db pod  pod2 ip, app pod only will connect to it.
  policyTypes:
  - Egress

- consider pod1 as app pod, pod2 as Db pod, pod3 as xyz.
- kubectl exec -it pod-1 -- ping [pod-2]       : it will work
kubectl exec -it pod-1 -- ping google.com          : it wont work, because pod1 can only connect to pod 2 which is db.

NAMESPACE SELECTOR : it allows connections based on NS.

ex : lets say we have ftp pod in a NS, and pod1,2,3 in a diff NS.if we look into rule , rule states allow ingress connection to ftp pod from NS of app with pods label reconile.lets say pod1,2,3 belongs to NS app, only 1 pod with the label of reconcile will be able to communicate with ftp pod.

nano netpol.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: namespace-selector
spec:
  podSelector:
    matchLabels:
      role: secure
  ingress:                          here allow ingress from the pod that has a label of role:reconcile,that too only from the NS which has label of role app
  - from:
     - namespaceSelector:
        matchLabels:
          role: app
       podSelector:
         matchLabels:
           role: reconcile
  policyTypes:
  - Ingress

- kubectl apply -f netpol.yaml
- we can attatch labels to NS's also.
- now our NP is associated with pod-1 because this has a label of secure. so now pod which has label reconcile from NS of label app will be allowed.means pod2 nor pod 3 wont be able to connect to pod-1.

- kubectl exec -it pod-2 -- ping [pod-1]          it wont work
kubectl exec -n external -it pod-3 -- ping [pod-1]   it wont work

kubectl label pod -n external pod-3 role=reconcile          adding label to pod-3

kubectl label ns external role=app    adding label to NS
 
kubectl exec -n external -it pod-3 -- ping [pod-1]   it will work.


***********************************************************************************************************************************************************

DOMAIN 8 : LOGGING & MONITORING : 

***********************************************************************************************************************************************************
KUBERNETES EVENTS :

- K8s events are created when other resources have state changes,errors, or other messages that should be broadcast to the system.
 lets say i created a pod, i want to see more details about it,so we can look into events. in events we see from def-scheduler , this pod(nginx) has been assigned to a node of 2ndry. then we also have certain messages from kubelet that it pulled a image, and it has started container from it. when things are not working well,then k8s events is useful.

- K8s events provides insights into what is happening inside cluster, such as what decisions were made by scheduler or why some pods were evicted from node.

- run kubectl get events : we get no resources found in def NS. why till now we have been creating pods, and managing cluster since so long, bu why there are no events, because if we look into api server doc under " --event-ttl duration : default 1hour" amnt of time to retain events.so after 1 hour the evnents will not be retained.this is why even we created many pods, we couldnt find events.incase if we want events for longer period then we can connect external monitoring system with k8s that can store the events later.

- launch a pod from nginx, and run get in events we see from def-scheduler , this pod(nginx) has been assigned to a node of 2ndry. then we also have certain messages from kubelet that it pulled a image, and it has started container from it, beside each event we see scheduler, pulling,pulled,created, started.

- now delete any older pod that was created earlier,run get events, we see 3events created, and the the reason is killing "stopping container busybox".

- All the events are stored in master server. to avoid fillig master disk, a retention policy for events is enforced : events are removed one hour after the last occurance. to provide longer history and aggregation capabilities, a 3rd party soln should be installed to capture events.

Events and Namespaces : 

- not that the events are namespace : lets say we have a kp NS, and i want to see events related to this kp NS,then we have to explicitly specify the --NS kp, if we want to see the events of kp NS.

- run get NS, we see multiple NS's. now run get events -n heman. we dont find any events. run get events -n default, we see events.

***********************************************************************************************************************************************************
UNDERSTANDING FIELD SELECTORS : 

- field selectors lets you slect k8s resources based on the value of one or more resource fields.
EX : i want to get all pods running in this k8s cluster except the one that are part of default NS.it can happens 100's of NS's, and each NS has certain pods,i want to list all pods, execept those are part of def NS.
- so when we want to search for a specific type of object based on a specific constraint then we can make use of field-selector.

- List all the PODS from All Namespaces
kubectl get pods --all-namespaces     : we see many pods from diff NS's.

- List all PODS from ALL namespaces except default namespace
kubectl get pods --field-selector metadata.namespace!=default   : so in all pods whose metadata.namespace value is not equal to default, those pods will be listed.

- metadat is not just about NS, we can use other metadata as well (name etc) :
List all Pods from ALL namespace except a specific POD
kubectl get pods --field-selector metadata.name!=<ADD-POD-NAME-HERE>   : we can give any pod that we dot want it in the list.

- Create New Deployment
kubectl create deployment test-deployment --replicas 3 --image=nginx

- kubectl get events : we see many events related to test-depl : like pulling image, and running container from it etc.

Get Events with Output JSON
kubectl get events -o json   : here we see list of many events. under one event we see involvedObject :name , kind:depl,apiversion etc we can see, alos other events kind :pod, kind:RS,like diff objects events.SO if i want to look for the events related to specific pod : 

Q : i want all the pods but only associated with specific pod.no other pods, deply's, RS's pod should be listed.
- Get Events Associated with a Specific POD
kubectl get events --field-selector involvedObject.name=event-pod  : now we see events associated with this specific pod.

DEf configuration : so as part of def config, whenever we run commands like k get pods etc, at this stage by def no slectors/filters are applied meaning  that all resources of that specific type are selected for that NS.

- when we run k get pods : it is equivalent to k get pods --field-selector ""   : "" is empty contenst 
- kubectl get pods  : we get 4 pods,
- kubectl get pods --field-selector ""    : here alsowe get the same 4 pods.

***********************************************************************************************************************************************************
MONITOR CLUSTER COMPONENTS : 

- we discussed how kubelet-works and how it will deploy the pods in nodes, aprt from that :
One of the imp fxnalities of kubelet is to retrieve metrics and expose them through the kubelet summary API.this fxnality of retrieving the metrics achieved with the help of subcomponente called as cAdvisor

- so we have kubelet as a component and there is one sub-component inside kubelet is called as cAdvisor.

WOrkflow : we have master on left and Wnode on right. in Wnode we have sub-comp called as cAdviser. cadvisor is collecting the metrics from pods which are inside that node,and these metrics are exposed via the kubelet. Now in master we have a metric server,this metric server will collect and aggregate thos metrics,and then it is exposing via the kube-api server. so once we have metric server , then we can run commands like kubectl dock etc.

- https://github.com/google/cAdvisor
- in website we see cAdvisor itself is a big component.we can run cAdvisor even if we dont have k8s env.how ever cAdvisor is now a sub-comp in kubelet, so we dont have to install additional module of it in Wnodes.

DEMO : i already have metric server installed.

- when we usually run kubectl top pods: it will give various resource statistics info like CPU and memory for the pods which are running.
-  kubectl top pods --all-namespaces : it will give various resource statistics info like CPU and memory for the pods in all NS's.

- kubectl top nodes : if we want to see CPU and memory utilization in Nodes.many times our pod or appl is not fxning as expected.so with the help of k top nodes we can get cpu and memory util of Nodes.

- run get pods, we see a pod of metric server.we discussed if we want to run commands like k top pods or nodes, then we will need to have a metric server deployed.

KUBE-STATE-METRICS : kube-state-metrics is a simp,e service that listens to k8s API server and generates metrics about the state of the objects.
- the aim of kube-state-metrics is not towards the health of the individual clusters components.but it rather focuses on the health of the various objects inside cluster such as deployments, nodes, pods etc.

- currently i have kube-state-metrics deployed in k8s cluster in aws.click on that server in aws, we see i have a great insights(understanding via the graphs, charts) insights related to deply's, pods, nods. in pod deplys it says we have 19 deplys, means we have 19 pods which are running.pods req is 19, available is 19.it also has metric associatwd with pod-unavaialble : 0. we can check insights from when the pods launched.

- there is a doc related to AWS-advanced-k8s-metrics : for k8s advncd metrics , it suggests to deploy a kube-state-metrics. so this kube-state-metrics allows us to see more info related to various objects within the k8s cluster.

DEPLOYING METRIC SERVER : 
- when ever we launch k8s cluster by default the metric server is not available.when we run kubectl top pods: we see metrics API not available. the overall installation of metric server is easy. in github page of Zeal, there is a yaml file, if we appy that yaml file in our k8s cluster, the metrics server will automatically gets installed. then run k top pods or nods we get o/p.

***********************************************************************************************************************************************************
UNDERSTANDING DOCKER LOGGIN DRIVERS : 

- typically in linux or unix subsystem , whenever we run unix or linux command they open up three i/p o/p streams STDIN, STDOUT,STDERR.similiar apples for the docker containers that we are running.lets say we are running dock containers, these would have STDIN, STDOUT,STDERR. we already discussed about IT-flags. in docker we usually specify -i and -t flag, this basically attach our terminal to stdin,(stdin is where we provide the i/p to speicifc process or command). we have a bash process running.
- in Docker Section 7 Storage pdf in CKA folder : we have a bash process running.to this bash we can provide cmds like ls or cd.these cmd's come from keyboard this is STDIN. STDOUT is : nginx can have lot of access logs within its standard O/P's.lets say nginx running on port 80 and 100's of people are connecting to it,so these logs nginx can share it to STDOUT.we have STDERR: where we have error related logs will be present. So docker allows us to monitor STDOUT, STDERR with the help of various logging drivers. because many times it is imp to see what is STDOUT and ERR data, thes can be achieved with the help of various logging drivers.

DEMO : run docker ps, no containers.
- run " docker container run  -d --name busy busybox ping google.com"  run ps we have busy cont, and command is ping google.com. so the o/p of this particular command"ping gg.." which is running , this o/p it can share it to the STDOUT. so if we want tp see STDOUT or STDERR associated with this container we can make use of docker logs. run docker logs busy : we see o/p associated with the command logs for google.com seerver.
- docker logs command allows us to get stdout,err associated with container. the o/p we get when we run docker logs cont, the o/p might be stored in some kind of file. lets say we might want to send this o/p in a centralaise locn, so depending upon our req we can make use of logging drivers which are available in docker.

- run docker info : within log we see various logging drivers avaialble like ( awslogs, fluentd,gcp,local,splunk etc), these are logging drivers available for docker. depending upon req, letsay i want to send all logs of docker(ex :we did pin on google, i want to send all of thos stdout logs to AWS cloudwatch service), then we can make use of logging driver of awslogs.if we want to send it to splunk , then we can use splunk driver.same for syslog also.

- run " docker info | grep -i "logging driver"  : in o/p we get default logging driver: json-file.so anytime when we create a container and we dont specify type of logging driver, by def docker will associate json-file logging driver with cont.
- run docker inspect busy , which is created above. in that inspect info , we see logconfig : json-file.also when we ran docke rlogs busy above, the o/p we got , that o/p will be stored in a json-file. to check it when we run inspect we get 'logpath :filepath'.this the log path where all logs getting stored.

- if we wan to create new container with diff logging driver : docker container run  -d --name busy1 --log-driver none busybox ping google.com"
- run ps we sse busy1. it has a log driver of none, means this will not capture stout,stderr. run docker logs busy1 : error response from daemon : configured loggin driver doesn't support reading " so depending upon the type pof log-driver we chose the amnt of config will change.

- we already seen diff types of logging-driver in docker above.but the docker logs command is not availble for drivers other than json-file and journeld.whenw e ran docker logs it entirely shown o/p, the reason it shown o/p is the driver we were using is json-file.if we are using some custom driver like awslogs or splunk, then the docker logs wont show any o/p.

- also depending upon the log driver we choose there can be certain caveats that can happen. lets say we chose syslog driver,and we have central syslog server,and due to some reason it went down,then there can be a chance that whatever logs that has been shown in stdout , they will be lost.so maksure whichever driver we implement for our env understand about adv and disadv.

***********************************************************************************************************************************************************
MONITORING APPLICATION LOGS : 

- earlier we discused about logging-aspects as container is concerned.now similair aspect applies to k8s also,because in the end k8s also launches the containers.

- pod.yaml :

apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
spec:
  containers:
  -  image: busybox
     name: mywebserver
     command: ["ping"]
     args: ["google.com"]

- so this will reate a busybox container, and it will contineously ping google.com.
- apply it and run get pods, we see running. now run kubectl logs pod_name(mywebserver). here we get standard o/p of logs, which is basically the o/p when ping command happens(o/p is 64 btes. when we usually in cli ping google.com o/p we get is 64 bytes. exactly the stdout of logs for pod).

- when we are debugging when things are not working. lets say we have metric-server unning as pod in kp NS. now run k logs metric-server-pod --ns kp.  we see o/p(basivcally the logs associated with appl which is running inside metri server)
- k logs are useful when things are not working well.

- manier times pod can have multiple containers :
-pod2.yaml :
apiVersion: v1
kind: Pod
metadata:
  name: multi
spec:
  containers:
  -  image: busybox
     name: ping-domain
     command: ["ping"]
     args: ["google.com"]             we are pinging domain of google.com
  -  image: busybox
     name: ping-ip
     command: ["ping"]
     args: ["8.8.8.8"]              we are pinging ip address.

- here we have 2 cont's within pod. apply above pod2.yaml, and get pods.we see 2/2 container in multi pod is running.
- now run k logs multi pod. we get error : it says this pod has multiple containers, in error we also see names of containers.so whenever we want to look into pods associated with pod which has multiple containers,then not only we have to specify the pod name, but we also have to specify container name with the pod.
- run ' k get logs multi ping-domain' we get o/p of logs.same for 2nd container.

***********************************************************************************************************************************************************
MONITORING CLUSTER COMPONENT LOGS : 

- for every comp within cluster all of them have the logs files associated.when we are troubleshooting any comp the logs become benificial.

- if we are making use of systemd, then we can view the component level logs with journalctl command. we used journalctl while we are configuring k8s cluster from scratch

- in k8s.docs under 'looking at logs' : we see logs associated with var compnents 

Here are the locations of the relevant log files. On systemd-based systems, you may need to use journalctl instead of examining log files. means if we are making use of systemd, we no longer have to go below log files ( like /var/log/kube-apiserver.log) instead we can use journalctl to look into log files associated with specific unit.

for master comp's :
/var/log/kube-apiserver.log - API Server, responsible for serving the API
/var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions
/var/log/kube-controller-manager.log :  a component that runs most Kubernetes built-in controllers, with the notable exception of scheduling (the kube-scheduler handles scheduling).

Worker Nodes comps : 
/var/log/kubelet.log - logs from the kubelet, responsible for running containers on the node
/var/log/kube-proxy.log - logs from kube-proxy, which is responsible for directing traffic to Service endpoints
Cluster failure

- in k8s master that we configured earlier go to " /etc/systemd/system/ kube-api.service : this is the unit of k8s-api server.if we want to check log files associated with this  kube-api.service, then we can use journal ctl. run " journalctl -u kube-apiserver" now we get all logs assciated with k-api.we see many logs, if we want to go to last line of logs press ( shift + g) we go to last line of logs.
- also run " journalctl -f -u kube-apiserver " ( -f means it is doing a watch, like any new logs that are coming that will come under this command o/p).

- in master run date we get time. run " journalctl --since "2023-11-09 14:10:10" -u kube-apiserver " : we get o/p. instarting we see logs from 14:10,logs are generated after 14:10.lets say api is haviung issues, but it was working perfectly well at 7am , nowe can check logs from 7am.

- journalctl --since "2023-11-09 14:10:10" --until "2023-11-09 15:10:10"  -u kube-apiserver : it shows logs b/w 14:10 to 15:10.

***********************************************************************************************************************************************************

DOMAIN 9 : TROUBLESHOOTING

***********************************************************************************************************************************************************
TROUBLESHOOTING APPLICATION FAILURE : 

- an appl can use various k8s resources objects like pods,services,deplys,secrets,configmaps,roles,rolebinding etc. If one of these intermidiatory things doesn't work then our appl can behave unexpectedly or appl can stop working at all.
- so whenever appl doesnt work then we have to look at the dependency that the appl needs inorder to work perfectly and then we have to check each and every dependency to check everything working fine.

SCENARIO 1 : we have 2 pods running and there is NodePort service.so we want to make use of the port which has been exposed by nodeport service to see the appl which are running in the pods from browser.

we have application.yaml : this template contains the objects in scenario 1 : 
---
apiVersion: v1
kind: Service
metadata:
   name: kplabs-service
spec:
   selector:
     run: nginx
   type: NodePort
   ports:
   - port: 8089
     targetPort: 80

---

apiVersion: v1
kind: Namespace
metadata:
  name: teama

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-first
  labels:
      env: prod
  namespace: teama
spec:
  containers:
  - name: first-pod
    image: nginx
    ports:
        - containerPort: 80

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-second
  labels:
      env: pord
  namespace: teama
spec:
  containers:
  - name: second-pod
    image: nginx
    ports:
        - containerPort: 80

- apply it we see 1 service, 1 NS, 2 pods created. run get svc : we see 1 service running on a nodeport 21310.  now get nodws, we get 1, describe it we see ip associated with that node. copy external ip and in browser search (ip:21310) we see it is not woerking.if things are working we should see nginx page.but some issue it isnot working.

SOLUTION TROUBLESHOOTING APPLICATION FAILURE : we have arch of 1 nodeport service and 2 pods. but it is not working,troubleshoot it step by step: 
- run get service it is running, it has port for ext connection also.describe this service, here we can see endpoints is none, but as per arch there should be 2 pods(endpoints) for this service.run get pods, we see no pods running.in yaml we see pods are created but they are in the NS of teama.run get pod -n teama.we see our 2 pods.but why these 2 pods are not registered to service, when we ran describe service, we se selector = "run=nginx" means this service select all pods which has label of run=nginx.  so now describe pod in teama NS, in labels we see labe=env=prod.this lable is not somiliar to servc selector req label.if it doesnt have a label of run=nginx, then we can change the label or we can add this pod manually to the service.

- so add service slector labels to pods (run=nginx) in above yaml :

  name: nginx-pod-first
  labels:
      env: prod
      run: nginx       same for 2nd pod.

- apply appl.yaml again, we see servc,NS unchanged and 2 pods are cofigured.run dewscibe pods we see 2 labels toit env and run. now describe serv we see still no endpoints, why endpoint  ot registered. so when we described service, in NS section we se ns=default,means this serv is created in a NS called as default.however if we look into pods, they are created in teama NS. so both pods and servc NS's are diff. so inorder to solve it we can either move both pods to def NS bu changing ns parameter in pods to def,the pods will be created in def NS. or we can create this service in a same NS where the pods are running thi is better.

- so in above yaml file add NS parameter in service yaml file .

metadata:
   name: kplabs-service
   namespace: teama   save it and apply, and get servc in NS teama, we see nodeportr servc running, and it is listening on a diff port 32170.- descibe this servc in teama NS, we see 2 endpoints associated with it. 
- in browser check (ip:32170) we get nginx page.

***********************************************************************************************************************************************************
TROUBLESHOOTING CONTROLE PLANE FAILURE :

- at this step of cluster troubleshooting, we assume you have already ruled out your appl as the root cause of the problem  you are expecting.
- sometimes things are not working as expected,earlier we looked at appl level troubleshooting to see any misconfig's or not.so once this part is verified an no misconfigs are there, and things used to work perfectly earlier but it is not working now,then the next step we have to do is to look at cluster-level components.

- verify if all the cluster level components are healthy : run "k get componentstatus " : we see status of various clusterlevel components lik scheduler,controller,etcd ..

- also we can do verify cluster info and dump that cluster-info for analysis : Also verify if the master and DNS servers are running with the help of kubectl cluster-info, when we run it we see cluster and dns are running.
- to get more details run 'cluster-info dump' : this will dump lot of relevant logs for debugging and diagnosis purpose.
- k cluster-info dump : it will give huge amnt of info, like all logs associated with all pods which are currently running,through this info we can diagnose

Check logs of individual components : for systemd base components we can make use of journalctl to look into the logs of various coponents.

***********************************************************************************************************************************************************
VERSION SKEW SUPPORT POLICY (imp point during upgrades, ) : 

- this policy is useful while upgrading cluster components and Wnode components and also upgrading Kubectl.

- k8s versions are expressed as x.y.z , where x is the major version, y is the minor version, z is the patch version.
Ex release : v1.15.0  1=major, 15=minor, 0=patch version.

Kube-apiserver component : in HA cluster, the newest and oldest kube-apiserver instances must be within one minor version.
   ex : newest kube-apiserver verion is v1.15 
       then the other supported kube-apiserver instance is either v1.15 or v1.14, it cant be v1.13.
    lets say we run 1 kubeapi at a version of 1.15, 2nd at 1.13 it is not supported.we will get ot of issues here.means there can only a diff of 1 minor version.

Kubelet : it must not be newer than kube-api server , may be upto two versions older.
ex : kube-apiserver verion is v1.15 , so the supported kubelet version can be 1.15, 1.14, 1.13

Use-Case : lets say we have 2 k-api server running in HA env. 1st k-apiversion=1.15, 2nd api v=1.14 .in this case supported kubelet version is 1.14 or 1.13. in this case 1.15 version for kubelet is not supported because it would be newer than 2nd k-api sewrever.

KUbe-Controller manager,Scheduler,Cloud-CM : the versions of these 3 components must not be newer than the kube-api server. these are exp[ected to match the api minor version. thse can be upto one minor version older(specifically during upgrades. lets say we upgrading api 1.15 to 1.16 , so there willl be a little time where there will be aversion mismatch.) so it ois allowed we can have upto 1 minor version older.

Kubectl : Kubectl is supprted within one minor version (older or newer) of kube-apiserver.
- k-api version : 1.14 , then the supported kubectl versions : 1.13,1.14,1.15.

- in cli run : kubectl version : we see ' Client Version: version.Info{Major:"1", Minor:"26+",' means client version is 1.26,  same for server version is 1.26.  so we discuswed above kubectl can supprt one minor version older or newer. here server version is 1.26. so supported version of kubectl is 1.26,25,24
-now run get pods or ingress we get o/p it is working.   
- i also have one more kubectl where its version is 1.28, and servers version is 1.26. means kubectl is 2 minor versions up to api(server). now run ingress here we get, and describe it it says error : server could not find requested resources. we might assume issue at server level, but the issue is at kubectl level.

***********************************************************************************************************************************************************
DRAINING WORKER NODES : 

- earlier we discussed about OS upgrading , we discussed about upgrading of W node: depending on the way pods are deployed and timeframe , resultant action would be different.
Scenario 1 : Immediate restart : if node start immediatly, the kubelet process will start the pods and node is back online.
   here we clickly restart a Wnode and both static pod, and pod which is part of depl will cmes up .

Scenario 2 : Brief Downtime : our controlle rmanger will go and terminate both the pods if there is a default downtime of more than 5 min.
 means here if reboot takes longer than default time(5min) then the node controller will terminate the pods that are bound to the unavalable node.
 if there is any RS , then a new copy of the pod will be started on a diff node.

- now we want have a better control over this.becasue we do OS upgrading on a monthly basis,so w dont want to rely on above scenarios.we want a better approach to work on these things.

- run get pods -o wide, we see both rs's associated with nginx depl are running in node-2.lets say it is time for patching for node2.if we shutdownthis server,then the both pods which are running in node2 would not be available means appl will go down. what we want is if we want to shutdown this server for maintanace or patching activity,we want to move both of this pods to another node which is node1.so we want both pods to migrate to node1 which is running perfectlu and is also patched.

- to have more control over the upgrade process, we can also drain a node.to drain noide run " k drain node_name".this will gracefully terminate all the pods on the node while marking the node as unschedule.once we drained this node , no new pods will be assigned into this node even if the kubelet is working perfetly well. for the pods which are based on RS , those pods will be replaced with the new pod and this new pod will be scheduled in a diff node which is running fine. Hoiwever the static pods will be terminated(the pods that are not part of deplys or Rs's).

DEMO : run get nodes we have 2 nodes.2 pods are running in node 2, we want to drain node2.so both pods which are part of depl would be started in Node1. 
-run k drain node-2. now run get nodes, we see Scheduling disabled for node2, means no new pods will come to this node. now laucnh a pod, and run get pods we see the latest pod is launched in node1.

- earlier when we ran k drain node we also got error : because node2 has daemonset-managed pods. so it states for the nodes which has Demonsetpods we can make use of (--ignore-daemonsets)additional args along with k drain node. because earlier even after drained it , we see our pods are still running in node2
- run " k drain node2 --ignore-daemonsets --delete-local-data " : we see it is evicting pods which are associated with daemonsets. now run get pods -o wide, we see all pods in node1.so we said that node2 is going for patching, and mighrate all pods in that node2 to another working node.

- run get nodes, noide2 is still scheduling disabled.now imagin all patching is completed now remove it from drain . run " k uncordon node2" : node2 uncordened,get nodes, we see both are ready. now imagin node1 needs to be poatched and it has pods of damonsets 5 pods. now run above drain command with ingnore and delete-local comand : we see all pods are evicting.now run get pods -wide , we see all pods are running in node2.

***********************************************************************************************************************************************************
TAINT BASED EVICTIONS (ADVANCED SCHEDULING ) : 
- we discussed 2 scenarios for fasiclitating OS upgrades.
  Scenario 1 : our server restarts or kubelet restarts, immediatly both of our static pods or replica sets pods will come up.
  Scenario 2 : Brief Downtime : if reboot takes longer (default time is 5 mins) then the node controller will terminate the pods that are bound to the unavailable node. What exactly this def time(5min) , how we can configure this.

- The Wnodes are automatically tainted in k8s based on specific node conditions( not ready, unreachable,out-of-disk, disk-pressure etc) in these cases we will have specific taint which would be associatd with this node. if node is out-of disk then k8s will add OOD taint to that node.

Demo : run get nodes, 2 Wnodes and 1 master. run get pods -o wide, we get 1 pod running in node-2. now login to node-2. inside node 2 run " systemctl status kubelet " it is running.Kubelet is running that's why the statusof node is ready.now stop kubelet in node-2( systemctl stop kubelet ), check kubelet status it is dead, now run get nodes, node-2 is still ready status.thre is something csalled grace period which is 40sec, pos which this status will change.
- run get pods -o wide : it is showing still running in node-2.although node-2 status is not ready , the status associated wit the pod is not yet updated.
- ru  describe node-2, in taints we see taints: node.k8s.io/unreachable:NoExecute, node.k8s.io/unreachable:NoSchedule. means no new pods will be scheduled in this node,and the pods which are present in this node they will be evicted.
Q : node-2 is in no-execute state, why the pods that is in node-2 not evicted yet. run 'halt'  : we shut down the entire server,even after shutdown the server run get pods -o wide, pod is still running in node-2.this acts as incoreect data, this can lead to downtime alot of time. so the data we see might be accurate always.Q : why pod is showing still running : ans is the toleration associated with this pod.
 run desribe pod, in tolerations : node.k8s.io/unreachable:NoExecute for 300secs, node.k8s.io/unreachable:NoSchedule for 300secs. for this toleration there is a period 300 secs.means if a node which has the spec taint is attached to it, then wait for 300secs post which the pods running ins this node will be evacuated. in scenario 2 we say if downtime of def time 5mins, then the pods will be wvicted to new node. so pods will wait for 300 secs,post which they will be removed from this spec node which has that taints, and will be scheduked to diff node.since there is a grace period of 300secs , when we run get pods -o wide, we see pod still running in that pod under 300secs, run get pods after 300sec wee see thatpod is scheduled in another working node.

- so even if server is shutdown or kubelet is stopped, pods will show the same status till 300secs, pos which they change status.
- Now we dont want to wait for 5mins, we just want to wait for 5secs.this we can achiev using pod Spec.
pod.yaml : 
apiVersion: v1
kind: Pod
metadata:
  name: nginxwebserver
spec:
  containers:
  -  image: nginx
     name: democontainer
  tolerations:                                    under tolaretions we provided 2 tolerations.
    - key: "node.kubernetes.io/not-ready"    1st toleration
      operator: "Exists"
      effect: "NoExecute"
      tolerationSeconds: 2
    - key: node.kubernetes.io/unreachable    2nd toleration
      operator: Exists
      effect: NoExecute
      tolerationSeconds: 2          toleration is 2secs.means if node has been assigned with 'node.kubernetes.io/unreachable' taint which has effect of no-
                                    execute then only tolerated for 2 secs instead of 300secs.so as soon as node has this spec taint added,within 2secs this
                                    pod will be evicted from this spec node.
- apply pod.yaml.pod will be created in node-2 out of 2 nodes.describe pod ,in tolerrations we see 2 tolerations noexec,Noschedule for 2secs. no connect to node -2,run docker ps we see 2 containers running of nginx. now stop kubelet in 2nd node, run status kubelet : it is dead.now run get nodes, node-2 is not ready, run describe node-2, in tainst we see 2 taints noschedule,Noexecute.run get pods we see pod in node-2 is terminating, and is starting in node-1.
- So we understand brief downtime is toleration secs.

- Also if we justhave a pod and specify toleration secs as 2, this pod will be terminated, but it will not be rescheduled to a diff node,However if we have deply or RS, that deply pod within that node will be terminated but new one will be created in a new node.


***********************************************************************************************************************************************************

EXAM PREPARATION SECTION : 

***********************************************************************************************************************************************************
IMPORTANT TIPS FOR EXAMS :

- we should practice questions in version which CKA exam is running. if CKA on 1.24 then we should also practice at 1.24.

Always use short names :
kubectl get network policies : k get netpol
kubectl get componentstatuses : k get cs
kubectl get certificatesigningrequests : k get csr

* RUN KUBECTL API-RESOURCES : we get all shortnames for resources

- Kubectl-autocomplete : when we run cd +  folder name starting then tab. like cd + 29 then press tab automatically 292528 will come up.

- source <(kubectl completion bash)
  echo "source <(kubectl completion bash)"
  alias k=kubectl
complete -o default -F __start_kubectl k

- in exam before 1st question we have to run these 4 commands. Because these 4 commands will enable auto-complete. if we just enter k in cli and run tab we get list like 'describe,get,run etc'.

- Create manifests via cli : it saves a lot of time during exams.
Q. create a pod from nginx image, expose port 80.
Approach 1 : go to k8s docs, copy pod manifest file and edit it in cli 
Approach 2 : k run nginx --image=nginx --port=80 --dry-run=client -o yaml>pod.yaml    then run k apply -f pod.yaml.

if we want to create deployment from image then run " k run deployment -h " we get help info related to deply's.
- run " k create deployment nginx-deploy --image=nginx --dry-run=client -o yaml > deploy.yaml

KNOW DOCUMENTATION WELL : we should be aware which doc contains which examples.
- sometimes entire exam Q's are similiar to examplkes shown in docs.
- focus primarly on "Tasks" related docs as they dorectly show the commands and examples
- https://kubernetes.io/docs/home/  here if we search any k8s object , in results we if we choose tasks result, then we get all information including cmd's.
 like this " https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/"

PRIORATIZE QUESTIONS : 
write details of questions which has high score in notepad.so that we look into them in later stage.


TIME MANAGEMENT IS VERY VERY IMPORTANT.

- Always prefer to create new object via manifest files
- This will help you have a doc and quickly modify things if req.
- always try to save manifests in yaml file. like 1_pod.yaml for 1st Q, then 2_pod for 2nd Q like this.

***********************************************************************************************************************************************************
DELTA CHANGES IN K8S VERSIONS : 1.18 onwards :

- in 1.16 version there were 2 aimliar commands for pods and depl's :

- create pods : k run nginx --image=nginx  --restart=Never
- create deplys : k run nginx-deply --image=nginx  if we run this cmd in 1.18 version it doesnt create deply, it will create pod. but in 1.16 this used to create deply.

- from 1.18 Onwards :
- create pods : k run nginx --image=nginx  
- create deplys : k create deployment nginx-deply --image=nginx
- '--dry-run=client' is there not '--dry-run to'

***********************************************************************************************************************************************************
IMPORTANT POINTERS FOR EXAMS - DOMAIN 1 ( WORKLOADS AND SCHEDULING ) :

- know howto create pods(single and multi cont's pods)
- be aware of k8s arch and imptance of k8s components.

- In exams they might ask to launch multi-cont pod. the pod should have three containers named nginx, colsul,redis.

- Labels : labels are key-value pairs that are attatched to objects such as pods
- list the nodes in k8s cluster along with labels.
  k get nodes --show-labels

- Selector : Selector allows us to filter onjects based on labels.
 Show all pods which has label env:prod
  k get pods -l env=prod       ( -l is selector, not label. for labels --show-labels)

Commands & Args : we should be able to start given pod with any given cmd/arg
Q : launch a pod from busybox with image of 1.22 , pod should sleep for 3600 seconds and exit

Deployments : be familiar with deply's 
Topics : Rolling Updates, Rollbacks, Scaling, Record Instruction. 
Note : Most of the commands for deplys , we willfind in kubectl cheatsheet.

Daemonsets : a Daemonset can ensure that all nodes run a copy of a pod.
 As nodes are added to cluster, pods are added to them.

NodeSelector : we should be able to schedule a pod on a specific Node 
Q : schedule a pod on a node which has a disk=ssd :

Taints and tolerations :
- Taints are used to repel pods from specific nodes. inorder to enter a taint WNode, we need a special pass called Toleration.

Q : list no of nodes which has a taint of NoSchedule : lets say we have 1 master and 1 Wnode, describe node, and write in text as ANS.

***********************************************************************************************************************************************************
IMPORTANT POINTERS FOR EXAMS - DOMAIN 2 ( SERVICES AND NETWORKING ) :

- K8S service is an abstraction which defines a logical set of pods and a policy by which to access them.

SERVICE TYPE : Cluster Ip
- whenever service type is cluster IP ,an internal cluster IP address is assigned to the service.Since an internal cluster IP address is assigned, it can only reachable from within cluster.
- If we are not creating a service with a specific type then the default one would be the clusterIp.

SERVICE TYPE : Node Port 
- whenever service type is NodePort, then k8s will allocate a port(default: 30000-32767) on every worker node.

Port Vs Target Port - Service :

- port refers to service port. on which port service is listening
- target port refers to port of the container.
Q : create a service where port of srvrice is 80, and target port is 8080.

Named Port : We can also associate a Name associated with the port.
- this name must be unique within pod.

nornmal way :
ports :
 - containerport: 80

Namedport way :
ports :
 - containerport: 80
   name: http

Namedport reference in Service file :
instead of port number, we can also specify the name while creating service
kubectl expose pod nginx --port=80 --target-port=http --name "kplabs-svc"       : here tergetport is http not 80

INGRESS :
- we should know the basics of ingress and how ingress resources can be created.
Q :
- create an ingress for service kp-serv
- path should be /hello
- port should be 8000
- Ns : kp-inte

SERVICE ACCOUNTS :
- Service Accounts are Namespaced
- Def SA gets automatically created when we create a NS
- PODS are automatically mounted with def SA
- Know on how we can create custom SA for a given NS.

***********************************************************************************************************************************************************
IMPORTANT POINTERS FOR EXAMS - DOMAIN 3 ( CLUSTER ARCH, INSTALLATION & CONFIGURATION ) :

KUBEADM : 
- we should know how to setup cluster on kubeadm
- setup master node
- setup Wnode
- Q : upgrade kubeadm version from 1.18 to 1.19 with OS of Ubuntu

ETCD- BACKUP & RESTORE :

- we should know how to backup and restore ETCD Based on Certs.

in exam following things and its locn's will be provided :
- CA cert, ETCD cert, ETCD key.

- store the backup in specific locn and restore the backup from spec Locn.

- It can happen that ETCD systemd file is configured with specific user, etcd. in this case whwnever etcd starts(systemctl start etcd), this etcd process will run with the user of etcd. Now if we perform restore oprn , then all the files/folders that we restore will have a permission from the user through which we have performed the restore oprn(usermight be rooot or etc). Now through root user we restored the files,now all this files have permission of root.now if we starts the etcd, since it starts with etcd user our etcd process will fail. So we have to make sure to change the permissions so that the ETCD can read the contents that has been restored.

- " chown -R etcd.etcd /etc/etc-data "   : R=recursive , path where we perform restore :/etc/etc-data

- in exam we have to verify the systemd file associated with ETCD, to check the user details. like throiugh which user ETCD is running.then we have to chenage the permissions of file that we have restored using chwon command.

NETWORK SECURIT PLOICY : netpol is a specification of how groups of pods are allowed to communicate with each other and other network endpoints
Q : create netpol according to given points :
- Pod1 can only communicate with pod5 in the same NS
- pod2 can only commun with pod10 residin in NS security
- No one should be able to communicate with pod 3
- Be prepared to answer question netoek security policy With a specific req.

AUTHERIZATION : 
- we should familiar with Role/CLusterRole, RoleBinding/ CLusterRoleBinding
- Know how to bind a CLuster ROle with a Specific SA. 
Q : create a cluster role that will have the create access on certain resoiurces like deply's pods for a specfific user or SA.

NAMESPACE :

- How to create a NS, launch objects in a given NS, search object from given NS.
Q : create a SA on a spec NS.
 - Create a role based on Spec NS.

***********************************************************************************************************************************************************
IMPORTANT POINTERS FOR EXAMS - DOMAIN 4 ( STORAGE ) :

- How to create a PV which is a piec of storage in cluster that has been provisioned by the admin or dynamically provisioned using storage classes.
PVC : how to integrate pv and PVC on pod.
Q : we have a req of createing a PV, and attatching that pv to a spec pod through PVC.

Vol expansion steps : 
- How to expand a spec vol. in exam they might give PV of 10gb.they ask us to expand it to 20gb.so first we have to enable vol expansion within storage class(allowVolumeExpansion: true. this step might be done by def in exam). Resize the PVC. restart the pod

IMP : if Q asks you to expandan existing vol and also to record changes ,then it is required to make use of --record
" k edit pvc mypvc --record"

SIDECAR PATTERN : Sidecar pattern is nothing but running multiple container as part of a pod in a single node.
Q : they give pod which is running.they ask us to add a additional sidecar container to the running pod.they also ask us that both the pods should share the same mount-point of /var/log.

***********************************************************************************************************************************************************
IMPORTANT POINTERS FOR EXAMS - DOMAIN 5 ( TRIUBLESHOOTING ) :

- Be familiar with amster and Wnode level Components.
- in exam they might give broken cluster and they ask us to fix it.
- KNow on how to fetch the logs of a specific pod: k get logs pod_name. and move this logs to a text file.

Be familiar with basic grep fxnality : 
- k get logs pod_name | grep "access-denied" > /opt/pod-logs.txt   : find a log of spec pod which has a message of access-denied,and move this spec logs to a text file.

- Be aware how to drain a W Node.
Q : Drain a Node-1.ensure all tha pods in Wnode-1 are migrated to a Newer Node.
   here if we run " k drain node-name" : this wont work
- we have to run " k drain node-1 --ignore-daemonsets --delete-local-data ".

Be familiar with metric server : 
- we should know how to get top utilization metrics of pod  : k top pods
- we should know how to get top utilization metrics of nodes : k top nodes
- Also metric server will be pre-installed in exam so no need to worry 
Also know how we can integrated with spec label.
  Q : find top utilization pod, for all pods which contains label of env:prod

FIX A BROKEN K8S CLUSTER : 
Q : Worker Node is not Ready State , we get some cluster data here
 Sol : run get nodes, we get master node and Wnode. Master is ready and W node is not ready. whenever Wnode is not ready there can be 2 primary areas :
 1. Networking related
 2. Kubelet Related
- connect to Wnode and run " systemctl status kubelet" it is in exited state.thats why node is not ready.so run " systemctl start kubelet". Now run get nodes we see it is running.
- in Q it also state that make the change as persistent.now if we run systemctl startkubelet, Wnode will start. If Wnode will be restarted , then again our kubelet will be in the stoipped state.so this is not the persistent state, Because it doesnt persist across the reboot.SO we have to ensure kubelet truns up automatically even if the system is rebooted. to do that run " systemctl enable kubelet"























	