***********************************************************************************************************************************************************

SECTION 1 : Getting Started With Dockers : 

***********************************************************************************************************************************************************
* typical installation of a software- workflow : i have a windows OS. i want to install an application.

1) the first step is to download a installer. installer would be (.exe file) 
 executable file(.exe)is a computer file that contains an encoded sequence of instructions that the system can execute directly when clicks the file icon

2) after installing installer will go and double click on installer.
3) in the middle of installation we will get a error saying that "some dependencies are missing or files are misssing)
4) we start to troubleshoot the issue 
5) after installing the specific dependencies we re run the installer
6) we will get another error ,which might not the same as previous error. we will go to 4th step again.
 this workflow is the end user workflow.

- there is anoter workflow related to development.
 1.a developer who works for developing a software;he has developed a specific software; now depending upon the way in which he has created the software 
   it might or might not work on the OS out of the box. ex ; if has created a software in vs code it might work on Windows.but it will not work in linux
    and mac.we can use java to work on all.but we don't want to use. so it is a pain for software. so what he can do? docker container.

 2. he create a software and put it in a docker container.now he can deploy docker container in windows,linux,mac. he's 100% sure that he has tested
    docker container in his local env will work perfectly in Windows, mac , linux. this really becomes much more easier for developer to push his software

    to wide variety of OS's and also all the dependencies which are required by the software we can put docker container itself. now end user doesn't have
    to do anything other than running the docker container.

DOCKER : Docker is an open platform, once we build a docker container we can run it anywhere say it windows,mac linux whether on laptop,datacenter,cloud.
       - It follows build once run anywhere approach. means once the developer created software in a container he can run it anywhere(windows,linux,mac) in
         a perfectly running condition depending upon the  test that he has performed while he has created the container.

DEMO : we will be taking NGINX. nginx is a popular webserver in the market. we will consider nginx as a software .
       nginx support amazon linux,debian,ubuntu,redhat. but it doesn't support windows. if we try to install in windows it doesn't have windows .exe file.
 - now if we put nginx in docker container we can install in windows.

* 2 steps that involve for using docker : this specific docker conatiner image (nginx software) it would be in a specific website.dockerhub is one such web
  site.now we have to pull or download this image in our OS where we want to start the container from it.for that we have a command of"docker pull nginx".
- once the image is downloaded we can go and start the container from it.(before downloading docker image we should install docker daemon in our OS.)
- docker image is downloaded now we will start contaner from it.

  Command : docker run -p 8080:80 -dt nginx 
 -p : port mapping 8080 to the 80 of container
  Unlike some other types of servers, webservers do not negotiate encryption. If it's encrypted (https), it must be sent to 443/8443.
  If it's plain text (http) it must be sent to 80/8080

command : docker ps  : show the list of docker containers running. we see nginx container running.
now if we search 127.0.0.1:8080 : we see nginx html page.

localhost is generally the address 127.0.0.1;  but the :8080 part means to connect to port 8080 instead of the default port 80.
Localhost is the default name of the computer you are working on. The term is a pseudo name for 127.0.0.1, the IP address of the local computer. 
This IP address allows the machine to connect to and communicate with itself.


* it would be better to use docker in linux than docker desktop.
  for docker basics desktop is fine. But in later stage we have to work with worker nodes and managed nudes for high availability.when we testing HA we need multiple servers to test the failover and clustering related capabilities.This is why installing docker in linux is good.

- if we are using linux based installation then we will need a server.so to begin we will be creating a new server in any cloud env, and choose ubuntu OS,
  because video for installing docker in linux would be based on ubuntu OS.once server upon running, we install docker. In digital ocean the server is                   called as droplet.

Demo : create an ubuntu based ec2 in AWS.with key pair docker_key.pem. 
  commands : chmod 400 docker_key.pem in cmd prompt in desktop folder
             ssh -i docker_key.pem ubuntu@public_ip   . Now we are connected to linux instance.

Installing docker in Linux : ( https://docs.docker.com/engine/install/ubuntu/)

in docker website we can see compatible docker desktops for windows and macos. and docker server also there ( compatible with centOS,Debian,Ubuntu). 
 Open docs for ubuntu based docker server installation.
- Run " sudo apt-get update "  update apt packages (Advanced Packaging Tool- Apt) 
 also update all ca-certificates,curl pckages.
- Also Add Docker’s official GPG key: (GNU Privacy Guard-GPG) . SSH is used for authentication while GPG is used for signing tags and commits
- after running few more steps again run sudo apt-get update.

* now run " sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin" to install docker engine
- verify docker is running or not in linux instance by " systemctl status docker " it will show running.
  we can also verify by running " sudo docker run hello-world " . it will say Unable to find image 'hello-world:latest' locally. it will start pulling 

docker image from docker hub ; latest: Pulling from library/hello-world
- we can shift to root permanently byu running " sudo su - " 
- How to check available storage of any disk attatched to server.
 That command is df -H. The -H switch is for human-readable format. The output of df -H will report how much space is used, available, percentage used, and the mount point of every disk attached to your system

***********************************************************************************************************************************************************


Docker Images vs Docker Containers : 

- Docker Image is a file which contains all the necessary dependency configurations which are required to run an application.
  we have a docker image for the neginx webserver. This image of nginx would have the nginx binary,it would have all the necessary configurations related to
  nginx.It also has all the necessary dependencies which are required for the nginx webserver to start. 

- so basically the image is all that needed the application to start. The image by itself will not be running.
- Docker container is basically a running instance of an image. so from image we launch a container. Inside container where the execution happens.

Demo : before we start a container we need to have the image within our workstation or server.inside dockerhub there are many docker images which are alread
  available.depending upon use case we can go and download required image. before we start docker container we need to download docker image associated with
  that container.for nginx container we need to download nginx image. to download docker nginx image go to cli and run " docker pull nginx" or run this in 
  linux ec2, already logged into ec2 from above just like we did to hello world image.

- now run " docker images " it will show list of images inside any server or workstation. these are just an images contains files. they are not running.if
  i tries to open nginx in browser it will not open because it is not running.
- if i do " docker ps " we can see list of containers. currently no contaoiner is running.
- now i need to create a container from specific image nginx. 
 to do that i will run  " docker run -dt -p 80:80 image_id or image_name " . now this will start a container from nginx image.
if we do " docker ps "we can see a list of containers. we will see one container which is created from nginx image. This is called as instance of an image.

[ When the ENTRYPOINT is bash or sh;  docker run -d ubuntu:14.04 will immediately stop, cause bash can't find any pseudo terminal to be allocated. You have to specify -it so that bash or sh can be allocated to a pseudo terminal ; 

 -i starts an interactive session and -t emulates a tty. But -d tells Docker to detach and run in the background. They don't really make sense together. ]
- after seeing the container running. if we run ifconfig. we can see the ip address of nginx webserver container address.
- if we want to check whether container is running with ip address run " curl -I ipaddress:80" if it is running on port 80.

- if we do " docker ps -a " we will see list of containers irrespective of status. ( we can also use " docker ps -all " .which is full form.
- inside ec2 we can install many docker images. and we can run containers from them.

DEMO : i got ip address of container by doing " ifconfig ". tried to connect to that ip address by adding ":80". but didn't work. later i changed ec2 SG settings. still didn't work. finally i tried to connect with ec2 ip address along with port 80 it worked.

***********************************************************************************************************************************************************
Container identification : 

- When you create a Docker container, it is assigned a universally unique identifier (UUID).These can help identify the docker container among others.
 UUID basically helps in distinguishing or identyfying the docker containers.
- when we run " docker run -dt -p 80:80 image_id or image_name " we will get a long string. that is called as UUID.
- UUID is good for system perspective.but for the users it is very diff to remember as well as identyfying which container is which. this is why we also has
  option of container names. Docker also allows us to supply container names.

- By default, if we do not specify the name, docker supplies a randomly-generated name from two words, joined by an underscore. above when we run docker             containers we didn't given names. docker automatically allocated some random names ex : " eager_einstein , infallible_volhard " .
- we can give our own name with the help of " --name=meaningful_name " we can specify this while we are running a docker run command.
- " docker run --name mynginx -dt -p 8000:80 image_name or image_id ".  here we are giving mynginx as container name. after running this we will get long string UUID.

To start or stop docker containers we can give " Short container id ; UUID ; container name "
- docker start short container id ; uuid ; container name . same for docker stop also.

***********************************************************************************************************************************************************
PORT BINDING : 
- above when we run docker containers from images we always give ports.

- By default Docker containers can make connections to the outside world, but the outside world cannot connect to containers.
- If we want containers to accept incoming connections from the world, you will have to bind it to a host port.

ex : we have a nginx container which is running it is running on port 80. By default someone from outside world will not be able to connect to my nginx on port 80.because this nginx server itself has a diff ip address. If someone wants to connect to my nginx we have to bind the port 80 of my nginx to one of
the ports of the host machine.we can bind it to the port 8080 of the docker host.after we bind it , from the outside world if someone tries to open the ip 
 address of docker host followed by the port 8080, then the request would be directed to port 80 of the nginx container.

CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS
    NAMES
7545e3b354f4   mongo          "docker-entrypoint.s…"   6 seconds ago   Up 5 seconds   27017/tcp, 0.0.0.0:8001->80/tcp, :::8001->80/tcp   peaceful_noether

- if i do " docker ps " we have one container which is running. in container details we can see there is a bind under ports option.
under ports option " 0.0.0.0:8001->80/tcp " : here 8001 is port of my host.and 2nd 80 is port of my docker container
- this means that any request  that comes to the port 8001 of the docker host would be sent to port 80 of the docker container. now since it is a nginx web
  server it is listening on port 80/tcp inside the docker container.

- run " docker inspect container_name " . we will see detailed info . at the end we can see the ip address of my docker container(172.17.0.2).This is the 
  private ip address.that means from the outside network i will not be able to connect to private ip.
- it's like nginx webserver is listening on private ip.( docker private ip 172.x listening on port 80). i cannot connect to 172.x because it is private networking. 
- so i have to connect to docker host 8001.

Now i will run " docker run -d -p 8000:80 --name mynginx nginx ". now if i do docker ps , we can see port 8000 of my hostmachine is mapped to port 80 of container.
- if i do " netstat -ntlp " we will see there is a new port 8000 and the associated programe name is "docker-proxy"

- type : 132.92.0.1:8000 we will see nginx server in chrome.here the request would be forwarded inside container.

***********************************************************************************************************************************************************
Attached and Detached Mode :
- When we start a docker container, we need to decide if we want to run in a default foreground mode or the detached mode.
- benefit of foreground mode we will be able to directly see all the output on your terminal.if we really don't want to see all the o/p within the terminal  we can make use of detatched mode ( means : You may want to use this if you want a container to run but do not want to view and follow all its output.)

- we have one docker image nginx. 
 demo : run " docker run --name attatched -p 8000:80 nginx ". earlier when we used to run the docker run command it was giving a big UUID and we got access back to our terminal. But here the container has been started with default foreground , we are not able to get back the access to the terminal.however if we 
 go to 8000 port in browser we will be able to see the nginx screen. now try to connect to nginx using ip address with port 8000 in browser. after that we
 can see nginx logs in terminal. since this is running in foreground all of the data coming to us in the terminal.

 problem with foreground approach : let's i want to have back the access to terminal or you have to open up the new tty(pseudo terminal ) you have to open a new terminal.i i press control c here ,i got back my access to the terminal.when i run docker ps. i can see container got exited.
 when i run " docker ps -a " i can see container "attatch" got exited. " THIS IS WHY "-d" REALLY PROVES TO BE HELPFUL.

* Dettatched : 
demo : run " docker run -d --name detatch -p 8081:80 nginx ". here when we run we will get a long UUID.the benefit of running in detatched "-d" mode is that 
 we have access tothe terminal.and the specific container process will be running in the background.

***********************************************************************************************************************************************************
Removing Docker Containers :
- run " docker ps ". we can see multiple containers running.

CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS                                              NAMES
   
a7de8a1657d6   nginx          "/docker-entrypoint.…"   6 minutes ago   Up 6 minutes   0.0.0.0:8081->80/tcp, :::8081->80/tcp              detatch
    
7545e3b354f4   mongo          "docker-entrypoint.s…"   18 hours ago    Up 18 hours    27017/tcp, 0.0.0.0:8001->80/tcp, :::8001->80/tcp   peaceful_noether

- if we run " docker ps -a " it will give list of containers which are running and not running.
- let's assume there are 20 containers which are running and i want to stop each and every container. how to do it.

- if we run " docker container stop container_name " or " docker stop container_name". this is the trivial way to stop the container.but it takes lot of time if we have many containers. so in order to do a specific operation in mass . let's say there are 20 containers are in running state. we want to stop all of them.in such cases we can do " docker container stop $(docker container ls -aq) ".
- Here i we run " docker container ls -aq " . it will give a list of id's associated with the docker containers which are currently present within my workstation. in above command we are passing the id's to the docker container stop. so docker container stop command will get the list of containers to stop

-so after stopping all containers run " docker ps ". no container will be running. run " docker ps -a " it will show list of all containers.

* if we want to remove any docker container run " docker container rm container_name or container_id ". ex: " docker rm attatch" it will remove attach cont.
- if we have lot of unnecessary containers which are in a stopped state and we want to clean it up. then we can make use of :
  run " docker container rm $(docker container ls -aq) " . it will remove all the stopped containers. suppose if any container is running , it will throw error saying that we can't delete running container. we have to stop and then we can run " ls -aq " command.

- TO DELETE MULTIPLE DOCKER IMAGES : docker rmi image1_id image2_id image3_id  ( we can give image name also)
***********************************************************************************************************************************************************
New Docker CLI Commands :
- earlier videos we were using docker commands like run,stop etc.although these are the working ones. however docker is moving to diff set of commands structure.
- go to " https://docs.docker.com/engine/reference/commandline/docker/".  here " docker " is a base command.then we have various commands like attatch,build
, checkpoint,commit,config etc.now how docker is moving is inside docker base page , under child commnad option if we press on docker container : now within
  docker container also we have commands like " docker container attatch or (attatch,commit,inspect,export,logs) ) also :
- docker container attatch or docker attatch ; docker commit or docker container commit all of these things done the same thing. however docker is now moving towards " docker container " specific structure.in the future or now "docker container" is the best practice.

***********************************************************************************************************************************************************
Docker container exec : The docker container exec command runs a new command in a running container.
- The command started using docker exec only runs while the container’s primary process (PID1) is running, and it is not restarted if the container is               restarted.
 EX :in the diag we have a nginx docker container.here the primary process in the PID1 is Nginx.however within the container there can be various other programmes as well.it can be bash,ping etc.we can definitly run these things as well alongside the nginx.and we will be able to connect to them.in the 2nd point above : however if the primary nginx process goes down then the container would exit.

Demo : create a new container. " docker continer run -d --name docker-exec nginx ". so nginx container started.here nginx container has it's process running
 also it can contain various apects like bash,ping etc and we will be able to connect to them.
 now run " docker container exec -it container_name bash or ping " ex : " docker container exec -it docker-exec bash ".

- "docker container exec -it docker-exec bash ". after clicking enter run " /etc/init.d/nginx status " this is like we are logging inside our docker container.now we are inside container( we can know this by "/#" which is inside container ).
- after running " /etc/init.d/nginx " this will say 'nginx is running'

(* The /etc directory is contained in the root directory. It stores storage system configuration files, executables required to boot the system, and some log files) - " ls -l ". it will show the list in vertical line order.

- run " cd bin " . and run " ls -l " . it will show list of binaries
- the reason why we are able to do that is because this container under the bin directory had bash which was available. there are other binaries which are also available. we are also able to execute all these binaries with the help of " docker container exec " command.

[1. Executable refers to the main exe file for a piece of software. Binary is a machine code representation of the Exe or may be a data file stored in binary format, rather than plain text.
 2.Binary file, composed of something other than human-readable text
   Executable, a type of binary file that contains machine code for the computer to execute
   Binary code, the digital representation of text and data  ]


- run "apt-get update " ( The sudo apt-get upgrade command downloads and installs the updates for each outdated package and dependency on your system. But just running sudo apt-get upgrade will not automatically upgrade the outdated packages – you'll still have a chance to review the changes and confirm that you want to perform the upgrades ).
- Basically this container is based on debian. so we have to run apt-get command.[ if our container is based on amazon linux or centos then " yum update " useful. ] after updating install net tools by running " apt-get install net-tools "

- now run " netstat -ntlp ". we can see :
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/nginx: master pro
tcp6       0      0 :::80                   :::*                    LISTEN      1/nginx: master pro

- we have pid1 which is nginx. and currently it is listening on port 80.
- run " which netstat " . it will show " /bin/netstat " currently it is inside bin dir. if we run docker ps it will show " docker command not found "
* in order to come out of the container we can press " exit ". now run docker ps. we can see list of containers.

[ -it (and by default without) runs a container in interactive mode, which allows you to interact with the Domino server console from the Docker terminal. 
   Alternatively, you can use the -d argument to run a container in detached mode ]

- now without logging to container if we want to see netstat command : run " docker container exec -it docker-exec netstat -ntlp ".  it will so pid's,ports.
  Means we can run the command inside the container while not being logically logged in to the container. however if we connect to bash then also we can do netstat -ntlp command.


- The command started using docker exec only runs while the container’s primary process (PID1) is running, and it is not restarted if the container is               restarted. ( above defn in 2nd point).if we want to verify : in the diag the container primary process is nginx. 
 if we run " docker container exec -it docker-exec netstat -ntlp " . at last it will sho under PID : nginx : master pro running on PID 1. if this specific
 process exits then the container also exits : 
 demo : run docker ps. it will show up for 5 min. Now run " docker exec -it docker exec bash ". we went inside container.we know that "netstat -ntlp " works
 inside container.

[ bash command : docker exec -it <container id> bash. This command is used to access the running container ]

- Now inside container run " /etc/init.d/nginx stop " here we are stopping the nginx process. when we stopped main process we automatically goes back to my local host( linux server ) .
- if we run " docker ps " . we don't find any container.because when we stopped nginx process , automatically container also got exit. (because nginx is the main process we stopped in a container).

- now if we want to start the docker contaner, we can run by normally " docker container start docker-exec ".no if we run docker ps we can see container running. now if we run " docker container exec -it netstat -ntlp " we can see result. 
- if we run " docker container exec -it docker exec ps " . it will through error saying that we can't use other commands which can't be used with docker container. here we can't use "ps " under docker container.

- Download Busybox docker image. run a container from it. now run " docker exec -it container_name(of busybox) bash " to login to busybox. but here we get an error saying (  exec: "bash": executable file not found in $PATH: unknown ). because bash is not found in binaries. so diff containers has a different way in which we can connect.So in the case of busy box we can make use of "sh".
- run " docker exec -it container_name(busybox) sh. now you can logged to container

***********************************************************************************************************************************************************
Importance of IT Flag : 
- " docker exec -it container_name bash ". now what does -it means : 
- Every process that we create in the Linux environment, has three open file descriptors; stdin,stdout, stderr.
- let's say that there are 2 processes that are running ( bash and nginx).both bash and nginx have all three(stdin,stdout,stderr).

- stdin : standard input.any input that we specify to specific process goes via stdin.one of the way that we can put the input is keyboard if we type certain commands.then we also have various pipe operators through which we can put the stdin.

- in fig : the things that i type from terminal goes to stdin. the things that shows up in screen goes to stdout.
- "-it" here '-i" basically keeps the stdin open.
--interactive flag keeps stdin open even if not attached. (shortcut- "-i") 
--tty flag allocates a pseudo-TTY(shortcut- "-t" ).

from above container ex:
- " docker container exec docker-exec bash ". if we press enter here nothing will happen or nothing will come.
- " docker container exec -i docker-exec bash ".( here -i means standard in ). after running this command although it really didn't get tty here.if we type anything like " ls" i will be able to see the o/p.if i type commands like "pwd" i will be able to see the o/p.however we don't really have a tty(pseudo terminal).

- " docker container exec -it docker-exec bash ". here we will be logged into container. it will show a representative tty starts with (root@ip-172-31-38-252:~#).we can come out from container by using exit or control+d.
- " docker container exec -t docker-exec bash " here we only put "-t".here we will get tty. but when we run commands like "ls" or "pwd" we will not really get o/ps.because the commands that you typing i need to goes to stdin(standard-input) of a specific process that you are running.so currently without "i"
 we are not able to send those specific commands.

- "-it" eventhough both i and t stays combine. they are actually two seprate flags. "-i" and "-t".

***********************************************************************************************************************************************************
Default Container Command : 
- in the earlier videos when we run " docker run -dt -p 8080:80 nginx ". it will create a container. here inside container the nginx automatically started.
 however we never defined which process should be started within the container. because a container can have nginx, it can have multiple binaries within  the path. So how did my container automatically start nginx and it didn't start other things like bash or netstat or any other binaries. the answer to this is default container command.
demo : run " docker run -dt -p 8080:80 nginx  --name default_command".now run docker ps : 

CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                   NAMES
a073395fbc14   nginx     "/docker-entrypoint.…"   3 seconds ago   Up 3 seconds   0.0.0.0:8082->80/tcp, :::8082->80/tcp   default_command

- no we logged into container by docker exec with bash. inside container if we search "/etc/init.d/nginx status " it will show running.we nginx automatically started. now there must be some place which basically defines that whenever a container starts this nginx something that should be started automatically.and that place is cllaed Command section above( "/docker-entrypoint.…") referreing to as default container command.

- Whenever we run a container, a default command executes which typically runs as PID 1. so incase nginx is the process which is going to start from default command it typically runs with the pid 1.
- this command that we were discussing which automatically starts a process when a container starts can be defined while we are creating a container image.
- if i run docker images there are multiple container images.let's say nginx is the container image.now container image will have it's own default command.
 let's you start a container image from different image(busybox), then busybox image will have it's own default container command.

- go to dockerhub. and click on nginx image. we can see dockerfile links. this dockerfile is used to create an image.at the end of docker file we can see 
 " CMD [ "nginx" ,"-g","daemon off;" ] ". this is the same thing that we will get under command section when we do docker ps. 
this means whenever we launch a new container from nginx image whatever command that has been specified in the image creation process inside(cmd section at end in dockerfile )this command will automatically get started when we start a contaner image.

- the samething we do for busybox. launch a container from busybox.we can see only "sh" under default command section when we do docker ps. the default command will change depending upon which container image we launch.you launch apache image. what we want whenever a container launch or a container automatically restarts then we want sapache process to automatically starts.in such cases we can specify what we want to start when ever a container starts or restarts through the" CMD [] " directory in docker file.

***********************************************************************************************************************************************************
Overriding default container commands : 
Q: can we override a default container command : 
demo : launch a container from nginx image. do docker ps. under command section we can see " /docker-entrypoint.docker daemon off " now i want to change this command to docker daemon off. to do that we have 2 ways : 
1. change the default container command itself when the image is created(in dockerfile modify the arguments in cmd section at the end).and then we can built a new image from this dockerfile.

2.we can also change default container command when we start a container image.
 command : " docker container run -d nginx sleep 500 ". we started a container from nginx image. run doocker ps. under command section we can see " sleep 500 ". means default container command was overriden with the command that we executed.

 - now login to container by " docker container exec -it container_name bash ". inside continaer got to " /etc/init.d/nginx status ". it will show " nginx is not running failed ". means some other process is running in place of nginx.	
- inside container run " apt-get update ". install a utility called as htop. run " apt-get install htop ".
 run htop now. we can see the " sleep 500 " which we replace the default container command with this has a PID of 1.(PID 1).and we already know once the pid 1 exits, the container will also exits.

- go out of container and type : "sleep 5" : it will sleep for 5 sec. the sleep will exits.and i will be back to my tty
- "docker container run -d nginx sleep 500" : here sleep 500 means specific process will sleep for 500 sec's , after 500 sec the sleep process which was created it will exit and once it will exit the container will also itself will exit.

- ex : " docker container run -d nginx sleep 20 ". now container starts with primary process as " sleep 20". here sleep 20 is PID 1. after sleep 20 runs for 20 min it will go off. when it will go off container will also go off. because whenever pid 1 goes off container also go off.

***********************************************************************************************************************************************************
Restart Policies ( automating start of containers ) : 
- By default, Docker containers will not start when they exit or when docker daemon is restarted.
- ex : let's say i have a docker host.and i have 2 containers(NGINX,Apache) which are running in docker host. Now due to some reason docker daemon got restarted.now after the docker daemon got restarted the containers which were running will not automatically starts. ex : let's say i restart the server.and

 after the restart of your server the docker daemon once it comes up then the containers which were running previously will not automatically start. so in order to solve this issue docker provides us restart policies.which allows us to control various aspects related to containers like when they exit or when docker restarts.

* Docker provides restart policies to control whether your containers start automatically when they exit, or when Docker restarts
- We can specify the restart policy by using the --restart flag with docker run command.

DEMO : if i do docker ps. there are no docker containers which are running.
- " docker container run -d nginx ". one container started running.now due to some reason our docker daemon got restarted . do " systemctl restart docker".
 now docker daemon got restarted.now there can be various other reasons( server itself got restarted,docker daemon got killed due to the memory issue etc). 

now if i do docker ps i see no container is running, means that docker container doesn't automatically restarts.if i do docker ps -a, i can see container is in exit state.so now i have to manually start this container.for testing this is fine. but in production patching activity restart of server happens,quite a lot.so you want that containers of production should automatically starts when the server restarts.it is very imp to specify appropriate restart policy to docker container.

We can specify the restart policy by using the --restart flag with docker run command.
- There are 4 types of docker container restart policies.
 1. No :             Don't automatically restart the container( this is the default) ( if we didn't mention --restart flag it is considered as default one).
 2. On Failure :     Restart the container if it exits due to an error,which manifests as a non-zero exit code.
 3. Unless-Stopped : Restart the container unless it is explicitly stopped. server or docker itself is stopped or restarted. used in prod env's. IMP
 4. Always :         Always restart the container if it Stops.

Demo : Unless-Stopped : run " docker container run -d --restart unless-stopped nginx ". now container started with a restart policy of unless stopped.
- now lest's restart our docker daemon. run " systemctl restart docker". after run " docker ps" i can see my docker container automatically restarted.

[ SYSTEMCTL :  The systemctl command manages both system and service configurations, enabling administrators to manage the OS and control the status of services. Further, systemctl is useful for troubleshooting and basic performance tuning    OR     systemd is a system and service manager for Linux operating systems. systemctl is a command to introspect and control the state of the systemd system and service manager. Not to be confused with sysctl.systemctl is the systemd command for controlling how services start on a Linux system. A service can be enabled, disabled, or masked, and it can be configured to start at boot, on demand, manually, or prevented from starting under any circumstances. Enabling a service means it will start at boot ]

REMOVING DOCKER CONTAINER IMAGES : 
- how to remove unnecessary containers that we launched.
- first we can stop the docker container by " docker stop container_name ". remove container image by " docker rm container_name ".
- if we try to remove running container we will get an error message from daemon saying that " we can't remove running container ". however we can use specify the "-f" for force in order to do that.	but best practice is to stop the container before removing.

***********************************************************************************************************************************************************
DISK USAGE METRIC FOR DOCKER COMPONENTS :
- like how we can monitor for disk usage on the host which is running docker containers. in prod we have to know how we can analyze disk usage associated with the containers. sometimes one container or 2 containers they consume a lot of amount of disk and due to that other containers or other appl's on the host not able to run very effectively.

demo : login to linux ec2(host). run " df -h ".this command is specifically for linux.
[ Filesystem      Size  Used Avail Use% Mounted on
  /dev/root       7.6G  4.2G  3.5G  55%   /   ]
 it says that there is one hdd which associated name is : dev/root. size of disk is 7.5GB. the used is 4.2GB.available is 3.5 GB. and utilization in terms of % is 50%. although this gives a good amount of info but it doesn't really tell the disk usage specific to docker components. if we want to have a component level disk metrics then we can run " docker system df ". o/p : 

TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE
Images          3         0         786.5MB   786.5MB (100%)
Containers      0         0         0B        0B
Local Volumes   8         0         633.6MB   633.6MB (100%)
Build Cache     0         0         0B        0B
- The docker system df will give info related to above 4 types(images,containers etc ). containers currently running is zero. now create a container from busybox image. login to container using " docker exec -it container_name sh ". now we logged into contaner. create a file of size 500mb.
 run " dd if=/dev/zero of=bigfile.txt bs=1M count=500 " means ( dd the i/p i will say zero, o/p i will say bigfile.txt, bs is of 1mb,and count is 500).
O/P: 
/ # dd if=/dev/zero of=bigfile.txt bs=1M count=500
500+0 records in
500+0 records out
524288000 bytes (500.0MB) copied, 5.287942 seconds, 94.6MB/s
/ # ls
bigfile.txt  dev          home         lib64

- this has created a file called as "bigfile.txt" of size 500 mb.if we want to verify run " du -sh bigfile.txt ".

/ # du -sh bigfile.txt
500.0M  bigfile.txt (it says bigfile size is 500mb.let's come out of container and run same " docker system df " 

TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE
Images          4         1         791.3MB   786.5MB (99%)
Containers      1         1         524.3MB   0B (0%)  ( here we can see in the container section we have the size associated with it ). if we want to go in detail. it might happen that there are 10 containers that are running.and i want to see which specific container is consuming the highest amount of disk. 

we can do that by running " docker system df -v " . this component will give a per component level size.we can see there is an image. the size of that image is 100mb.respectively for evry images in host. also within the container space usage it will show a container name busybox with the size of 524mb .

- so [ df -v ] gives you a per component level disk usage , this command really helps you to have a good insights into which container or image or volume, buld cache is taking the max disk utilization.

***********************************************************************************************************************************************************
Automatically Delete Container On Exit : 
- create a container by running " docker container run -dt --name testcontainer busybox ping -c10 google.com " . here we are giving name and we launched from busybox image. we are speciying a command line say ping 10 times to google.com  .if we do docker ps we can see running container.
- now if do docker logs on test container we see that it is pinging. now after pinging 10 times this container will exit.if we do docker ps -a we can see testcontainer is in exited state.
- many times containers are created with a specific job. once the job completes then there is no use of that specific container.i want the container to automatically delet once the job is done i don't want container to be in exited state because it will unnecessarily take lot of storage. in such cases we can specify " --rm option ".

- run " docker container run --help | grep rm " . it gives explation related to docker container rm command.
( --rm : automatically remove container when it exits ).

- Now run " docker container -dt --rm --name testcontainer01 busybox ping c10 google.com ". now run docker ps we can see container is running. also run " docker logs testcontainer01 " .we can see it will ping for 10seconds.after it was able to ping for a destination target for 10 times it will exit. when we do docker ps we can't see that container. when we run docker ps -a we can see no testcontainer01. because it automatically delete when it's job has been done. because we mentioned --rm command.
[ if we want to comeback from sudo su - , press ctrl+d ]



***********************************************************************************************************************************************************


SECTION 2 : IMAGE CREATION, MANAGEMENT, and REGISTRY


***********************************************************************************************************************************************************


Working with Docker Images : 
- Every Docker container is based on an image.
- Till now we have been using images that were created by others and available in Docker Hub.
- Docker can build images automatically by reading the instructions from a Dockerfile
- A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.

* in order to built a docker image we need to have a dockerfile.The dockerfile basically contains all the instructions necessary to build a docker image.
 ex : i have a image for nginx.so i will need the nginx package to install and all the relevant dependencies.all of those we will be writing in a dockerfile
- now after i written all the instructions , we perform docker build , and after docker build command we will have the docker image.so this docker image is something that we can put to the public repositories like dockerHub, we can also put in private repositeries like ecr.

- within a cli i am in a directory called as docker image.within dir i have a file called as dockerfile.if i open dockerfile " nano dockerfile ".i can see instructions which are written inside dockerfile.and when i do docker build docker will basically read all the instructions and it will perform all the operations accordingly.
- to do docker build run " docker build . " here we are specifying current directory. we can see all the instrcutions that are written in dockerfile are performing.(like from ubuntu , run apt-get update, run apt-get install nginx, CMD [ "nginx", "-g", "daemon-off"].when i do ls i dont see image within the directory.
- we will see image in local.run docker images we can see our cretaed docker image in that list. we can create a docker container from our custom image.
- run " docker container run -d -p 80:80 custom_image ". after running container run "netstat -ntlp ". we can see on host it is listening on port 80.
- easiest way if we can do "curl 127.0.0.1"we can see welcome to nginx


***********************************************************************************************************************************************************
Overview of Dockerﬁle : 
- we have a dockerfile which has a list of commands.then we make use of docker build.the o/p of docker build is docker image.

The format of Dockerfile is similar to the below syntax:
# Comment
INSTRUCTION arguments

- A Dockerfile must start with a FROM` instruction.( FROM basically means that from which base image we want to start creating a image from). let's say i want nginx container.the first thing we want to define is from. our container may be from the base image of ubuntu.
- the instruction is basically the list of various commands that the dockerfile supports.it does support a lot of commands : 
   FROM
   RUN
   CMD
   LABEL
   EXPOSE..etc
- depending upon the use case and req we might use one of them or we might not use one of them. but FROM is that one thing we will be used in all dockerfile

USE CASE : Create custom nginx image. 
- we want to create a custom nginx image from which all containers with org will be launched.
- the base container image should have custom index.html file which states : " Welcome to Base Nginx container from HEMANTH ".

- run a docker container named " mynginx " from ubuntu image. " docker container run -dt  -p 80:80 --name mynginx ubuntu ".
- connect to the container by " docker container exec -it mynginx bash " . now we logged into container.if we want to have nginx installed within ubuntu then the firstthing that i have to do is run " apt-get update ". and run " apt-get install nginx ".we can also give automatic permisiion by adding '-y' here " apt-get install -y nginx ". this will we automatic installation.

- once nginx is installed verify " /etc/init.d/nginx status " it will show 'nginx is not running'. but our requerment is( written above) : the index.html file should states : " Welcome to Base Nginx container from HEMANTH ". it's like once nginx has started or once someone puts the ip address of the nginx container within the browser he should be presented " Welcome to Base Nginx container from HEMANTH " this message.

- let's look where nginx is storing its default file. it should mostly be in " cd /var/www/html/ ". run 'ls'. we will see 'index.nginx-debian.html' open the file by ' cat index.nginx-debian.html '. we can see html page.

- currently we are in " /var/www/html ".
- run : echo " Welcome to Base Nginx container from HEMANTH " > index.nginx-debian.html    ' here we added our thing in html page.
- if we run cat index.nginx-debian.html : we can see '  Welcome to Base Nginx container from HEMANTH '.

- once we have done let's start the nginx : run " nginx -g 'daemon off;' ( this will run in foreground - we dont have access to terminal). now copy the ip address of ec2(http://13.233.112.151/) and paste in browser. we will see (Welcome to Base Nginx container from HEMANTH ) in browser. to stop the prpocess and come out of container press ctrl 1st and press ctrl d next.
[ the command that we used to get 'welcme hemanth' : apt-get update  ;  apt-get install -y nginx   ;  /var/www/html/index.nginx-debian.html ]

- now go to root ( cd /root ) then create demo ( mkdir demo ).go inside ddir demo ( cd demo ).insie that create a file called as dockerfile(nano dockerfile)
inside docker file  write : 
 ' FROM ubuntu                             we can specify any other image like amazon linux etc 
   RUN apt-get update                      here we have to run apt-get command
   RUN apt-get install -y nginx            next install nginx with automatic yes . to this specific point our nginx is installed.now we want to have a     
                                           custom file which is index.nginx-debian.html 
till now save the dockerfile by pressing(ctrl^X) to exit and press enter at Y to save.
- Now create one more file by ' nano index.nginx-debian.html ' in demo folder. open by nano. and copy message in that. press (ctrl^s to save and press ^X to exit.)   

- now we want this index.nginx-debian.html file to be copied in docker container on a specific path. that specific operation can be achived with an instruction of COPY. now open dockerfile by nano and write ' COPY index.nginx-debian.html ' and we also have to specify to which path this specific file should be copied. in this case the path should be ' /var/www/html': so finally ' COPY index.nginx-debian.html /var/www/html ' written in dockerfile.

- now one last command that is imp is the default container command.this is the container command which automatically gets run when a container starts or restarts. in our case we run ' nginx -g 'daemon off;' this command to start nginx. so we have to use this command as default container command.we can specify that in dockerfile with "CMD  nginx -g 'daemon off;' ". whenever someone starts a docker run on the image which is created from our instruction automatiocally this  nginx -g 'daemon off;' command will execute. it can also be overriden during docker run oprn.

- finally dockerfile : 
FROM ubuntu
RUN apt-get update
RUN apt-get install -y nginx
COPY index.nginx-debian.html /var/www/html
CMD nginx -g 'daemon off;'
- in demo folder we have 2 files dockerfile,index.nginx-debian.html
- now in demo folder we run " docker build . "  means docker build current path(.). when it is running it will show 5 steps.
1. it will pull an ubuntu image.once we have ubuntu image
2. it will run apt-get update would be executed then
3. it will run apt-get install -y nginx (here all packages and dependencies of nginx will be installed)
4. we have a copy instruction, it will copy file of .html and paste it in /var/www/html
5. we have a default command ( at the end we can see successfully built kshdsdk(image_id) )

- now run docker images. we can see a new image which got build.
- start a container from our custom created image. with port 8000:80 with custom-image-id.
- we can see a container running when we do docker ps.
- now run " curl 127.0.0.1:8000 : we will get ' Welcome to Base Nginx container from HEMANTH ' this as o/p.

***********************************************************************************************************************************************************

COPY vs ADD Instruction : 
- in cli i can see three files (add.txt, copy.txt, dockerfile)
- nano dockerfile : 
 FROM busybox         in 1st instruction we have a FROM instruction which will create a image from busybox
 COPY copy.txt /tmp   in 2nd instruction we have a COPY INSTRCUTION which will copy the file called as copy.txt to the temp directory
 ADD add.txt /tmp     in 3nd instruction we have a ADD INSTRCUTION which will add the file called as add.txt to the temp directory
 CMD ["sh"]           we have cmd instruction which will basically runs the shell.

- build this image by " docker build -t demobusyboz'. so our image is built. do docker images we can see our demobusybox image.
- launch a container from demobusybox.
- in dockerfile 
  2nd instruction copy is copy copy.txt and is putting in temp directory.
  3rd instruction add is adding add.txt to temp directory.
- so in the container that we have created both copy.txt and add.txt should be present in temp directory.
 login to container by ' docker exec -it container_name sh '. inside container run " cd /tmp " . in temp dir we can find add.txt,copy.txt.

- COPY and ADD are both Dockerfile instructions that serve similar purposes.
- They let you copy files from a specific location into a Docker image
- COPY takes in an src(source) and destination. It only lets you copy in a local file or directory from your host( whatever file that we have in the source the copy will copy that specific file in a specific source. source can be a local file or a directory host. it will copy that file into the container image)

- ADD also does the similiar thing which copy instruction does.but it supports 2 other kind of sources : 
  1. add will you to make use of url instead of local file or directory.( copy can only copy a file which is present locally or directory. add can also do
     that but in addition to that it can also fetch file from url,and it can move it to a specific container directory)
  2. you can extract a tar file from the source directly into the destination.( if we have compressed file within our source, then if we make use of add 
     instruction, add instruction will decompress the file and it will store the components of decompress file to the destination.
- Using ADD to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead

DEMO : supported sorces of add - create a file compress.txt in the same dir where we created copy and add.txt by running " touch compress.txt ".
 compress the compress.txt file by running " tar -czvf compress.tar.gz compress.txt ". if we do ls we can see compressed file( compress.tar.gz).
- now add one more ADD instruction compress.tar.gz and put it in /tmp : " ADD compress.tar.gz /tmp " in dockerfile. save it and exit.now go and build the image : " docker build -t demobusybox02 . " now create a container from it:

- "docker container run -dt --name demobusybox02 demobusybox02 " : launching a container from demobusybox02 image.
- login to busybox container using sh. now go to " cd /tmp" directory. here we will see 'add.txt,compress.txt copy.txt' now here add instruction does was it took ' compress.tar.gz ' file and add uncompressed it and performed the decompression on that file and whatever decompressed files which were present it moved it inside the container.that is what add is specifically used for.

FINALLY : if we want to move a file like add.txt,copy.txt to the image we can avoid usiung the add command.we should make use of copy command.
          incase if we have compressed file and we want to move the contents of compressed file inside the container we can make use of add. now add also 
          has the capability to fetch files from the remote url and that something that we should avoid and instead we can make use of curl or wget.

***********************************************************************************************************************************************************
EXPOSE INSTRUCTION : 
- The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime.( let's say that i have a docker image and the service or the appl that is within the docker image is listening on port 5000.
- so how will a individual who is running a docker image know that a service is listening on port 5000.because the service can listen on 8000,9000.
- The EXPOSE instruction does not actually publish the port.but to inform on which port specific apll or service is running.
- if we want to actually publish a port at docker level we have to use the '-p' option.

- It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to  
  be published (generally there are 100's of images within the dockerhub.now a specific image as we are discussing it can listen the service or appl can container can listen on any fmrl port.how will a person that is running a container know on which port that appl is listening to.Anyone who builds their image, They inform with the  EXPOSE instruction at a specific service or appl.

USE CASE : we have a service container on the right side.when we perform a curl on the domain followed by the port then this service container basically returns the datablog.Now we really don't know on which port this service container is listening to.that is where EXPOSE instruction comes into picture.

DOCKERFILE : ' FROM ubuntu ;  apt-get install service ;  EXPOSE 9324 ;  the image which gets built from this dockerfile ,from the image a person who is going to to run can identify that this specific service is going to listen on 9324 and he can publish the port accordingly.

demo : launch a container from nginx image.withing the port section when we do ps. we can see '80/tcp'. so that basically means that this 'nginx -g daemon' 
       which is running inside container it is listening on a port of 80 at tcp level.this info'80/tcp' doesn't come from the container by itself.this nginx can even run on 9000.that doesn't really means that port will be updated accordingly.infact this info is coming from EXPOSE instruction which the  image creator had added in the dockerfile.

- when we do docker inspect on nginx image ' docker inspect nginx' there we can see within the "ExposedPorts : 80/tcp "
- run one more container " docker container run -d --name nginx2 -p 80:80 nginx ". when we do docker ps under ports section we can see " 0.0.0.0:80->80/tcp"
 means an traffic from (0.0.0.0) which is coming on the host port of 80  will automatically forwarded to the container port of 80. here in this case when we made use of ' -p 80:' here we are saying that any request which comes to the host port of this server on 80 will be forwarded to port 80 of the nginx container. from expose instruction it was easy to find second part.

- however there can be a lot of apploications which might listen on port 5000.then "-p 80:x" x part will change accordingly.
- i have custom dockerfile. in that i have " EXPOSE 9080 " instruction in dockerfile which , it using nginx image. in that dockerfile it is using nginx image so code : " FROM nginx:1.17-alpine ". after launching a docker image and acontainer from this cpontainer , run ps we can see two ports in ports 

section. " 80/tcp, 9080/tcp ". here we can see 2 ports because, now run docker inspect container_name : there we can see " EXPOSEDPORTS : { 80/TCP: {} , 9080/TCP : {} } . here 80/tcp comes from the image that we are using in docker file to creaste custom image.
- now what happens if we don't expose any port(9080). if we don't expose a port it will be a little diff for the person who is going to run your appl container to indentify on which port your appl is running.

- i have one more dockerfile. here we are centos as base image. and we are not giving expose port instruction. now create a image from it and run a docker container. do ps : we can see nothing in ports section.

Q. Can I create a docker image without a base image?
   The short answer is no. we need a base OS image or a base function image

***********************************************************************************************************************************************************
HEALTHCHECK Instruction : HEALTHCHECK instruction Docker allows us to tell the docker platform on how to test that our application is healthy

- When Docker starts a container, it monitors the process that the container runs. ( process would run as PID 1 ).If the process ends, the container exits
  this is just a basic check and does not necessarily tell the detail about the application. ( sometimes it happens that the process is running,however there are lot of errors and it is not really responding properly. so there is a need to have a proper health check to verify whether the application is 
working as expected or not. inorder to do that we make use of HEALTHCHECK instruction )

DEMO : run a container from busybox image " docker run -dt busybox sh". we can see container in docker ps. get the ip address associated with busybox container by " docker inspect container_name ".it is " x.x.x.x".
- now i also have a dockerfile ,open it using nano. inside that we can see 
  Instructions : " FROM busybox  
                   HEALTHCHECK --interval=5s CMD ping -c 1 172.17.0.2  "  ( here we are running a healthcheck, we specified the interval 5sec, and then we are running a command which is ping on 172.17.0.2 (busybox ip address)" CMD ping -c 1 172.17.0.2 " this specific command would run a interval of 5 sec. this can monitor whether the container is up or not. build this using " docker build -t any_name ". after getting a docker image named ping run a container from it.
- do docker ps : in container sections under status we can see : " up for 4 seconds (health: starting) ".
- whenw e do docker ps again under status we can see " up 34 seconds ( healthy) " . the reason it is healthy is it is successfully able to fetch 172.17.0.2)

- we can also find a great info in " docker inspect container_name ": here we can find logs. in that logs we can see it is successfully able to get the ping reply.and we can see " ExitCode = 0 " means everthing is working as expected.

- after that run " ping -c1 google.com" we will be able to ping google successfully. 
 then run " echo $? " : it will give 0. Means Exitcode is 0. means execution is done successfully  { do the same thing for any random ip address . it won't reply anything. press ctrl^c. now run " echo $? ". it will give 1. means EXITCODE is 1.. means something bad happened }.

- HEALTHCHECK makes use of EXITCODE. ( in docker ps : if exitcode is 0 we will see healthy as status, if exitcode is 1, status is unhealthy
- now stop the busybox container which has ip address of 172.17.0.2. the the ping will fail for ping image container.
- now run docker ps we can see docker ping container status is unhealthy ( exicode=1). do docker inspect. in the logs we can see ping is failed because it didn't get any reply back.

We can specify certain options before the CMD operation, these includes:

HEALTHCHECK --interval=5s CMD ping -c 1 172.17.0.2
HEALTHCHECK PARAMETERS :
● --interval=DURATION (default: 30s)  ( if we don't give any value in sec it will take 30sec as default).
● --timeout=DURATION (default: 30s)
● --start-period=DURATION (default: 0s)
● --retries=N (default: 3)

***********************************************************************************************************************************************************
HEALTHCHECK instruction options : whenever we are using HEALTHCHECK instruction there are multiple options to get the desired result.(written above)
- 4 options( interval,timeout,start-period,retries). if we don't specify an of these options while adding a health check instruction then the associated default values will be automatically used.( ex : if we don't specify interval, then the default value of interval is 30 sec)

- The command's exit status indicates the health status of container.the possible values are :
   if we have Exit_status=0: success ; the container is healthy and ready for use
              Exit_status=1: Failure ; the container is not working properly
              Exit_status=2: reserved ; do not use this exit code
EX: check every 5min or so that a web server is able to serve the site's main page within 3 secs : 
( we have a health check instruction, where we have a curl on the local host ; incase if we have a application or webserver running the below command will give the result, if not the command will fail.)
Command : " HEALTHCHECK --interval=5M --timeout=3s \        ( it will check every 5min whether the curl on localhost is working or not and the timeout is 3 
            CMD curl -f http://localhost/ || exit 1  "        sec so the results should be appearing within this timeout)

- HEALTHCHECK we not only specify this as part of dockerfile. we can also make it available as part of the docker run command.
- if we are doing docker run we can specify healthcheck can also be done.
  --health-cmd            Command to run to check health
  --health-interval       Time between running the check
  --health-retries        Consecutive failures needed to report unhealthy
  --health-timeout        Maximum time to allow one check to run
  --health-start-period   Start period for the container to initialize before starting health-retries countdown
  --no-healthcheck        Disable any container-specified HEALTHCHECK

- create a new docker container : " docker run -dt --name temp --health-cmd "curl -f http://localhost " busybox sh " ( --health-cmd : Command used for healthcheck , command using for health check is http-localhost where the curl will happen, if there any appl or webser that is listening on localhost, then this command would be successful)

- run docker ps : in status it will show ( up 8 secs: helath : starting) after some sec run ps again it will show : up about a minute(health : starting) ) why is it taking so much amount of time for the health to be started.it should either be healthy or not.answer to that is default option.since we havn't specified any, default option the interval of health check would be 30sec,the timeout is 30sec,and the retry is 3sec( it will retry for 3 times in the interval of 30sec,before sharing a specific result) 

- now run docker ps it will status as unhealthy, because default time has been elapsed, now container is unhealthy.do inspect ,in logs we can see container is unhealthy because curl command not found. now stop and remove the container.

- create another container :  " docker run -dt --name temp --health-cmd "curl -f http://localhost " --health-interval=5s busybox sh ". here the interval is 5sec, since we havn't specified retries, it will have the default of 3.). now do docker ps we can see status is unhealthy after 10 sec, after it tries for 3times. now create one more container with name tmp2 and also menation retries as 1

 " docker run -dt --name temp --health-cmd "curl -f http://localhost " --health-interval=5s --health-retries=1 busybox sh ". when we do docker ps we immediatly see status as unhealthy, because in 5 sec it will retry 1 time and it will show result

IMP : whenever we are making use of curl specifically in scripts it is advisable to also make use of "-f" which is used to make it fail silently
      this is mostly done to better enable scripts etc to better deal with failed attempts.
- without -f whenever we do curl on a non existen part we get very big amount of error in html page with "404 not found"
- if we use "-f" immediatly we will get a 1 line error stating that  " returned error : 404 not found "

- if we run : " curl -f http://non-exist.in || exit 1  " here it will show : " returned error : 404 not found " and ( i did curl in side sudo command in my ec2). now this curl failed. i automatically logged out from sudo, and i go to ec2 login terminal with(ubunt@ipaddress)


***********************************************************************************************************************************************************
ENTRYPOINT Instruction :
- The best use for ENTRYPOINT is to set the image’s main command.( we can even set the main command with cmd. whenevrer image is launched whatever things we specify with cmd  will run. the problem with cmd is the command can be overriden. But ENTRYPOINT doesn’t allow you to override the command.t is important to understand the distinction between CMD and ENTRYPOINT. )

Demo : go to cli. and do ls. we can see dockrfile. open dockerfile with nano. inside we can see
 : FROM busybox
   CMD ["sh"]           build an image bas01 from it " docker build -t base01 ". also launch a container from base01 image. do ps. we can see "sh "under command section . So now whatever we specify in cmd in dockefile can be overriden
- run one container from the same image with " docker container run --name base02 base01 ping -c 10 google.com " . now run docker ps we can see "ping -c 10 google.com " undr command section. after 10 times the command would exit. 

- open docker file  and write : 
  FROM busybox
  ENTRYPOINT ["bin/ping"] . build a docker image base03 from it. and run a container from it also ping to google.com for 10 times. when we do docker ps we can see " bin/ping c-10 google.com " under command section. here eventhough we ping google.com , the main binary remains same because of ENTRYPOINT.

***********************************************************************************************************************************************************
WORKDIR Instruction : 

- The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile
  ex : we have container A. within container a there are 2 folders available.any instruction we have (ex:RUN), that we want to use that instruction in a specific folder then we can make use of WORKDIR. once we have created "WORKDIR folder 1". now any subsequent instruction that is specified after the WORKDIR 
 will associate with folder 1.

DEMO : create a dockerfile: 
	"FROM busybox
	 RUN mkdir /root/demo                            we will create a new folder demo under root directory     
       WORKDIR /root/demo                              we will set the WORKDIR to root/demo
       RUN touch file01.txt                            create a file called as file01.txt
       CMD ["bin/sh"]                                   CMD instruction which loads the shell.
- now build a dockerimage workdir1 from dockerfile.and run a container from it
  " docker container run -dt --name workdir1 workdir1 sh " at end sh : we want to load a shell.
- now connact to that container : " docker container exec -it workdir1 sh " [o/p : ~/demo ]. here we automatically connected to demo directory.if we do pwd:
  we are automatically inside root(root/demo) where our WORKDIR was set. and run ls we can see file01.txt.

 NOW : 
       WORKDIR /root/demo     ( means here we set workdir to root/demo. so any instruction that are following that ( run ,cmd) these instructions will run 
                              under /root/demo directory     
       RUN touch file01.txt      ( here the touch command will run under /root/demo)
       CMD ["bin/sh"]            ( even cmd, whenever we connect to the container will automatically connects under root/demo folder.

- The WORKDIR instruction can be used multiple times in a Dockerfile
Sample Snippet :
WORKDIR /a
WORKDIR b
WORKDIR c
RUN pwd

- here 3 workdir instructions (/a,b,c). So when we run the pwd command which will show o/p as : a/b/c. combined o/p of all the dir's that have been specifie
DEMO : dockerfile :
	"FROM busybox
	 RUN mkdir -p /root/demo/context01/context02  : here we are creating a folder. then we have multiple workdir instructions.
       WORKDIR /root/demo	
	 WORKDIR context01          give onemore WORKDIR 
	 WORKDIR context02                              
       RUN touch file01.txt           Now file01 will create under  /root/demo/context01/context02/file01.txt  
       CMD ["bin/sh"]                                   
- Now build a image from it. and run a contaijer from it.and connect to container normally .we can directly logged into" /demo/context01/context02".
  run pwd : /root/demo/context01/context02 we will get this.if we run ls we can see file01.

***********************************************************************************************************************************************************
ENV Instruction : The ENV instruction sets the environment variable <key> to the value <value>. (key-value pair).

DEMO : ENV NGINX 1.2                ( here key is nginx, and value is 1.2)
       RUN curl -SL http://example.com/web-$Nginx.tar.xz     ( run instruction does the curl on link. " $Nginx.tar.xz" means here the value(1.2) associated                   
                                                               with nginx key will be replaced here
       RUN tar -xzvf web-$NGINX.tar.xz       ( here we are using variable($NGINX) again . this will replaced with whateever value we specified in ENV instr.
CREATE A DOCKERFILE : 
	FROM busybox
	ENV NGINX 1.2
	RUN touch web-$NGINX.txt
	CMD ["/bin/sh"]
	Using ENV in CLI:
- build an image fromit. and create a container from it.and connect to container using sh. and run ls inside container we can see " web-1.2.txt"( 1.2 is the value associated with ENV variable.

- You can use -e, --env, and --env-file flags to set simple environment variables in the container you’re running or overwrite variables that are defined in the Dockerfile of the image you’re running.
- docker run --env VAR1=value1 --env VAR2=value2 ubuntu env | grep VAR
DEMO : create a container. and specify a environment variable (--env USER-ADMINUSER) 
 " docker run -dt --name env02 --env USER=ADMINUSER busybox sh ".
- connect to this container . now if we want to see value associated with USER then run " echo $USER " we will get o/p=n ADMINUSER. 

***********************************************************************************************************************************************************
Tagging Docker Images : 

- in cli run ls.we have dockerfile . build an image from it.after image is built we get image id.this inmage is not tagged.if we have multiple of such images with diff functions, then it is diff to remember what exactly image does.if there is tag we can easiuly understand. 2 ways to create tag: 
- " docker build -t . "( -t is for tag. we can specify the tag that we want to associate with it.) 
" docker build -t demo:v1 . " now run docker images .we can see demo under repository ,v1 under tag.

2nd : incase we already have an image and doesn't have tag.in that case we can tag 
    " docker tag image_id demo:v2

- also we can also associate a tag associate with existing image.we have ubuntu and is already tagged. now i want to add 2nd tag.now run :
   " docker tag ubuntu:latest myubuntu:v1".
run docker images, we can see 2 images with different tags with same image_id.

***********************************************************************************************************************************************************
DOCKER COMMIT : 
- when we launch a new container from image and specifically when we are modifying lot of files to achive specific taret , it can be useful to commit those changes   .(container file changes or settings into a new image)
EX : we have a running container , and we have modified a lot of files and configurations within that running container.we can commit that running container so that it forms a new image.

- run " docker container commit container_id new_image_name"
DEMO : we have busybox image. launch a container from it and login to it with exec -it container_name(busybox) sh. and inside go to root folder.and create a folder change1.txt with touch.

 Now i made some modifications in this image.now i want to create a image from trhis container . now exit from container and run ps.
 run " docker container commit busybox busybox-modified-image". now run docker images. we can see busybox-modified-image new image.
- we can run a new container from the custom image. and connect to that container in order to verify changes that we have made to the container previously before creating an image are still present or not. run ls we can see change01.txt.

- when we are doing docker commit there are certain aspects that we are able to modify. run docker inspect and take original busybox image container.there we can see CMD : "sh". also we have Env:Path:.. with commit we can modify all these aspcects. we can change cmd from sh to csh or bash
- run " docker container commit --change='CMD ["ash"]' busybox(containername)  ;here we are specifying the change ,and we giving the instruction that we want to change. we want change instruction of CMD.we want to change it from sh to ash. now new image formed. and launch a contaner from it and we can see ash under command section.

- The --change option will apply Dockerfile instructions to the image that is created
Supported Dockerfile instructions:
CMD | ENTRYPOINT | ENV | EXPOSE
LABEL | ONBUILD | USER | VOLUME | WORKDIR

- By default, the container being committed and its processes will be paused while the image is committed.( whenever we commit a container, the process will be paused while the image is commited. there is a way in which we can override this.but remember the process get paused while commiting)

***********************************************************************************************************************************************************
Docker Image Layers : 
- A Docker image is built up from a series of layers.
- Each layer represents an instruction in the image’s Dockerfile.
 If we look into diag. it has multiple layers.Each layer represents an instruction in the image’s Dockerfile.
- " FROM ubuntu       ( layer 1)
    COPY . /app       ( layer 2)
    RUN make /app
    CMD python /app/app.py .      this is a dockerfile. when an image is build from this dockerfile, this image is consists of multiple layers. each of layer is corresponding to instruction present over here.
- in the diag the bottom layer is formed from from ubuntu. for 2nd layer=Copy,3rd = run..( there can be layer of 0 bytes)

- The major difference between a container and an image is the top writable layer.
- All writes that we make to the container that adds new or modifies existing data are stored in this writable layer. the image that has been built from docker file, this image is read only. we can't really modify this image.on top of that there is (R/W : read/write layer). and any modification that we make if we add a file, if we remove this file that modification is been returned to R/W layer.

In diag : there are multiple docker containers each and every docker container can share same image.there are 4 containers which are using ubuntu image.the ubuntu image is ready only.And each container has it's R/W layer and any changes that they make got stored to that r/w layer. this is best . if we have 10 containers and are depends on a single base image instead of creating a new image for each.

 - if we want to see layers associated with specific image we can do that with " docker history " command.
Demo : run docker images.if i want to see layers associated with linux image run " docker history nginx " . we can see an amount of layers associated with nginx image.if we know dockerfile we will be able to see each of layer.we have cmd,add,env layers
- amount layers the image has diff from each image to image.

- I have a dockerfile with 5 instructions. this 5 instructions will correspond to 5 layers.
 in layers format : 
    RUN dd if=/dev/zero of=/root/file2.txt bs=1M count=100 - layer-3
    RUN dd if=/dev/zero of=/root/file1.txt bs=1M count=100 - layer-2
    FROM ubuntu       - layer -1 =90mb
- here 1st instruction is from ubuntu. it will pull ubuntu image which is 90mb. here we create an image from above dockerfile.now run " docker image history image_name". now we can see many layers.here the (from ubuntu)ubuntu image itself has multiple layers
- finally layers : the ubuntu images layers + the instruction layers in dockerfile

- in dockerfile we will write 2 mores instructions to delet the filkes that we created in the dockerfile above file01,file02.
 -  RUN rm -f /root/file1.txt      - layer 4 - 0B
    RUN rm -f /root/file1.txt      - layer 4 - 0B
    RUN dd if=/dev/zero of=/root/file2.txt bs=1M count=100 - layer-3- 100mb
    RUN dd if=/dev/zero of=/root/file1.txt bs=1M count=100 - layer-2- 100mb
    FROM ubuntu       - layer -1 =90mb
 Q: after i run the whole docker image what is the size of docker image is it 290mb or ? the size will be same as 297mb when we ran earlier with 3 instructions. it is same also when we write instructions to remove 2 files.

Q : eventhough we write instructions to remove 2 files of 200 mb why the size is still same 290mb.
- we can uderstand with diagram of layers. inthe layer 1 100mb,2, 3 layerds 100mb each. 0B in layer 4,5 we are reoving both files but they are of size 0B each and they are dealing in layer 4,layer 5 and below layers(1,2,3) are not modified.

- even if i remove file01 which is generated in layer2 and this command is executed in layer 4, even i remove it ,the file still exists in layer2. this is the reason why even if we do rm -f in the later layers. the file will exist inthe older layers(2,3).

- incase if we execute(rm -f) instructions in the same layers itself(2,3) then the overall size of image will decrease.we can do that by "&&"
demo :  
    RUN dd if=/dev/zero of=/root/file2.txt bs=1M count=100 && rm -f /root/file2.txt     layer 3
    RUN dd if=/dev/zero of=/root/file1.txt bs=1M count=100 && rm -f /root/file1.txt     layer 2
    FROM ubuntu       - layer -1 =90mb layer 1

- in layer 2 the file will be created and then the file will be deleted. it's happening in the same layer. finally this layer will have size 0B.
Now build image from this dockerfile.in images we can see the size of image is 87mb. run "docker image history image_name"

* it is important to have as minimum as layers for dockerfile,this will helps to save storage and performance improvment

***********************************************************************************************************************************************************
Managing Images with CLI : 
- Docker CLI can be used to manage various aspects related to Docker Images which include building, removing, saving, tagging, and others.
- We should be familiar with the docker image child-commands
- Here are some of the sample child commands that are available 
docker image build
docker image history
docker image import
docker image inspect
docker image load
docker image ls
docker image prune
docker image pull
docker image push

- if we run docker image --help we will see all the child commands(these are listed above).
- EX : docker image pull : if we run docker pull splunk; it will pull latest splunk image.the same thing can be achieved with docker pull command without using image. 
- In terms of recommendations : docker recommend to use : " docker image pull " command.

- if we run " docker images " or " docker image ls " both will give list of images. but docker image ls is recommended by docker.

***********************************************************************************************************************************************************
Inspecting Docker Images : A Docker Image contains lots of information, some of these include:
Creation Date
Command
Environment Variables
Architecture
OS
Size
- When we are dploying docker image there are certain automation and that automation requires some of the above details. it is imp to now on how we can get the specific details and also how we can filter certain info,from all the above info.
- docker image inspect command allows us to see all the information associated with a docker image.

DEMo : in cli we have nginx image. run " docker image inspect nginx". we cansee lot of info ( sha id, env variables,command,exposed ports,labels etc). among all this info i might want to know a certain aspect of it. so i just want to see the hostname associated . How to do :
- easiest way is make use of linux tools(grep): run -
  " docker image inspect nginx | grep Hostname " . it will give all the fields associated with hostname.

- But recommended way is to make use of "format" command. run " docker image inspect --help " it will give options of inspect:
options :
  -f, --format string  format the o/p using the given Go template ( this format allows us to format and retrieve the info from the entire chunk based on the use case we have)
- run " docker image inspect nginx --format='{{.Id}}' " it will give the id of nginx as o/p. here id is easily we can get. but certain things has parent child. when we run docker inspect nginx some prameters like (ContainerConfig has parent childs 'hostname,domainname,user'.Now try to format 'containerconfig

 - run " docker image inspect nginx --format='{{.ContainerConfig}}' " .this will retriev all the values(hostname,domainname,user etc) associated with containerconfig. Here the o/p contains only values.it doesn't show keys associated with values if we want to see keys also we have to specify 'json'
- run " docker image inspect nginx --format={{json .ContainerConfig}}' " her the o/p will containboth keys and values.
- if i want to see just value associated with hostname(which is parentchild of containerconfig) then use(containerConfig.Hostname):
 run " docker image inspect nginx --format={{.ContainerConfig.Hostname  

***********************************************************************************************************************************************************
Pruning Docker Images : 
- docker image prune command allows us to clean up unused images.
- By default, the above command will only clean up dangling images.
- Dangling Images = Image without Tags and Image not referenced by any container.( it is "and" condition.like both the given statements need to satisfy)

Demo : run docker images.and we can see list of images. one image doesn't have tags.the dangling image which doesn't have tags and image not referenced by any container.
- run "docker image prune " : this will remove all dangling images. now if i run docker images we will see the image that doesn't have tags was deleted.
- run " docker image prune -a " : this will remove all images which are not referenced by any container.many times people pull images, but they never start any container. now it will delet all the images that doesn't have any container associated with it. now run docker images we will see only ubuntu image,except that all are deleted. run docker ps we will see one container associated witrh ubuntu image. that's why ubuntu is not deleted.

- create a dockerfile and build a docker image from it. now run docker images, we can see ubuntu and new image. new image doesn't have any tags. launch a container from new image. now run " docker image prune " ; which will delete dangling images. now see docker images. it didn't delete any image because new image doesn't have any tags but it has a container which is associated with it .so now new image is not a dangling image.

- pruning process is not only associated with images. it can be associated with containers,networks,volumes.

***********************************************************************************************************************************************************
FLATTENING DOCKER IMAGES : ( modify image to single layer ) 
- In a generic scenario, the more the layers an image has, the more the size of the image.
- Some of the image sizes go from 5GB to 10GB.
- Flattening an image to a single layer can help reduce the overall size of the image. because more the layers, more the size of image

Demo : run dokcer images. i have ubuntu among images. now run " docker image history ubuntu " we can see  5 layers in ubuntu. aim is to make it to 1 layer.
- to merge multiple layers into a single layer there are multiple approaches :

1.Import and Export the container : crreate a container myubuntu from ubuntu image.then run " docker export myubuntu > myubuntudemo.tar " here  i am exporting container.and giving o/p name as myununtudemo.tar
- run ls we can see : myubuntudemo.tar.
- ls -l myubuntudemo.tar : we can see size of file. now we have to import it back to do that run : 
  " cat myubuntudemo.tar | docker import - myubuntu:latest ". now run docker images .we can see myubuntu image which size is little less compared to ubuntu image. now run " docker image history myubuntu " . it will show only one layer ( which is imported).

***********************************************************************************************************************************************************
Docker Registry : 
- analogy of registry : it is something similiar to github. a coder kept a code in github , which has centraliz=se access, will be able to pull code. if we put code in laptop or hdd, what if it fails we will loose code. that's why it is recommended to store it in a centralaise location. 

- same concept applies to docker registry.
- if we are having custom images, it is not recommended to store in local.we should store it in a centralaize place where everyone can access it an dit can remains safe.

DOCKER REGISTRY : 
- A Registry a stateless, highly scalable server-side application that stores and lets you distribute Docker images
- Docker registry is used to store custom images. 
- Docker Hub is the simplest example that all of us must have used.if we do docker pull, it will pull from a registrywhich is stored in cloud which is docker hub.

- There are various types of registry available, which includes:
Docker Registry    ( open source,limited features. but it allows us to have the basic functionality of storing the images)
Docker Trusted Registry(DTR)- enterprise variant.many features
Private Repository (AWS ECR)
Docker Hub - it is a cloud based offering.we can store our docker images.

- Go to docker hub . and run "docker pull registry". here registry is a basic registry application which offers a simple functionality. if we want some complex functionality opt for docker trusted registry.
- in dockerhub if we scroll down we can see doc's to pull regsitry also " docker run -d -p 5000:5000 --restart always --name registry registry:2 ". it is giving port and giving tag as 2. now run docker ps .we can see registry container and is listening on port 5000.

- now we have to verify weather we are able to push the custom image inside the registry application.
- now pull ubuntu image. and run images.we can see 2 images regustry and ubuntu. when we do pull it basically fetches from dockerhub. we can also have images in other location.(like listed above 4 types of registries)

- Open AWS ecr. we canb see 2 repo's. open 1 repo and we can see 2 images.it is not necessary to pull from docker hub only., we can pull from ecr,or any other registry.if i want to push my commited image, i can push it to dockerhub,docker registry,ECR.

- we need a format to push an image to a registry : like i have ubuntu and a registry in local.i want to push ubuntu to tje registry.
- run " docker tag ubuntu:latest localhost:5000/myubuntu "     i have to specify a dns name where registry appl is running.in my icase it is localhost:5000 and specify the name of image(newname) that i want to store in registry.
- now run docker images. i can see 3 images(registry,ubuntu,localhost:5000/myubuntu). now if want to push localhost.. image to registry then run :
 " docker push localhost:5000/myubuntu  " it will push the image to our docker registry appl which is currently running.

- if we want to pull same " docker pull localhost:5000/myubuntu " : here we are specifying the registry where it is running followed by image name that we want to pull.

- now delete localhost,ubuntu images. except registry image. run images we can see only registry image.
- try to pull myubuntu image : " docker pull localhost:5000/myubuntu " : it will pull myubuntuimage from registry which is running on localhost:5k.

***********************************************************************************************************************************************************
PUSHING IMAGES TO CENTRAL REPOSITORY(dockerhub) : 
- if i want to push images to ecr or dockerhub we need to go through an authentication process.
Demo: in docker create a repository 5066/myubuntu. to push image to repo we can see " docker push 5066/ubuntu:tagname

in cli : run images we can see registry image.run pull busybox.we have 2 images now.
- for dockerhub we need to do login : " docker login " it will ask username(of dockerhub),pswrd(for username). now how we can push busybox to registry in dockerhub. the run " docker tag busybox 5066/mydemo:v1                       ( here dns name : 5066(username)/:mydemo(name):v1.
- now run docker images : we can see 5066/mydemo image . now push to dockerhub : " docker push 5066/mydemo ".
- go to dockehub. we can see one more repo of '5066/mydemo'. within that repo 1 tag associated which is 'v1'

- now if we want to pull docker image from repo. first delete the 5066/mydemo image and busybox image. now run : " docker pull 5066/mydemo". now we can see 5066/mydemo image in cli.

- we can do the same with ecr. if we want to push or pull 1st we have to login particularly for private regsitry or repo's.
- later to logout run " docker logout"

***********************************************************************************************************************************************************
APPLYING FILTERS FOR DOCKER IMAGES : 
- how we can search for a docker image from a docker registry. registry contain lot of iumages.we run certain docker commands to find a docker image.
- docker search : if we run docker search we can see lot of lists(images) related to nginx
- run docker search --help : it will give options : 
Options:
 -  -f, --filter filter   Filter output based on conditions provided 
  got to docker docs. there we can see filtering in dokcer search . filter based on stars,IS-AUTOMATED(whether the image has automated build) or, IS-OFFICAL(list the only official images)
 run "docker search nginx --filter "is-official=true" . now we should only see 1 image. becuase only 1 image in dockerhub.


-     --limit int       Max number of search results
- run docker searhc nginx --limit 5 : it will give 5 results related to nginx ( here it will give listing of 5 ). and top 5 will be based on stars
   
***********************************************************************************************************************************************************
MOVING IMAGES ACROSS HOSTS : 	
use case : James has created an application based on Docker. He has the image file in his laptop.He wants to send the image(myapp) to Matthew over email
 we required few steps to do this.
- save that image to archive(tar) file that can be achieved with docker save command
 run : " docker save busybox > busybox.tar ". here busybox is image name. and we are saving it to busybox.tar. now we will send this tar file over mile.
- once mathew recieves the tar file from james, he can run the docker load command which will load an image from tar archive
 - run " docker load < busybox.tar " 

***********************************************************************************************************************************************************
BUILD CACHE (layer cache) :
- Docker creates container images using layers.
- Each command that is found in a Dockerfile creates a new layer.
- Docker uses a layer cache to optimize the process of building Docker images and make it faster.
DEmo : when we try to build an image from a dockerfile. under that for many of the instructions it is directly using cache instead of building it from cache.
- in cli run ls we can see 2 files(dockerfile, xy.txt[it has some packages mentioned inside file])
in docker file : 
 " 	FROM python:3.7-slim-blaster
	COPY . .                             ( it is copying the xy.txt to inside the container )
      RUN pip install --quiet -r xy.txt    ( install all the packages mentioned in py.txt )
 	ENTRYPOINT ["python", "server.py"]     "
- build an image from this dockerfile. by " docker build -t without-cache . "
 it will show 4 layers each layer has installing packages.
- build anothe rimage with same docker file with image name different 
  run " docker build -t with-cache . " . it will build an image very fastly. here except 1st instruction every instruction is making use of cache.
- if a specific instruction based on which cache is already available with docker the docke rinstead of building the instruction details from scratch, it can directly make use of cache.

- If the cache can’t be used for a particular layer, all subsequent layers won’t be loaded from the cache
 ex in diagram : we have old docker file,new dockerfile
  three layers in each of them ( A,B,C) ,(A,B_CHANGED,C). for the b layer we made certain changes. on the right side we have a logic if the docker use cache or not. for layer 1 ( YES,A==A) cache will be used 
  		layer 2 ( NO ,B!=B_CHANGED )                          for this since instructions are different we can't use cache.
		layer 3 ( no previous layer(2nd) wasn't loaded from cache) 
- since the layer 2 is build from scratch. all the other layers(3,4,...) that are beneath layer 2 will also be build from scratch( means they don't use cache)


***********************************************************************************************************************************************************
 
SECTION 3 - NETWORKING : 

***********************************************************************************************************************************************************
OVERVIEW OF DOCKER NETWORKING : 
- it is very imp to know about docker networking. 
- i have multiple docker containers.some of containers wants to communicate with internet, some might want to be in isolated condition because they might have critical data.
- in a Vmware application i have a Vm running. in Vmware i can edit network settings : ( Nat,Host-only,custom,Bridged) similiar to this we have diff network drivers for docker
Now docker :
- whenw e install docker in workstation by default Docker takes care of the networking aspects so that containers can communicate with other containers and also with the Docker Host
- in linux system, if we have docker installed , if we run ifconfig : we can see docker0 interface which get created. docker0 interface is for bridge base networking.
- Docker networking subsystem is pluggable with the help drivers.
 There are several drivers available by default and provides core networking functionality.( each fdrive provide own fxnlaity depending upon use case)
 bridge
 host
 overlay
 macvlan
 none

- in ec2 if i run docker network ls : i will get bridge,host,none. incase if we are using enterprise we will have one more drive(overlay).
- diagram : left side i have a bridge network. this bredge network has a cidr of(172.17.0.0/16). any time a docker container created with the bridge network it will automatically have the ip address within the range defined in bridge network.
- a docker can have multiple networks. it is not necessary to have all networks of type bridge.we can also have diff network. in diag on right side i have a custom network of 192.168.0.0/16 . the containers which will launch under this custom netwotk will have the ip address range of 192.168.0.0

- if i want to see more details associated with bridge network then run : " docker inspect bridge " it will give a lot of info. in that info we can see the 
  "subnet" : 172.17.0.0/16 " any containerlaunch in bridge network will get an ip from this subnet. also "gateway: 172.17.0.1 "
- also we launch a container already. so it shows under inspect: container_Name": "myubuntu", "IPv4Address": "172.17.0.2/16",
- run "ifconfig" in docker0 : inet : 172.17.0.1.. run inspect we can see gateway: 172.17.0.1
- for myubuntu container if it wants communicate with internet it needs to send traffic to this 172.17.0.1 specific gateway. this gateway is nothing but docker0 interface

- when we create a docker cointainer we can launch it any network which is available.
 run docker network ls. we can see 3 types of networks. bridge,host,none
- run docker network inspect host ( running inspect on the host network)
- whenw e talk about host network it means that a network which has the host network driver. like a network with host driver, network with bridge driver.
- let's say there is no container running in specific network.nowlaunch a container :
 run " docker container run -dt --name myhost --network host ubuntu " launching a container in host .
run " docker inspect network host " we can see one container availablein this network o/p.

***********************************************************************************************************************************************************
BRIDGE NETWORK DRIVER : 
- A bridge network uses a software bridge that alows containers connected to the same bridge network to comunicate while providing isolation from containers which are not connected to that bridge network.
ex : i have 3 containers. the containers will have ip address depending upon the subnet associated with bridge network.
  we hav create a Docker0 bridge (172.17.0.1) and this acts as a gateway . now a container which wants to communicate with internet has to go through the Docker0 bridge, from there we have the IPtables/NAT and from there we have eth0 interface. Here eth0 has the connection towards the internet.

- the containers that create in these type of bridge network can also communicate each other.
- run docker ps : we can see a container demo from ubuntu is running. if we want to see the network associated with the container run " docker container inspect demo(containername) " : here we can see under the network section it showinhg bridge. also it shows the ip address the container has etc.

* whenever we create a container and if we don't specify the network type, then the container gets created in bridge network.
- create 2 containers from ubuntu image name bridge01,bridge02. login to 1st container and run ifconfig. it will say command not found because we have to install net-tools. then run " apt-get update && apt-get install net-tools ".now run if config: 

 we can see there is "eth0"interface in the container and that eth0 interface has ip address of 172.17.0.3. now come out of container.
- now login to bridge02 container and run same ifconfig , install net-tools, again run ifconfig: 
- now run docker container inspect bridge02 : we can see lot of info including ip address (172.17.0.4).
- now connect back to bridge01. and try to ping 172.17.0.4(bridge02 ip address) .it will throw error. install " apt-get install iputils-ping ". now try to ping bridge02.now we should be able to ping docker container2.

- in bridge network the communication b/w containers happen automatically.for other networks comminication b/w containers doesn't happen
- whatever traffic that container might want to send b/w each other and b/w the container and the host it has to go through the Docker0 Bridge interface which is 172.17.0.1
- go back to cli and run " route -n " it will show Destination(0.0.0.0)=gateway(172.17.0.1)

Bridge is the default network driver for Docker.If we do not specify a driver, this is the type of network you are creating. and containers also get launch in bridge network by default
When you start Docker, a default bridge network (also called a bridge) is created automatically,and newly-started containers connect to it unless otherwise specified.
We also can create a User-Defined Bridge Network which is superior to the default bridge.
- There is a lot of diff b/w default bridge network and user-defined bridge network.

***********************************************************************************************************************************************************
USER DEFINED BRIDGE NETWORK : 
- in bridge network Docker0 bridge acts as a virtual switch.
- any container that i launch within the bridge network , that container has its associated interface.the interface inside a container is called as eth0.and from outside it is something like 'veth002..'. each of this specific interface has it's own set of ip address.
- run brctl show( brctl is a package which is not available in windows) : this will show :
  bridge name , bridge id,and list of interfaces('veth002..','veth00jau3..'etc)  which are part of this bridge

- There are various diff's b/w bridge and user-defined bridge newtworks. some of them :
 when we install docker it basically has it's own default bridge which is created. however we can also create our own user-defined bridge. user-defined comes with it's benefits.
some of them :
1. user-defined bridge provides better isolation and interoperability b/w containerized appl's
2. user-defined bridge provides automatic dns resolution b/w containarized applications.
3. containers can be attatched and dettathced from user-defined networks on the fly.
4. each user define network creates a configurable bridge.
5. linked containers on the default bridge network share env variables.

2. user-defined bridge provides automatic dns resolution b/w containarized applications : inorder to create container in user-define network we need to create a user-define bridge.
demo: root@ip-172-31-38-252:~# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
57f89e52bf9a   bridge    bridge    local
f96ec0d54b7d   host      host      local
e677e6374f2b   none      null      local
- the names that we give is irrespective.we can give any name to our network. however the driver that is connected to network is matter.
to  create a new network of driver bridge run " docker network create --driver bridge mybridge "
 o/p :
 NETWORK ID     NAME      DRIVER    SCOPE
 0bb6342f59ea   mybridge   bridge    local

- if we don't specify any network driver docker will cretae a default driver which is bridge driver " docker network create mynetwork "
o/p : 
NETWORK ID     NAME      DRIVER    SCOPE
854483b7c2ac   mynetwork   bridge    local

- now inspect mybridge network : " docker network inspect mybridge " : we will get lot of info : this network has subnet of : 
  "Subnet": "172.18.0.0/16",
  "Gateway": "172.18.0.1"

- run " ifconfig" : we can see two more interfaces that has been cretaed for mybridge and mynetwork drives. for the default bridge we have docker0 interface
- for mybridge and mynetwork we have 'br-0bb6342f59ea','br-854483b7c2ac' interfaces.
root@ip-172-31-38-252:~# brctl show
bridge name     bridge id               STP enabled     interfaces
br-0bb6342f59ea         8000.024228d7c567       no
br-854483b7c2ac         8000.0242b7cc1131       no
docker0         8000.0242e1934d56       no
- here since we don't have any containers associated with bridge network , so it doesn't show any interface associated with them


- create container that will associated with mybridge network driver :
  " docker container run -dt --name mybridge01 --network mybridge ubuntu ". container created. now run brctl show :
root@ip-172-31-38-252:~# brctl show
bridge name     bridge id               STP enabled     interfaces
br-0bb6342f59ea         8000.024228d7c567       no              veth39d26a3
br-854483b7c2ac         8000.0242b7cc1131       no
docker0         8000.0242e1934d56       no

- here we can see br-0bb6342f59ea(mybridge) has 1 interface which is created.
- if i run " docker network inspect mybridge " : we can see a container that has been created and ip address which is associated with container.
- create 1 more container in mybridge network .  " docker container run -dt --name mybridge02 --network mybridge ubuntu ". container created
- run ps we can see 2 containers mybridge01,02.
-login to container bridge01 with bash and install net-tools and ip-utils ping.
- now in user-define bridge we will be able to ping.we will able to connect based on container names or dns. run " ping mybridge02" we can see bridge01 able to see ping successful on bridge02(ipaddress)

- when we talk about normal default bridge : now imagine we have 2 containers in default bridge.connect to one of the 2 containers. now tring to ping bridge02 we will get reply " name or service not known ". eventhough we have bridge02 oin default bridge network

***********************************************************************************************************************************************************
HOST NETWORK :
- This driver removes the network isolation between the docker host and the docker containers, so the containers can use the host’s networking directly.
- For instance, if you run a container that binds to port 80 (appl in conytainer bind to port 80) and you use host networking, automatically the appl which is running inside the contaienr will automatically bind to the port 80 of the host ip address.

- when we compare this with bridge base networking what we have to do is we have to publish the port with '-p', in host base network this is not required.
2nd adv of host network over bridge network : if we look in bridge network every container has its own interface associated and ip address . now if we want monitor the traffic of the eth0 interface that is not possible.
- let's say i want to install services like intrusion detection system we can't install that in a container that is in bridge network.if we do that then the ids will monitor the eth0 of the container will not monitor that of the host.
- however if we can monitor the traffic associated with eth0 of host from application then we have to use host network.

- in CLI : run docker network ls : we can see host network in that list.
- now create a contaienr in host network : run " docker contaienr run -dt --name myhost --network host ubuntu " contaiwner cretaed.and login it using bash
- and install net-tools packages in container.and exit.
- creeate one more container mybridge03 and don't mention network. it will automatically create in bridge network.

- run ps. we can see 2 containers.myhost,mybridge03.  login to docker container mybridge03 and install net-tools. now run ifconfig we can see 'eth0' interface .run ' netstat -ntlp' we can see no ports or services are running. now i will install nginx using apt-get . now start it by " etc/init.d/nginx start'. now run 'netstat -ntlp'. it will show nginx process which is connected on port 80. now come out of container. and run 'netstat -ntlp' outside container. we can't see any port 80 opened on host level. so nginx appl and the port which it got opened was inside the container.in bridge network whenever we want that the outside world should be able to connect to port 80 we have to specify '-p 80:80' where '-p' is to publish and open host on port 8080 and forward the traffic to port 80 of the container. this is specific to bridge.

- for host base networking this is not required.it directly associate itself directly with the host network.
- now connect to hostbase networking container myhost using bash. and run netstat -ntlp. it will show list of ports which are listening on host server.
 if we run 'ifconfig' it will show lot of interfaces(docker0,eth0) associated with host. let's say i have intrusion detection system . my idea is yto monitor the traffic associated with eth0 of host.if we want to put the ids appl inside contaienr then i have to put it in host base networking.because in hostbase networking we see all the interfaces of underline host are been exposed inside container.

- now install nginx inside myhost container and start .now run netstat ntlp we see nginx is running on port 80. now logout from container.
- run netstat -ntlp we will see 80 port of nginx on host.port 80 is from the docker container.for host base networking we don't have to mention -p 80:80.
one disadv : we can't really run 2 appl which are running on port 80.let's say i have 2 appl in 2 diff appl's. both the appl can't start on same port.because the port is associated with host.

***********************************************************************************************************************************************************
NONE NETWORK : If you want to completely disable the networking stack on a container, you can use the none network.
- in this there is no ip address which gets configured on the container.so that the container doesn't have access to any external network or it doesn't have access to any other containers. smiliar to this we used to have network for our virtual machine where we used to do a virus testing etc so that the virus can't escape and it can't connect to other virtual macines or towards the internet.  this is about virtualization.

- run docker network ls we can see none network type. launch a contaienr mynone from none network driver from alpineimage.
- login to container using 'ash'. we already discussed in this approach there is no ip address associated with container. this container doesn't have access to any external network or it doesn't have access to any other containers.
- run ifconfig we can see only one loop'lo' interface.
-run ' ping google.com'. iam not able to process.becausethere is no networking stack or ip address associated with it.

***********************************************************************************************************************************************************
 Publishing Exposed Ports of Container : 

- We were discussing an approach to publishing container port to host.
- Docker container run -dt --name webserver -p 80:80 nginx
- "-p" : This is also referred to as a publish list as it publishes the only a list of the port specified.

- There is also a second approach to publish all the exposed ports of the container.
- docker container run -dt --name webserver -P nginx
- This is also referred to as a publish all.
- In this approach, all exposed ports within the container are automatically published to random ports of the host.
ex ; launch a container from nginx : " docker container run -dt --name web nginx : 
docker ps : we can see port :80/TCP. here the port 80 is inside the container. try to curl " curl 127.0.0.1:80 " it will show connection refused.

-run " docker container run -dt -p 8080:80 --name web01 nginx ". run docker ps we se the port " 0.0.0.0:8080->80/tcp " means the port 8080 of the host is mapped to the port 80 of the container. now run " curl 127.0.0.1:8080 " we will be able to see the dfault nginx page.

- Now use '-P' " docker container run -dt -P --name web01 nginx " . run ps : under ports: 0.0.0.0:3786->80/tcp. here any random port(37648) which got opened and automatically assigned itself to forward to the port 80 inside the contaiuner. now run " curl 127.0.0.1:32768" here we are able to see the default nginx html page. generally when we make use of '-P' what happens is : any ports which is exposed in a specific image '-P' will correspndingly create a random port in the host and it will map it to the exposed port of the container.in this case your not specifying any port it automatically does for us.

***********************************************************************************************************************************************************
Legacy Approach for Linking Containers : 
DEMO : create a container from busybox using dt and sh and name container01.
- create a container from the link . when we specify a link we have to specify a source container and the alias name as container and launch it as name container02 from busybox. " docker container run -dt --link container01:container --name container02 busybox sh ".
- now connect to container 02 . and ping conter01.here it automatically resolves the ip address associated with container01.this is in the default bridge network and not in the user-defined bridge network. this feature also similiar with the user-defined bridge network.
-" --link " option was which is legacy and is the older approach of defining things.however later docker came up with the approach where this kind of approach is already available in the userdefined network.


***********************************************************************************************************************************************************


SECTION 4 : ORCHESTRATION :-


***********************************************************************************************************************************************************

OVERVIEW OF CONTAINER ORCHESTRATION(COS): 
- Container orchestration is all about managing the life cycles of containers, especially in large,dynamic environments
Demo : i have 3 virtual machines.i have webserver container and app server container in each vm1 and vm2.if we have small env like this we can go and start the container manually. but there are other aspects apart from starting the container. one among them is monitoring. let's say the webserver container in 

VM1 went down, in order for prod env to work perfectly we  need to have a min of 2 webserver containers to work always. what happen is i installed webserver and it went down.who will monitor this web server container?.and also monitoring script should have event driven mechanism in such a way if webser goes down it also restart the container or due to some reason the container is not restarting then the script should automatically start a web container in another VM3. this can be acheived with the help of Container Orchestration.

Container Orchestration can be used to perform a lot of tasks, some of them includes:

- Provisioning and deployment of containers among VM's. let's say i have 100 VM's and i want to deploy containers across them it is diff manually, container 
   orchestration can easily do that
- Scaling up or removing containers to spread application load evenly
- Movement of containers from one host to another if there is a shortage of resources  EX : vm1 has 2 containers.but vm1 has short resource. there is only like 50mb of ram which is available.container orchestration tool can move the container from VM1 to vm which is much more empty.
- Load balancing of service discovery between containers
- Health monitoring of containers and hosts

whenwe have COS there can also be lot of mis config's that can happen.let's say that i defined a req in cos which states that a min of 2 web containers should be running all the time.now cos can launch the 2 web containers in the same Vm although we have 3 vm's, it launches web's in one vm.here both in a same vm,1st there will be a resource issue if the prod traffic increases.2nd this is not highly available.the vm1 itself goes down then both the containers goes down.
so here we need to have a good set of req's and the cos should be intelligent in such a way that if the cos tool has a defn that the 2 webservers should be running all the time it should be intelligent to split the webserver containers across the vm's so that the load is balanced and HA would be taken care of

 ex : ecs is a orchestration tool specifically for aws. it has a feature of task placement. tasks use various constraints like az or instance type to split containers across vm's.
in AWS : we have vm1 and vm2 in same AZ.in sduch case we have web contaienr1 in vm1 and web container2 in vm2 although it might be good interms of load balencing but interms of fault tolerance it is not good.both of this im same az.we need inteligence in such a way that we server container split across the instances in a diff AZ's.if a single az goes down that should not effect ur appl.this is great capability of ecs

There are many container orchestration solutions which are available, some of the popular ones include:
- Docker Swarm
- Kubernetes
- Apache Mesos
- Elastic Container Service (AWS ECS)
There are also various container orchestration platforms available like EKS.aws takes care of managing the k8s and we take care of the rest.

DEMO : in cli :
- run " docker service ls" : i can see 1 service helloworld which is using nginx image and has a replica of 1/1. currently the mode is replicated.
- run "docker service ps helloworld" it basically shoiwing earlier it got launched in a node or vm called as swarm02 but due to some reason this VM stopped responding.and then what OS(orchestration) tool did : it started this specific container in diff vm named swarm03.it has the capability to monitor whether the appl or container is running or not.and if it is not running it will automatically started in a diff vm which is available.
- earlier we discussing : a webserver container stopped working in a vm1 then the os tool will automatically starts the web containr in diff vm.

demo : login to swarm03 container and manually stop the container which is running in swarm03 and will look into whether the container tool (swarm) whether it is able to detect this specific container has stopped working and whether it launches a new container yet again.:
- connect swarm03 where my service is running we can see status up for 3omin. now stop the docker daemon : " systemctl stop docker " . this is the worst case.it might happen that the server itself has stopped working and it has shutdown.since we stopped docker service the appl would have gone down.

- now run " docker service ps helloworld" : now after it detected the container within the swarm03 is not running what it does was it started the container in swarm01 node automatically. if i docker ps i can see docker container is running from nginx image(service)
-COS tool can automatically detect whether the container is up or not , if it is not up the it can either restart it or it can start the specific container in the diff node alltogether.

***********************************************************************************************************************************************************
Overview of Docker Swarm & Building labs : Docker Swarm is a container orchestration tool that is natively supported by Docker.
DEMo : we will build a swarm cluster. cluster will have 3 nodes(virtual machines). in these 3 vms or nodes we have to install docker and then we have to initialize docker swarm cluster. let's go and initialize 3 nodes. then we will initialize docker swarm setup.

- in aws create 3 ec2s(names-dockerswar01,02,03). login to swarm01( virtual machine). to intitialize swarm cluster we need to install docker.
- i created a simple bash script,which configure docker repo and it install docker and start the docker and enable the docker at the root.
in swarm01 : login to ec2. and run " nano docker-install.sh "(if nano doesn't work use vi)
inside docker-install paste :
" #!/bin/bash
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce
systemctl start docker
systemctl enable docker " this script for centOS

for ubuntu the script is 
"
sudo apt-get update
sudo apt-get install \
    ca-certificates \
    curl \
    gnupg \
    lsb-release
sudo mkdir -m 0755 -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin  "

- make this file executable using " chmod +x docker-install.sh"
- and run the scrip using "./docker-install.sh"  (./ means current dir)
- here we have to edit s.g of 3 ec2's for communication.also open port "2377,7946,4789"
TCP port 2377 for cluster management communications
TCP and UDP port 7946 for communication among nodes
UDP port 4789 for overlay network traffic
***********************************************************************************************************************************************************
Initializing Docker Swarm : 
- A node is an instance of the Docker engine which is going participate in swarm. in oiur case we have 3 nodes ,all of them going to part of swarm cluster.

- To deploy your application to a swarm, you submit a service definition to a manager node. means among all of these whenever we intialize a swarm one will acts as a manager node,like we basically dictate to that manager node , let's say i need 2 containers of nginx which should be up and running all the time. so we submit that  service definition to a manager node. manager node can be one among them that we can select.when we say manager node that we need 2 containers of nginx run all time.then The manager node dispatches units of work called tasks to worker nodes.
- once the manager node knows that it needs to deploy 2 containers of nginx it will dispatch that to worker nodes

IN ORDER TO INITIALIZE DOCKER SWARM :we have to run 2 commands
1. in manager node : " docker swarm init --advertise-addr<MANAGER-IP>
2. in worker nodes : " docker swarm join-token worker ".

- will select node01(swarm01) as manager node. node2,3 acts as worker nodes. before we start make sure we have 3vms(ec2's) and connectivity b/w all of these. install net-tools in all vm's.
- now make manger node: login to swarm01 . and run ifconfig. here we need to have ip address associated ith eth0. whatever ip which is public select that ip ' inet:'
- run " docker swarm init --advertise-addr ip ". ip is address associate dwith eth0. here we will get O/P :

' Swarm initialized: current node (tt1fm619rf7itypw9y09frdad) is now a manager. (current node is now manager node)

To add a worker to this swarm, run the following command:

    "docker swarm join --token SWMTKN-1-11t47bpt72b8n0jd7a8y8swin83e0qzm24gfeunn54oe4b3yke-79x47u0w0yn055vd9b3bb0s0p 172.17.0.1:2377" this command allows worker nodes to join our swarm cluster. now run this command in worker node2,3

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions '

- here in node1 run " docker node ls " it will show one node which is currently working. manager status is leader
ID                            HOSTNAME          STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
tt1fm619rf7itypw9y09frdad *   ip-172-31-33-23   Ready     Active         Leader           23.0.1

- now run " docker swarm join --token SWMTKN-1" this in node2,3. it will show " this node joined a swarm as a worker"

- now node1 is manager node. we specify the service defn to the manager node which contains how many containers we want to run all the time among the nodes and various others

***********************************************************************************************************************************************************
Services, Tasks, and Containers : 
- A service is the definition of the tasks to execute on the manager or worker nodes.
 run " docker service create --name webserver --replicas 1 nginx " . this command create a service ,name of service webserver,specified no of replicas 1 from which image our appl contaienr should launch isnginx.

- after running the above command swarm will create 1 nginx container in one of the node which are the part of the swarm. now once the container is created and due to some reason the container has stopped working,the swarm will automatically restart the container, if not it can also 9if entire node has failed and is not responding) in this case it will go ahead and create container in diff working node. so that our sewrvice is always operational.if we say replica of 1 means at an instant of time 1 container would always be up and running.
 
one more ex : we have 3 nginx replicas in service " docker service create --name webserver --replicas 3 nginx ". in servuce we basically specify amount of replicas that we want within our cluster. in this case if we say that we want 3 nginx replicas then what orcehstrator will do  it will create 3 tasks, and each task is associated with the container.(ppt.pg.no:4).we can see 1 task in each node1,2,3.and each task has container. container is like initiation of specific task.

demo : in clie we are in swarm01(masternode). run docker node ls. we can see 3 nodes running.run 'docker service ls' we can see there is no service created as of now.create a service now '  docker service create --name webserver --replicas 1 nginx '. here container launched from nginx image. since we have 1 replica. 1 task will be created.and it will show 1/1 task is running.

- run 'docker service ls' : we can see name: webserver, mode:replicated, replicas:1/1, image:nginx:latest
- run ' docker service ps webserver' we will get more info.info extra to above : node:node01(which noide service is running) ; desired state:running
- run docker ps : 1 container running and containe ris of nginx. now stop that container 'docker sto[p cointainer_id).

- run 'docker service ps webserver': it shows there was a container which was running in swarm01 and that state is now shutdown.what it did was it created a container in a diff node in this case it created a container in swarm03 node.go to swarm03 and see whether the container is launched or not.

- in swarm03(3rd ec2): run docker ps : we can see 1 container which is up and running.and is create dby docker orchestrator. here stop the docker service all together 'systemctl stop docker'we have stopped the dokcer that means whatever appl which was running over here has stopped working.
-from swarm1 node run docker ps webserver: we can see we have 3rd o/p.means (1st we shutdown docker container in swarm01,now we have shutdown the docker in swarm03 all together, now orchestrator detected that the container has stopped working and it created the container in swarm01 yet again.) if i run docker ps we can see new container which has started.

- run docker service ls we can see ' name: webserver, mode:replicated, replicas:1/1, image:nginx:latest'
- if we want to remove the service run'docker service rm webserver'. run docker service ls we can't see any service.run docker ps we can't see any container

***********************************************************************************************************************************************************
Scaling Swarm Service : 
- Once you have deployed a service to a swarm, you are ready to use the Docker CLI to scale the number of containers in the service. let's say initially i launched a service of replica 1, means we have 1 container up and running.so tmrw u know that the amnt of prod traffic cmng would be high so i want to scale up.i can sace in such a way that there can be 2 webserver container or there can be 5 webservr contrs .(Containers running in service are called “tasks.”)

DEMO(to scale up or down services) : run docker service ls 'no service'.
create service' docker service create --name webserver --replicas 1 nginx ' . run docker service ps webserver' we can see 1 task which is running in on node02(swarm02). now i want to scale the container to 5 containers : 
run " docker service scaele webserver=5 "   i specified the amnt of tasks .o/p : 'webserver scaled to 5 ,1/5..5/5 running verify:converged(created)'. one among 5 already created addition tpo that it created 4 more.

- run docker service ps webserver : i can see 5 tasks which are running in swarm1,2,3( in swarm2 2 tasks running. stop 1 task in swarm02 to verfy whether orchestrator will be able to verify and come out of it)

- in swarm02 run docker ps i can see 2 containers.stop 1 container using id or stop total docker service using 'systemctl stop docker'. our docker daemon is stopped now..froim swarm01 run docker service ps webserver again.we can see 2 tasks which are stopped(shutdown) associated with swarm02.now in swarm01 run docker ps i can see 3 tasks(containers) running

- the way we scaled up we can also scale down : run " docker service scale webserver=1" ( changed from 5 t0 1) :o/p: webserver scaled to 1 ; 1/1 running,; orcehstrator went and removed 4 containers which were running.
run docker service ps webserver we should only see 1 container up and running in swarm01

***********************************************************************************************************************************************************
MULTIPLE APPROACHES TO SCALE SWARM SERVICES : 2 ways to scale services in swarm
1.docker service scale webserver=5   ( service name=webserver we have seen above this)
2.docker service update --replicas 5 mywebserver  ( how much replicas we need (5) followed by service name). diff b/w both 1 and 2.

DEMO : in clis run dock service ls : no service . create 1 by ' docker service create --name server01 --replicas 1 nginx ' created 1 task which contain 1 container. simliar way run another service name service02 ' docker service create --name server02 --replicas 1 nginx '
- run docker service ls we can see 2 services running each service has a associated container. if i run docker ps i can see 2 containers in for each service

- we already seen scaling docker service scaele webserver=5  above.
- now we will see " docker service update --replicas 5 mywebserver "

- in this case : " docker service scale service01=2 ". it will create 2 tasks.
- run " docker service update --replicas 2 service02 " : this will create 2 tasks
- above 2 commands do the same operation. at the end we should have 4 docker containers. run docker ps : 4 containers running ( 2 contaiiners-1 srvice)

- diff b/w 1 and 2 commands :
1." docker service scale webserver=5 " in scale command  we can specify multiple services within the same command : 
    " docker service scale service01=2 service02=3" within 1 command itself i can go ahead and scale multiple services, this only possible with docker service scale command. in 2nd command(docker service update command) we can only specify 1 service in 1 command

***********************************************************************************************************************************************************
REPLICATED vs GLOBAL SERVICE in docker swarm : 
- There are two types of services deployments, replicated and global
- For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an NGINX service with two replicas, each serving the same content then we spicify "--replicas".

GLOBAL SERVICE : 
- A global service is a service that runs one task on every node.
- Each time you add a node to the swarm, the orchestrator creates a task and the scheduler assigns the task to the new node.
ex : antivirus is something that i want to run in each and every node.any time a node joins the swarm i want that antivirus container to be created in that specific node.in this scenario we create a specific container with the global service
DEMO : im in swarm01 instance.currently no service running. create a service name myreplica 
 - run " docker service create --name myreplica --replicas 1 nginx " run docker service ps myreplica ,i can see 1 container unning in swarm01. this is something which replica can do. now create globalservice :

- run " docker service create --name antivirus --mode global -dt ubuntu " here mode is global.i specified ubuntu image from which container would be launched. since we have mode of global, now docker will create this serviceof name antivirus and it will launch in each and every node.
- run ' docker service ps antivirus ' i can see antivirus is created in swarm01,2,3.if i run docker ps in swarm01, i can see 1 container which is running.
- services like splunk(monitoring tool),intrusion detection service(ids),antivirus for these to be installed in every node we can use global service 

***********************************************************************************************************************************************************
- Draining node from a swarm cluster : 
ex : run docker node ls : currently 3 nodes(swarm01,2,3) with availability of active.now as part of monthly patching lifecycle i want to restart the swarm03 server.i do something like ' yum -y update 'to install the latest patches and then i want to restart the swarm03 server. or if i want to do some kind of maintainance and i want to bring it down for few hours : in thi case it is req to drain specific node so that what ever containers that swarm might be running in this node would be switched to a diff node available here

DEMO : create 5 replicas of nginx name webservice " docker service create --name webserver --replicas 5 nginx ". run dcoker service ps : i can see 5 tasks in swarm1,2,3. among them there are 2 tasks which are running in swarm03 node. now i want to restart or put in maintanance swarm03 server for maintanance for few hours, in order to do that it is not recommended to stop the docker daemon. we can migrate all the containers which are running to a active node which we know that it is going to run for a longer period of time and that can be achieved with the help of draining.

- run " docker node update --availability drain swarm03 "  ( i specified availability,in availability i have(active drain) i give drain and specified node name swarm03.i told orchestrator that swarm03 might be going for maintanance go ahread and drain it.
- run docker service ps webserver : i can see container srunning in swarm03 got shutdown, and new containers have been brought up in a alternative node. so any time if we increase the avaialability of containers , in this we increased the availability of containers to 5,so let's say we moving to 10.even if we move it to 10, docker orchestrator will not create the new task within a node which is marked as drain.it will onlyu move it to nodes which availability is active. run docker node ls : we can see swarm03 availability is drain, for swarm1,2, is active.

- not only docker orchestrator has removed the tasks which are running in swarm03,but also anytime if we create new services it will not be deployed in swarm03. we can bring the node swarm03 from draining to active by changing availability to active
- run " docker node update --availability active swarm03 " . run docker node ls now we can see swarm03 availability is active

***********************************************************************************************************************************************************
INSPECTING SWARM SERVICE AND NODES : 

- Docker inspect provides detailed info of the docker objects : 
- docker objects : docker containers,docker network,docker volumes,docker swarm services, docker swarm nodes
- Docker inspect related to swarm services, swarm nodes :

- in swarm01: run docker service ls i can see 2 services which are running.in one of the service01 0/3 replicas running, in service02 3/3 replicas running.
 run docker service ps service01, here current state of task are ' in pending'. i want to see what are trhe constraints that might want to see on what arethe containers constraints that might had been applied. that can see with docker inspect command.

 -run " docker service inspect service01 " : we will get lot of info related to service, we can't understand , it is very long.to make it short run --pretty
 run " docker service inspect service01 --pretty ". it will give info in easy way.
 in info :we can see 1 placement :constraints:  [node.labels.region==1mumbai]  ( here 1mumbai becuase this all task going in pending state)

- DOCKER NODE : run " docker node inspect node_id --pretty ". we will get detailed info related to node like(hostname,OS,joined at,)

***********************************************************************************************************************************************************
- ADDING NETWORK AND PUBLISHING PORTS TO SWARM TASKS : 

( how we publish ports for the task that have been sent to the worker nodes via swarm ) 
- generally when we start a container in bridge network we specify the --p(pubish port) and we speicy port mapping from host to container. now how we achieve this in swarm.
- run " docker service create --name webserver --replicas 2 nginx ". run docker service ls we can see service running .and 2 tasks in swarm1,3.
- run docker ps in swarm01.i can see container running on port 80/tcp. however this port 80 is not going to get bind on host level. so if someoe from internet wants to access website which is part of nginx container they will not able to do it. if we run (curl 127.0.0.1 : connection refused)
- so it is necessary whatever service that we going to create like webservice we have to map port correctly. so now remove the service webserver using rm.

- creta a service with publishing port as well : " docker service create --name mywebserver --replicas 2 --publish 8080:80 nginx " run docker service ls 2/2 replicas has been done. 2 taks in swarm01,03. run ps in swarm03. we can see 1 container.run " netstat -ntlp " we can see dpcker on port 8080 .
- to verify run " ifconfig on eth0 " cpoy the inet address. and run curl inet_ipaddress:8080. now i can see html page of nginx

***********************************************************************************************************************************************************
DOCKER COMPOSE : 
- Docker Compose is a tool for defining and running multi-container Docker applications. like till now we are running single container appl. so in prod an appl that require 2 containers or 3 containers. to achive docker compose is a tool which is extensively used.
- With Compose, we define the req's that we need in a  YAML file to configure our application’s services. once we define req's We can start all the services with a single command - docker compose up.We can stop all the services with a single command - docker compose down.

DEMo : in windows docker compose comes default.in linux we have to install docker compose.in linux we can install using git or pip install docker-compose.
- in ec2 create a folder and inside that create a file. run " mkdir dockercompose " . nano docker-compose.yml ". 
- within the yml file we can define what are the services,appl's that we want to run. code : 
 " 
  version : '3'
  services :
     webserver :
        image : nginx     in this simliar way we define one more tier called as database.and we ill make use of image redis
     database :
        image : redis   ". now run docker-compose config : O/P :

root@ip-172-31-38-252:~/dockercompose# docker-compose config
services:
  database:
    image: redis
  webserver:
    image: nginx
version: '3'    : it basically saying that there is a webserver and it should start from image nginx.and there is Db and it should start from image redis.
- if we want to start our appl we can run " docker-compose up -d". -d for detatched mode. now run it.
we will get : Status: Downloaded newer image for redis:latest
Creating dockercompose_webserver_1 ... done
Creating dockercompose_database_1  ... done    " like this. now run docker ps we can see 2 containers running of nginx,redis

- if we want to stop our appl run " docker-compose down " . now run docker ps no containers running.

- run " 
  version : '3'
  services :
     webserver :
        image : nginx     
        ports :
          - "8080:80"
     database :
        image : redis . run docker compose config and run docker compose up -d. and run netstat -ntlp we can see docker running on port 8080.
- run curl 127.0.0.1:8080 : we can see wlecome to nginx html page.

- we have 1 more docker-compose.yml file :

version: '3.3'
services:
   db:
     image: mysql:5.7
     volumes:
       - db_data:/var/lib/mysql
     restart: always
     environment:                          ( env var's related to db)
       MYSQL_ROOT_PASSWORD: somewordpress
       MYSQL_DATABASE: wordpress
       MYSQL_USER: wordpress
       MYSQL_PASSWORD: wordpress
 
   wordpress:
     depends_on:
       - db       ( it depends on db. so first db will install then wordpress will install)
     image: wordpress:latest
     ports:
       - "8000:80"
     restart: always   ( restart policy)
     environment:
       WORDPRESS_DB_HOST: db:3306
       WORDPRESS_DB_USER: wordpress
       WORDPRESS_DB_PASSWORD: wordpress
       WORDPRESS_DB_NAME: wordpress
volumes:
    db_data: {}   "


- now i can see 2 containers up and running. and run ifconfig eth0(to get public address of VM) and run public_ip:8080 in browser.we can see wordpress running.

***********************************************************************************************************************************************************
DEPLOYING MULTI-SERVICE APPS(APPL'S) IN SWARM : 
- in docker-compose we understood that an appl will contain multiple backend services which are required.( wordpress require webserver,it req actual appl itself,it also require db.) with docker compose we can go and deploy the containers within the same host. however if we want to deploy within the swarm cluster : 
- A specific web-application might have multiple containers that are required as part of the build process.Whenever we make use of docker service in swarm,it was a single container image.
- incase if we have multiple dependency containers (ex: wordpress) we have to deploy in docker swarm.then we can make use of " docker stack DOCKER STACK"
- docker stack command will take the yaml which we written in docker compose and it can deploy in docker swarm env. The docker stack can be used to manage a multi-service application.

- A stack is a group of interrelated services that share dependencies and can be orchestrated and scaled together.
- A stack can compose a YAML file like the one that we define during Docker Compose.
- whatever things that we need we can define everything within the YAML file and we can deploy it with the help of docker service

DEMO : in swarm01 run ls. we can see docker-compose.yml. run " docker-compose up -d " it will run also give warning " the docker engine that we are running in swarm mode. and compose doesn't use swarm mode to deploy services to multiple nodes in a swarm.all containers will be scheduled on current node" means whatever contaiers or service sthat we have defned within docker compose wit will only only part of this VM. it will not go to nodes within swarm.

 it also shwos " to deploy oiur appl across the swarm , use " docker stack deploy " "
- im in swarm01 which is leader node.run ls we can see docker-compose.yml . run docker node ls we can see 3 nodes. to deploy services which are part of docker-compose.yml across swarm cluster run " docker stack deploy --compose-file docker-compose.yml mydemo" . now it will create services 
which are part of  compose.yml.

- run " docker stack ps mydemo " ( stack name is mydemo ) . we can see one of the containers is running in swarm02 and anotehr in swarm01.
- depending upon req we can change the yaml file and deploy it with the help of docker stack.
- to remo " docker stack rm mydemo ". it will remove all the containers as part of docker stack.

***********************************************************************************************************************************************************
LOCKING SWARM CLUSTER : 
Swarm Cluster contains a lot of sensitive information, some of which includes:
- TLS key used to encrypt communication among swarm node
- Keys used to encrypt and decrypt the Raft logs on disk
 if above keys are unencrypted and if our swarm compromise due to some reason, the attacker will be able to caught all keys in unencrypted form,and they will get lot of sensitive info.  Docker Lock allows us to have control over the keys.
DEMO : 
- in swarm01(leader) run node ls i can see 3 nodes(swarm1,2,3). go to " cd /var/lib/docker " i can see many dir's. go to " cd swarm " i can see many dir's inside that. run " cd certificates " and run ls -l . i can see swarm_node.cert, swarm_node.key,swarm-root-ca.crt.
- if i run cat swarm_node.key i can see value of it. if attacker will get cert and this key he will be able to compromise commun, and raft encrypted logs.
- to avoid this docker suggests docker provides lock feature for swarm cluster.

- run " docker swarm update --help " we can see many options. " --autolock " is one and it have options(true,false). if autolock has true, the next time when we restart ur swarm , the swarm will basically ask for pswrd whenever we want to perform some oprn.

- run " docker swarm update --autolock=true" : it says ' to unlock a swarm manager after it starts , run docker swarm unlock and we have to pass the key we got at this time. now stop and start the docker using systemctl stop and start docker.
- now run docker node ls it will give error saying that error from daemon.swarm is encrypted and need sto be unlocked before it can be used using docker swarm unlock.
- run docker swarm unlock key ( the key we got while locking swarm). now after unlocking swarm if we want to retrieve(see) the key we have to run " docker swarm unlock-key"
- we can alos rotate key using " docker swarm unlock-key --rotate ". i will get another key and copy it.

***********************************************************************************************************************************************************
TROUBLESHOOTING SWARM SERVICE DEPLOYMENTS : 
- A service may be configured in such a way that no node currently in the swarm can run its tasks. In this case, the service remains in state pending
- There are multiple reasons why service might go into a pending state. run docker service ps demotroubleshoot " here we can see the desired state is running. but current status is pending.that means the specifc service is not able to deploy task.task went to pending.

- multiple reasons why service might go into a pending state : If all nodes are drained, and you create a service, it is pending until a node becomes available.
- You can reserve a specific amount of memory or cpu for a service. If no node in the swarm has the required amount of memory,cpu, the service remains in a pending state until a node is available which can run its tasks
- You have imposed some kind of placement constraints

DEM: run service ls.1 service demotroubleshoot. run docker service ps demotrou. cyrrent state pending. to check why pending state run :
 " docker inspect task_id(one of the task id) " in a lot info check " status : status pending;err: no suitabkle node "(scheduling constraints not satisfied on 3 nodes ". 
- here in my case i created a service,and name demotrroi , and given --constraint node.labels.region==1mumbai --replicas nginx ". here the replicas will inly run which has labels.regin=mumbai. in current nodes, none of the nodes has "--region=1mumbai". this is why all the service tasks associated went into pending state.

***********************************************************************************************************************************************************
MOUNTING VOLUMES VIA SWARM : 	how we can mount volumes in container through swarm.
run " docker service create --name myservice --mount type=volume,source=myvolume,target=/mypath nginx "  it specifying --mount and type is volume, source, and target is /mypath(root) and launching nginx image.

- run above command it create taks. run docker service ps ,w ecan see task in swar02. run docker ps we can see a container. and also run
 " docker volume ls " we can see ' driver(local)=myvolume(volume_name) ' this vol mounted on /mypath to verify login to container.
- " docker container exec -it contaienr_id bash " inside run ls -l . i can see a dir of mypath. this is basically mount which happened inside counter.  
- run cd mypath. create file (touch test.txt) then exit.
- now run docker volumes ls . i cansee 1 vol.
- run cd var/lib/docker/volumes/. run ls. go to myvolume. run ls . run cd _data. run ls. we can see test.txt.

- let's say the container got terminated.all the data that we have , if we are storing in mypath dir , we still able to see it irrespective of container lifecycle.
- run ocker service ls. i can see 1 swrvice. now remove it.

Q : we have volume.the volume create dwith the swarm.what happens if the service itself gets deleted.will the volume still persists or not ? 
 currently the service is running in swarm02. now remove it and run docker ps. we don't see anything. and run docker volume ls. we can see the volume still.

***********************************************************************************************************************************************************
CONTROL SERVICE PLACEMENT : 
- in aws ecs we have seen how ecs will place tasks based on AZ or instance type.similiarly docker swarm also has a capability to push task or containers worker nodes depending upon the specific constraints.
The constraints :
- Swarm services provide a few different ways for you to control the scale and placement of services on different nodes
below factors that we can use to puch containers on worker nodes : 

- Replicated and Global Services    ( if we do replica we have to give replica count, for global we launch the task in all the nodes which are available as 
  part of swarm
- Resource Constraints [requirement of CPU and Memory]  - container will only be launched on specific nodes based on CPU,memory
- containers whould only run based on Placement Constraints [only run on nodes with label pci_compliance = true]
- Placement Preferences

EX : PLACEMENT BASED ON LABELS :- containers whould only run based on Placement Constraints [only run on nodes with label pci_compliance = true]
demo : run node ls.i have 3 nods. docker allows us to label each and every node which are part and depending upon label we can have a placement.
- imagine swarm02 based on mumbai.swarm03=banglre. the new service we are deploying we want only to the loads which are based on banglre region.

- run " docker service create --name myconstraint --constraint node.labels.region==blr --replicas 3 nginx "  this service will launch the task in node where the label has the key of region and value of banglr. our services deployed now run " dcoker service ps myconstraint " we can see all the 3 tasks or containers have been launche din swarm02 node.
- how to see a label which is associated with a node. " docker node inspect node_id ". we can see labels inside the info.
- remove service now.nonservices running.

- add label to node03 : in swarm01 run " docker node update --label-add region=mumbai swarm03 or node_id"
- run docker node ispect node_id(03). in labels we can see region mumbai.

- now run the same service using mumbai region labels 
   " docker service create --name myconstraint --constraint node.labels.region==mumbai --replicas 3 nginx " here (node.labels) is common.

***********************************************************************************************************************************************************
OVERVIEW OF OVERLAY NETWORKS : The overlay network driver allows us to have a distributed network among multiple Docker daemon hosts.
- Overlay network allows containers connected to it to communicate securely.
EX : a swarm cluster has 3nodes. we have 3 containers(webserver) in 3 diff nodes. each node(VM) hasit's own docker daemon.

- if webserver container in node01 wants to communicate with node2,3. this doesn't posible with host or bridge network. to make communication we need to make use of overlay networks. whenever we initiate swarm , we find overlay network driver has been initialized.
- in swarm01 : run network ls. we can see a driver named overlay ,scope:swarm, name:ingress.

 now run docker network inspect overlay : we get peers details. 3 pairs.each has diff ip's. like we have 3 containers which are running in each of nodes.
 -in node01 run docker ps.and run docker contaienr inspect container_name . we will see the contaienr has ip address ,it also shows is in overlay network driver. similiarly for node02,03 we will get each container ip address. now login to node01 container . and try to ping the node2,node03 containers using the contaienr ip addresses. we are able to ping that container.
- finally we tried to ping from node01 container to contaienrs in node02,03

***********************************************************************************************************************************************************
CREATING CUSTOM OVERLAY NETWORKS FOR SWARM : 
- create a network driver of type overlay and give name mynetwork : " docker netwrok create --driver overlay mynetwork " 
- create a service name myoverlay and give network mynetwork and 3 reolicas " docker service create --name myoverlay --network mynetwork --replicas 3 nginx"
- here we created mynetwork only in swarm01. but now we define it with service.it will spread to worker ndoes with service.

- after creating service ,we can see tasks in 3 diff nodes. and run network ls in every node. we can see our ,mynetwork in that nodes.
- find ip addresses of containers usng docker netwpork inspect container_name in node1,2. now login to container in swarm03 and try to ping other containers in node1,2 using the ip addresses. successful. before to it install iputils-ping using apt-get in every swarm.
- when we remove service. the overly network(mynetwork)will be removed from the node2,3.

***********************************************************************************************************************************************************
Securing Overlay Networks : 
- For the overlay networks, the containers can be spread across multiple servers.
If the containers are communicating with each other, it is recommended to secure the communication

- like i have node1,2,3 and container 1,2,3 in each node.container1, container2 can communicate each other in overlay network. the problem is security. in b/w node1 and node2 is a network.if there is any attacker in b/w network, he will get some sensetive info. this is why security is imp in overlay network.
- here b/w 2 conatiners in 2 serevers, the traffic will be sent in netrwork or over interet.
- To enable encryption, when you create an overlay network pass the --opt encrypted flag:
- " docker network create --opt encrypted --driver overlay my-overlay-secure-network "

Points :
- When you enable overlay encryption, Docker creates IPSEC tunnels between all the nodes where tasks are scheduled for services attached to the overlay network.
- These tunnels also use the AES algorithm in GCM mode and manager nodes automatically rotate the keys every 12 hours.
- Overlay network encryption is not supported on Windows. If a Windows node attempts to connect to an encrypted overlay network, no error is detected but the node will not be able to communicate

***********************************************************************************************************************************************************
CREATING SWARM SERVICES USING TEMPLATES : 
- We can make use of templates while running the service create command in the swarm.
ex :
 - " docker service create --name nginxservice --hostname="{{.Node.Hostname}}-{{.Service.Name}}" nginx " ( here the service is not yet created. name of service will be kept in this place holder {{.Service.Name}} .and name of node where it is been created will place in {{.Node.Hostname}}. the host name which is given(--hostname) to contaienr would be the combination of the hostname of the node where the service is created and the name of service
- run above command. it will create 1 task. and run ps we cxan see 1 container. the host name of this container depends on the value that we define above.

- run hostname .o/p: mydemo . ( hostname of VM is mydemo)
- {{.Node.Hostname}}-{{.Service.Name}}" this will have values (mydemo nginxservice ). to check it login to contaienr using bash. inside run hostname we will get value " mydemo-nginxservice"
- usecase : i want to deploy a container in random server. i want to have a proper hostname . in that case we will use this approach.
- There are three supported flags for the placeholder templates:--hostname,--mount,--env. for this flags we can use diff placeholders( check ppt.not mandat)

Hostname : A host name is used when a web client makes an HTTP request to a host. The user making the request can specify the IP address of the server rather than the host name, but that is now unusual on the Internet. Host names are more convenient for users than numeric IP addresses
- Host Name: The unique identifier that serves as name of your computer or server can be as long as 255 characters and consists of numbers and letters.

***********************************************************************************************************************************************************
Split Brain & Importance of Quorum :

Split Brain problem : 
EX: i have cluster and it has 2 vm's. 1st vm is master. 2 vms have webserver1,2 running. cluster has centralaized dick space. master node is responsible for storing and accessing centralaise disk.vm1 is master and it accsess disk. due to some reason the connectivity b/w webserver1 and 2 got stopped, like network stopped. but both servers up and running.but theya re not able to communicate each other. the passive webserver2(on right side) will assume webserver1 went 

down. then webserver2 will assume itself as master and it will connect to centralaise disk. now problem is both of them is master,and both of them trying to communicate with centralaise disk space.now chances of data corruptions are extremly high.this problem is referred as split brain problem. to go away with this problem there is a oncept of QUORUM : 

- Quorum is nothing but how many servers in cluster.curently i haev 2 wervers thats why i have spit problem.if i have a Quorum of 3 means 3 servers.
ex : i have 3 vm's and 3 webservers. 1st vm is master and is writing to centralaise storage.in Quorum type arch voting is imp.like each vm has 1 vote.
- now webserver1 is master has the connectivity issue and is not able to communicate with vm2 and vm3. now websewrver1(vm1) will determine how many votes it has. here vm1 will have 1 votes because it has its own vote. it is not able to communicate with vm1,2.

- now vm2,3 has 2 votes because they has both vm1,vm2 votes and they are communicateing each other. trhe right side vm2,3 will win. and vm1 will diconnect from centraialise disk.
- there will be algorithm which will chose b/w vm2,3 as master.now vm3 is master. it will connect with centralaise disk.
- once connectivity will back b/w vm1 to vm2,3 .we know vm3 has become master due tp voting.now vm1 will step down and acts as achild node.
for imp pointers check ppt.

***********************************************************************************************************************************************************
Swarm Manager Node HA :
- Manager Nodes are responsible for handling the cluster management tasks.till now we created 1 manager node and multiple worker nodes.the problem here is if manager goes down all of the cluster oprns will not work. ex : i have 1 master and 2 workers. now master is down we won't be able to perform any swarm related service with the cluster.this is why multiple manager nodes for HA.
 
Manager Node has many responsibilities within the swarm, these include:
-  Maintaining the cluster state
-  Scheduling services
-  Serving swarm mode HTTP API endpoints
Using a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it
- in page 11 we can see we have internal distributed state store achieved with help of raft implementation

Swarm comes with its own fault-tolerance features.
- Docker recommends you implement an odd number of nodes according to your organization’s high-availability requirements.
- Using a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it
- An N manager cluster tolerates the loss of at most (N-1)/2 managers. ex : n=3. 3-1/2=1. if we have total of 3 managers then it can tolerate a loss of upto 1 manager node. in quorum table in ppt is derivedfrom the n-1/2 formulae.

- same in qourum if we have 3 cluster nodes. we can tolerate upto a loss of 1 node.ex : n=3. 3-1/2=1. 
  in ebven clustewr nodes = 4-1/2 = 1.5.look for lowest one. then in 1.5 1 is lowest. 4 cluster node can tolerate a loss of 1 node.

DEMO : i have 3 VM's names Manager01,2,3. in manager 1 run docker swarm init public_ip. it will generate a token for worker nodes to join this swarm.here manager 1 is master. now i want to add manager nodes with token not worker nodes. for that when we run "docker swarm init public_ip" we will get : 
 To add a worker to this swarm, run the following command:

    "docker swarm join --token SWMTKN-1- " for worker nodes
  To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions ' for manager nodes

- docker swarm join-token manager run this in manager1. it will given token for manger nodes. run this in manager2. it will say this node joined swarm as a manager. now we hav 2 nodes(split brain problem ). now stop manager1 using systemctl stop docker. now run docker ls. it will show swarm doesn't have a leader. now start the docker in manager1. add manager3 also as manager node using manager token.
- run node ls.we have 3 nodes. among them 1 is acting as a leader.and 2 others are reachable.
- now stop the leader(manager1-swarm01). now run node ls in swarm03.we can see node1 is unreachable.and swarm03 has a status of leader.

***********************************************************************************************************************************************************
Running Manager-Only nodes in Swarm :
-when we launch a service in swarm  by default even manager node also perform some tasks related to the service(means manager node also acts as a worker node. we cansee task has been created in the manager node as well in default approach). this is not recommended service. manager node should only perform management operations.
- if we want to make manager node acts as manger node only and not as worker node : 
- now run "docker node update --availability drain node_id" this willdrain the manager node which we created in default approach. run docker service ps we can see all contauners went away in manger node. 
- now run docker service ps service_name : we can see all tasks running in worker nodes only.
- manager node itself has lot of oprns, so better not to involve it in working nodes. that's why always drain manager node. so that task really doesn't get created.

***********************************************************************************************************************************************************
Recover from losing the quorum :
how we can recover from a lost quorum : 
- there are 3 managers in a cluster. we know for a cluster size of 3, we accept a min of 1 member loss. we look what if 2 members lost,or total quorum lost.
DEMO : i have 2 manager nodes in cluster and 1 worker node. according to qorum we can loss a single server out of 3 server . if we loss 2 the quorum will lost. run node ls 2 manger nodes (1 reachable,1 active),and 1 worker node.

- create a service name webserver replicas 3. before this drain both manager nodes" docker node update --avaialability drain node_id " . now reatre service. docker service ps we can see all tasks in worker node. since we have a quorum of size 2 . what happens if quorum is lost. stop the currewnt master node using systemctl. it will show error when we run node ls sayinmg that swarm doesn't has leader. how to recover from it. but containers in worker node will continue to work properlly. but if we want to scale up or down we can't do that . becays ethe swarm state is not sufficient
to overcome this problem docker recommends to initialise the swarm cluster in running node.

-run this mangeer2" docker swarm init --force-new-cluster --advertise-addr ip(of eth0) " this initisalize ne wcluster.
- run d=service ps wevserver : it will show 3/3 tasks in worker node. this happens because manager nodes has internal distributed state store from raft.manager2 also has this specific data.this is why it was able to recover

***********************************************************************************************************************************************************
DOCKER SYSTEM COMMANDS : 
- imp to know docker system child commands.
- run docker info or docker system info we will get same o/p.
- " docker system events : it will give real time events from the server. events can be generated for certain aspects like images,containers,volumes,networks.

- i have few containers.now stop any 1 container .now run docker system events : we can see events related to kill,die,kill, network disconnect,container stop. let's say we have log monitoring systems like splunk etc. we can monitor these events and any time a container gets stopped we can create an alert.
- docker system df :
docker system df
TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE
Images          0         0         0B        0B
Containers      0         0         0B        0B
Local Volumes   1         0         231.7MB   231.7MB (100%)
Build Cache     25        0         17.39MB   17.39MB








***********************************************************************************************************************************************************
 
KUBERNETES :

***********************************************************************************************************************************************************
Introduction to Kubernetes : 
- we have docker swarm, apache mesos, k8s and other orchestration tools. By far Kubernetes is the most popular container orchestration engine.
- Kubernetes (K8s) is an open-source container orchestration engine developed by Google.
- It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.

ARCHITECTURE OF K8S: 
- we have k8s master and worker nodes(physical servers or VM's where the containers would be running). k8s master is the one who will be managing the container. K8S master will basically recieve the commands from the users. how will it recieve commands (it can recieve commands via API,CLI,GUI : just like we run commands in docker swarm master node to create services). then k8s master will process the things which are mentioned within the commands.

EX : i have command "i want to launch app A " we will give this command to k8s master . now k8s master will check how many nodes it has.there are 3 nodes which are associated with k8s master. it will launch the app(container) in one of the nodes. various algorithms based on which specific node is selected. it's not like that Node A is currently having 100% CPU load, in that case k8s master will not really send the appl to node A.it will send to node which has most resoirces available. k8s master will mange launching the container and managing the container in longer term.

- now due to some reason worker node A got stopped working.there might be power issue, network connectivity or kernel issue and various others.once the worker node got stopped working K8s master will autpomatically detect it. it migrate the container to diff running worker node. this also referred as record of intent. record of intent is given bu user which is "i want to launch app A ".

- k8s master not only launch container once it also performs health check.master not only  checks that the node is down it can also detect if the app is not working properly.if app container is not working properly it will restart the container, it automatically migrate the container to diff worker node.
DEMO : k8s master is 1. it can have any no of nodes from 1 to 100. in our case we have 1 worker node . run " kubectl get nodes " . it will show 1 node.here my record of intent is to launch app A container. run " kubectl run myapp --image=nginx ". run " kubectl get pods ". i can see 1 container(pod) runnig.

- here we told k8s to launch a contaienr from nginx image.this was supplied to k8s master. k8s master will look into worker nodes and it will launch that specific pod in one of the worker nodes.refer pod as a container. here container is running in cli where we are passing commands. run docker ps | grep pod_id ' i will get container ' . now stop the container " docker stop container-id". 

- now k8s will detect the container that is not working. then the k8s will automatically start a new container in a diff node. now run docker ps we can see a new container got launched.
- we already seen 3 way to supply commands to k8s (api,cli,gui). in the demo we ran kubectl command we used CLI.

***********************************************************************************************************************************************************
INSTALLATION OPTIONS FOR K8S : 
1.Use the managed K8S service generally offered by cloud providers
2.use Minikube in local system setup
3. Install and config k8s manually( Hard way )

1.Use the managed K8S service generally offered by cloud providers : Various cloud providers like AWS,IBM,GCP and others provided managed K8s clusters. in managed k8s we can run commands, here providers manage k8s clusters

2.use Minikube in local system setup : minikube is a tool that makes it easy to run k8s locally. Minikube runs a single node k8s cluster inside a VM on your laptop for users looking to try out k8s or develop.

3. Install and config k8s manually( Hard way ) : in this we install and config all k8s components individuallly.

Things to config while working with k8s : 
KubeCtl: CLI for running user commands against cluster.(in AWS just like AwsCLI)
K8S master : K8s cluster itself.
Worker node agents : k8s node agent ( agents that sits in worker nodes	.we have various agents like kubelet, kubeproxy we need to configure in every  agent)

Components to be configured in (cli,gui,api) : 
CLI :- we have to first configure k8s master. it contains multiple components. once i have master and worker nodes i have to configure CLi(kubectl).we have to send commands to master using cli.

- In managed service: we have to download Kubectl cli only. managed provides take care of master , worker nodes, and also agents in nodes
- Minikube: minikube take care of master and worker node( which is basically same workstation where master is configured), and we have to configure kubectl.
 disadv: with minikube it only provisons single node cluster

***********************************************************************************************************************************************************
USING MANAGED K8S SERVICE FROM CLOUD SERVICE PROVIDER : 
- 3 components in k8s : CLI( will install in laptop) , k8s master is referred to as control plane ( in digital ocean k8s cluster is free. In aws k8s cluster costs 0.10USD per hour.). Worker Node Agent( where our appl will be running. worker node agent is charged in aws, and digital ocean)

DEMO : in digital ocean give worker node name, region, cluster name , select no of worker nodes 1.
- once k8s cluster is up and running , in order to schedule ur appl or create various resources , the 1st thing we need is client(CLI). the client will able to communicate with k8s master. it's not like client will communicate with k8s master without , master and pswrd. this is why once the cluster is up and running we can find Cluster access config file( this contains address of k8s cluster,credentials). config file is yaml file.

- in yaml fiule we can see address of where k8s server is running. when ever we want to perform some activity we will send req to this server. we know if we just send req without credentials that req will be rejected. that's why we are also provided a token. consider this token as pswrd for k8s cluster.	

***********************************************************************************************************************************************************
OVERVIEW OF KUBECTL : 
 Install kubectl : 
- k8s command line tool which also referred to as Kubectl, it allows us to run commands against k8s cluster. let's k8scluster running.i want to create resources like pods, any other resources for that we need a command line(kubectl). through kubectl we go ahead and send istructions to k8s cluster based on our instructins, the resources will be created or deleted. in order for kubectl to connect to cluster it needs various info like DNS/Ip of cluster, authentication cred's luike username,pswrd. this is why we have one more file called kubeconfig file. it contains info of token userame,dns etc.
 path : kubeconfig resource file - Kubectl binaries - k8s master 

- to get kubectl for windows : download kubectl binary with curl on Windows. put the .exe file in a folder. and in powershell run kubectl . we can see list of commands
- run " kubectl get nodes " it will show :Unable to connect to the server. beacuse here kubectl dowsn't know the dns associated with k8s master service. and we know we can find all this info in kubeconfig file. copy the kubeconfig file that we downloaded from digital ocean k8s cluster and put it in  installation folder, in kubectl.exe location.
- now run kubctl.exe file and associate kubeconfig file. run ' kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes ' 
 this thrown an error " Unable to connect to the server: x509: certificate signed by unknown authority ".

IN LINUX :
 in ubuntu install kubectl binary for linux using " curl -Lo "https.." ". then make kubectls as executable file using " chmod +x ./kubectl ". now run kubectl file and associate kubeconfig file.
- run  kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port? it says connection was refused and it assumed cluster is in localhost. since k8s cluster in digital ocean.we need to fetch the config file that we fetched from digitalocean.

- so copy the config file to kubectl.exe location and there run : " kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes " we will get o/p :
 kubectl --kubeconfig "k8s-cluster-kubeconfig.yaml" get nodes
NAME                STATUS   ROLES    AGE   VERSION
worker-node-qhdi5   Ready    <none>   17h   v1.26.3

- here everytime we run kubectl we have to specify kubeconfig parameter --kubeconfig "k8s-cluster-kubeconfig.yaml" to get nodes. to avoid that we can make use of easy way. inside working directory create a file " mkdir .kube " . and there run " cp k8s-cluster-kubeconfig.yaml ~/.kube/config ".now go to"~/.kube"
 and run ls we can see cache and config. there run " cat config " . we can see all the contents of kubeconfig.yaml.
- go to location where kubectl.exe present and run " kubectl get nodes " we can see list of nodes.

***********************************************************************************************************************************************************
UNDERSTANDING PODS : 
- in docker if we want to run a docker container " docker run --name webserver nginx "
- in kubectl if we want to run a container " kubectl run webeserver --image=nginx " 
If we want to create a container in the available worker nodes in kubectl. "kubectl run webserver --image=nginx" 'pod/container created' in kubectl  is referred to as POD. run "kubectl get pods " we can see 1 pod named webserver is running
NAME        READY   STATUS    RESTARTS   AGE
webserver   1/1     Running   0          72s

If we want to connect inside a container:
in docker run " docker container exec -it container_name bash"
in Kubernetes : " kubectl exec -it webserver -- bash "

- " webserver -- bash " here -- seperates the argument that we want to run as part of container(pod). ex : i want to run "ls -l / " inside pod then :
  " kubectl exec -it webserver -- ls -l / "

- In docker if we want to remove docker container then we stop it and we remove it . EX: docker stop container_name , docker rm container_name.
- In K8s if we want to remove pods : then run " kubectl delete pod webserver " 

POD : A Pod in Kubernetes represents a group of one or more application containers and some shared resources for those containers.
EX : in pod 1 i have 1 container and it has it's(container's) ip address
     in pod 2 i have 1 container and storage volume and ip address
     in pod 3 i have multiple containers  and 1 storage volume
     in pod 4 i have multiple containers and multiple storage volumes
- there can not only a single container in pod, also has multiple containers that are part of a single pod.

Benefits of PODS : Many appl's might have more than one container which are tightly coupled and in one to one relationship.
 EX : i have 1 appl and it requires 2 containers to be up and running.1st container name is funxtion01, function02 : 2 containers	

- here " docker run -dt --name myweb01 function01 " ,same for function02. we are running a containers from function01,02. only after both the containers are up and running then only the appl will work. if one container goes down at the end the appl will go down.
 in this approach we are managng at container level. let's say we have fxn01,02 thiingsd are working fine. tmrw good amnt of load is happening.we need to know 1 to one relationship here. we need to have a sheet which states that fxn01 is dependent on fxn02 and so on. 

- One of the ways to deal with tightly coupled appl in k8s is to make use of PODS.
- in pod i have fxn01 and fxn02.we will go and create a pod , pod intern will go ahead and create multiple containers here.we don't have to create individual containers just like in docker. here things become simpler at pod level. 

EX : i have pod1 and inside it fx1 and fxn2 working fine. tmrw more load will come and things will slow down. so we can go ahead and create one more pod this pod will intern create a containers of fxn1,fxn2 that's it. we don't need to know about 1 to 1 relationship everithing k8s will deal in background.

- if something is not really working well like fxn2 is not working well in pod1 ,k8s can go ahead and automatically create 1 more pod so that our appl can go ahead and  remain available all the time.let's say 2 pods that are serving the prod traffic,however the load is increasing k8s can create more pods that can recieve the prod traffic so that the load is evenly distroibuted. just like if we work with ASG feature in aws k8s is similiar to that.

- A Pod always runs on a Node.( ex run kubectl get nodes : we have 1 node. all the pods that we are creating , they will create in this worker node)
- A Node is a worker machine within Kubernetes env.
- Each Node is managed by the Master.( we have k8s master which will manage worker nodes)
- A Node can have multiple pods( we can even create 100 pods)

***********************************************************************************************************************************************************
KUBERNETES OBJECTS :
- in earlier videos we discussed " record of intent" it is basically what we tell to k8s master and master will make sure whatever  we have informed will always be running all the time. thiis is what record of intent means. " i want to launch App A " was record of intent.k8s master will recieve and it willlaunch in nodes. master will make sure app A will always be running.if A doesn't work then Matster will move that app A to another worker node.
- Here technically record of intent is refrred to as K8S OBJECTS.

- Kubernetes Objects is basically a record of intent that you pass on to the Kubernetes cluster.
- Once you create the object, the Kubernetes system will constantly work to ensure that object exists.
- this object can be pod , it can be a namespace,itcan be a deployments etc.in k8s we can do apart from just creating a pod.those aspects are referred to as k8s objects.
There are various ways in which we can configure a Kubernetes Object.

- The first approach is through the kubectl commands. we ran kubectl run command to create a pod. that pod is referred to as object
- The second approach is through a configuration file written in YAML.( here we can create things via YAML file.we have config file written in YAML, and that config file can also be used to create K8S object. in the given yaml file (pg.no:14). in yaml file it states that it creates a pod(kind:pod means k8s should create a pod, nameof it is webserever, and it will run from image nginx. )
- save the yaml file as (pod.yaml) in one folder.if i want to create the things which are mentioned in yaml file the run " kubectl apply -f pod.yaml" (-f stands for file) in the o/p : pod/webserver created. run kubectl get pods we can see 1 pod which is running.

- YAML :
- YAML is a human-readable data-serialization language. It designed to be human friendly and works perfectly with other programming languages. there are other products which actively uses YAML.Ansible is one of the famous configuration management tools whic uses yaml.we also have kubernetes which also uses yaml.Even Cloudformation which uses yaml files.
- XML and json are a bit of pain if we write it from scratch.machine specific purposes is quite good for machines. However if we want to write things from scartch yaml ois good.
- This is why YAML is one of the supported language for writing the K8s Objects.

BENEFITS OF CONFIGURATION FILES : 
- it integrate well with change review process.in many org's we  go through a change review process, where before we apply a change we have to get it review from the peers.

- It provides the source of record on what is live within K8S cluster.: let's say there are 5 people who are managing the k8s cluster and everyone is using kubectl command to perform various write operations . if we want to look into what are the changes that are happening tdy ,ystrday,or changes thatare lying at an instant of time it is quite chalklenging to do that.if we have everything within the config file a new joine can easilyread the comfig file and he can understand on what are the things which are lying.
- it is easier to troubleshoot changes within version control. since this is config file we can even version control it, we can find out the changes applied

EX : i have pod.yaml file which has basically referenced.now some one ran this pod.yaml file and the system broke. i want to know because i tested this yaml file in dev env and everithing was working fine.when i ran it in prod things were not expected working properly. so i want to see if this file was modified or not. so we can see commit switch which were made towards this pod.yaml file and we can see changes.

- ALSO i edit one line in yaml and commit.however before this commit goes to matser i can do a curl pull req and within a pull req i can add a reviewers there. Means before this specific change goes to master branch and after it gets applied what i can do is i can go through a pull req whwre my peers were working with me they can review the change. if they find things not going expected they can reject my change, this can prevent downtime.

***********************************************************************************************************************************************************
Creating First POD  with  YAML Conﬁguration file :
- Already discussed about multiple ways in which we can configure object withing k8s cluster.
1st : run kubectl command which automatically does lot of things for us.it can convert our command to specfic json format before it sends it to api.
2nd : we can create our own config file and that config file can be written in yaml format.
- DO the 2nd approach from scratch : we already seen how a pod can be created with pod.yaml
- what contents we need in pod.yaml : expl below

apiVersion: v1
kind: Pod
metadata:
  name: mywebserver
  labels:
   app: demo-app
spec:
  containers:
  -  image: nginx
     name: mywebserver

  apiVersion(version of api that we want to use , we can't use all api it depends on Kind(pod or etc . we have pod and this pod isv1 core api group. in k8s   
  there are lot of api paths available.)
- after api,kind we need to define metadata. metadata uniquiely identifies a specific k8s object within the give namespace. here i given name as mywebserver
 SPec : is the desired state of object. i given containers name is mywebserver and image is nginx.

- in officlai k8s docs -https:k8.io : api overview in that there are various objects available in workloads(pods,job replicaset). click on pod we can see the yaml config for pod.if we click on replicaset it will give template(config file) for replica set.
- in kubectl run : kubectl api-resources or kubectl explain pod : it will give detailed info about pod.

DEMO : i created a newpod.yaml. this is where we will be configuring our new object through yaml file.
apiVersion: v1
kind: Pod
metadata:
  name: nginxwebserver
spec:
  containers:
  -  image: nginx
     name: democontainer

- now run: kubectl apply -f newpod.yaml : o/p- pod/nginxwebserver is created.kubectl get pods we can see 1 pod.
- to delete run : kubectl delete -f newpod.yaml ( -f is file) : o/p- pod is deleted.

***********************************************************************************************************************************************************
LABELS AND INSTRUCTORS : 
- labels are key/value pairs that are attatched to objects, such as pods.
1st ex : i have 2 diff env's. 1st env : server,DB,LB  in 2nd env : LB,Server,DB.
- now if someone says to stop all the resources associated with dev env.from above it is diff to know which is dev env server, because above there is not appropriate label associated with all resources

2nd ex : we have added label to each resource like 1st server has label where key is name and value is kplabs-gateway, and second label where key is env and value is prod.\
  ex: name = kplabs-gateway
      env  = prod             same for all resources in 1st env is prod and 2nd is dev.
- in aws tags for servers similiar to tags. in aws ec2 tag : name(key)=kplabs(prod)

SELECTORS : Selecters allows us to filter objects based on labels.
EX : show all the objects which has label where : env:prod. this will show all resources where env is prod

- in aws i have 10 ec2 running. above in ec2's earch option(filters) we can consider filter as selector.i search i can see a tag optio. give dev to that tag, it will show only resources which belongs to dev.

There can be multiple objects within the k8s cluster 
 some of the objects :Pods,services,secrets,namespaces,deployments,Daemonsets. we can attatch label to objects in k8s cluster just like we did it for lb,db,servers.etc
In K8s : we have multiple pods. we can't identify each pod without labels.give label env:prod,dev for pods. we can get list of pods using sleectors.

Use case : run kubectl get pods . i can see 2 pods. from the name of pod is diff to understand which pod belongs to dev or prod.to know use -l(label).
  run : kubectl get pods -l env=dev . it will give the pod which has label env as dev.    
 or we can run : kubectl describe pod pod_name. it will give lot of info. in that we can see labels : env:dev.

IMPLEMENT LABELS AND SELECTORS FOR K8S OBJECTS :

Demo create 3 pods (names=pod-1,2,3) by : kubectl run pod-1 --image=nginx .same for pod2,3. now run kubectl get pods we can see 3 pods. to see labesl associated with pods run " kubectl get pods --show-labels " we can see each pod has a label of (run=pod-1,2,3) with this way it is diff to understand fxnlty of each pod.
- to add a label to pod-1 : run " kubectl label pod pod-1 env=dev " . env=dev is the label thatwe attatchedto pod-1. same for pod2,3( in env stage prod). run " kubectl get pods --show-labels " we can see (env=dev,stage prod for each pod)

SELECTORS : run kubectl get pods it will give all the pods.but we want to look for a pods that has env of dev.to get that we add a selector.
 run " kubectl get pods -l env=dev " . here (-l) is the selector and (env=dev) is the label we will get pod-1. same for prod,stage we will get pod2,3.
- run " kubect get pods -l env!=dev " : pod2,3. labels which doesn't have dev env are pod2,3.

- to remove labels for pod : run " kubectl label pod foo bar - " ( foo is name of pod. name of key is bar )
  run " kubectl label pod pod-1 env- " this will remove the env label associated with pod-1.

- but in prod we mostly use Yaml files for objects not kubectl.
run " kubectl run nginx --image=nginx --dry-run=client -o yaml " dry run client with the o/p of yaml.we will get a yaml file of the pod nginx. inside yaml we can see labels section under metadata

-  run " kubectl run nginx --image=nginx --dry-run=client -o yaml > label-pod.yaml " here yaml file will store to label-pod.yaml file. inside that filke under labels section we can add 1 more label (env=dev) and save file. from cli run " kubectl apply -f label-pod.yaml ". pod will be created 

- if we want to apply labels to all pods run " kubectl label pods --all status=running " this will apply label(status=running) for all the pods.
- to get more info about label run : kubectl label --help.
- to delete all pods run " kubectl delete pods --all " 

***********************************************************************************************************************************************************
OVERVIEW OF REPLICASETS : A ReplicaSet purpose is to maintain a stable set of replica Pods running at any given time.
EX : we have a replicaset. within replicaset we have 2 configuration settings.1.desired state = 3. 2.Image=Nginx . Now replicset will maintain 3 pods of nginx always running within our k8s cluster. this is replicaset purpose.it always maintain desired amount of the pods from the images thatwe have specified to replicasets.
DesiredState: The state of pods which is desired at any given moment of time
Current state : The actual state of pods which are currently running within k8s cluster.

Replicaset : DesiredState=3,  CurrentState=3, Image = Nginx.  Means we would like to have 3 replicas of pod from nginx image to be up and running.current state is 3, because 3 pods running. Now let's say due to some reason pod 2 stopped working.now currentstate becomes 2.however desired state is 3. Now replicaset will go ahead and it will launch a new pod so that our desired state matches the current state. Remember replicaset will always tries to maintain desired state=Currentstate.

DEMO : run kbectl get pods .we can see 3 pods running.
 run " kubectl get replicaset " we will get 1 replicaset, name is frontend.Desired is 3, current and ready is 3.
- run kubectl get pods.we cans see 3 pods from frontend replicaset. in list of 3 pods delet one pod from replicaset . run " kubectl delete pod frontend-pod-id " .  now run " kubectl get pods " we can see 1 more pod got automatically launched 10sec back.it is luanched by replicaset. becuse we deleted one pod and total count(currentstate) didn't match the current set.this is why replicaset automatically created one more pod. 

***********************************************************************************************************************************************************
CREATING REPLICASET: implement replicaset through yaml file. Docs : https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/  .we can find the yaml exampe for replicaste.
replicaset yaml file in k8s docs :

apiVersion: apps/v1              (replicaset is part of apps/v1. run "localhost:8080/apis/appv1" we get resourceslist in apps/v1 like controller, daemonsets   
                                  deployments,replifcasets)since replicaset is in apps/v1. so the apiversion of replicaset is apps/v1.
kind: ReplicaSet
metadata:
  name: kplabs-replicaset     ( name of replicaset)
  labesls:                    (label to replicaset)
    tier: frontend 
spec:
  replicas: 5                 ( specifying no of replicas to be up and running.)
  selector:                   ( here we are matcjing the label where the key is tier and value is frintend) 
    matchLabels:
12    tier: frontend
  template:                   ( template associated with the resource of pod. here in template we don't give apiversion and kind replicaset will take care. 
                                we start from metadata)
    metadata:
      labels:                 (metadata is empty.attatching a label to the pods which will created. we specified replicas=5. means 5 pods will be created
13      tier: frontend        ( all the 5 pods will have a label of tier frontend)
    spec:
      containers:             ( in spec we have 1 container , its name, and image)
      - name: php-redis
        image: nginx

- Save the above file as replicaset.yaml. and run " kubectl apply -f replicaset.yaml " it will say 'kplabs-replicaset has been created'
  run ' kubectl get replicaset ' we can see newe replicaset name is kplabs-replicaset, desired state and current state is 5. and ready is 0. (ready=0) becuae it can happen the containers are in creation stage. after some time pods(containers) get created. now run get replicaset, we can see ready=5.
- we alreday discussed replicaset will always try and maintain  the desired amount of pods. delete one pod from replicaset " kubectl delete pod pod_name ".
 - we deleted one pod.run " kubectl get pods ". replicaset will create 1 more pod . 

- run " kubectl get pods --show-labels ". we can see all the pods has a label of frontend. where we specified in yaml file : metadata:labels:tier:frontend
- in line 12 : tier: frontend = this label is for replicaset to see how many pods are running. replica will look into labels.in pods replicaset will check if 1 pod has tier=frontend it will count as 1..and 2nd 

- it is vry imp that Selector should match the labels that we associate it with the pods
- replicaset shortcut=rs . run " kubectl get rs " it will give list of replicasets.
- run " kubectl delete rs kplabs-replicaset" this will delete kplabs replicaset.

- if tier: backend in 12th line and tier:frontend in 13th line above , this will throw an error : label within the template doesn't matchj the label within the selector. whatever label we give in the selector should match with the label in template.

***********************************************************************************************************************************************************
Deployments : 
- Challenges with replicasets(RS): In rs we get basic fxnality like managing pods,scaling pods etc
EX : we have rs. it has desired state is 3. tmrw i want 5 pods.here we go and change the desired state of rs.and rs will launc 2 more pods so that curerent state becomes 5.for these basic fxnaloities we can use rs.

- however if we want certain imp fxnalities related to rolling out changes,rolling back changes and so on , then we need to go with deployments.
- Deployments provide replication functionality with the help of ReplicaSets, along with various additional capability like rolling out of changes, rollback 
  changes if required.

- we have a deployment set which sits at the top , now deployment set internall it makes use of replicaset to achieve replication capability related to pods. along with that deployment can also do various things like rollback, it can maintain the appl revision.

BENEFITS OF deployments : ROLLOUT CHANGES -
- We can easily roll out new updates to our application using deployments. let's say we are managing k8s within our org.it might happen that for every few days there will be new version of our a[ppl which has been tested and we want to rollout that new version of our appl.

 use case : we have a Deployments.it uses rs.and rs have 1 pod. Now tmrw there is a new verion of our appl which has been released and we want to makesure that our prod has that new version.So now Deployment will create one more replicaset it will launch a pod with the latest version of our appl.and then it will go ahead and connevt to that replicaset. now once our new version of appl has been created then Deployment will remove the older version of our appl.

- This appl will makesure that whenever a new version of our appl is deployed the appl is not down.because appl not down is imp aspect.
- Deployments will perform an update in a rollout manner to ensure that your app is not down.

Benefits of Deployment - Rollback Changes : 
- let's say we deployed a new version of appl and due to some reason the appl is giving a lot of issues to our users.now i would like to rollback from the latest update to the previous working  update which is was working perfectly.in this case we can easily do it with deployments
- deployments behind the scenes it uses replicasets.however it provides lot of additional capabilities like rolling out our rolling back changes

DEMO : run kubectl get deployments : i see 1 deployment(kplabs-deployment) which is currently available.
- look into rollout history associated with this deployment. we can see multiple revisions which has been made to this specific appl. like we have revision 2,3,4 etc.let's say we switched to revision2. now due to some reasons there are lot of errors that are happening in this revision. now we can roll back to revison 1 or we can even change our appl revison to version 3,4 etc.this is one of the adv's of deployments that we can not only look into the rollout history to see on what of the updated changes that are been happening to our appl.we can also rollback to a specific revison incase if latest version of appl is giving errors.

CREATING OUR FIRST DEPLOYMENT : we already saw the video of replicasets we already know the structure and imp of every field within the yaml file. we will make few changesto replicasets yaml file like( kind:deployment, change name (optional) ) and save file as kplabs-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx

- now run " kubectl apply -f kplabs-deployment.yaml " it says 'deployment.apps/kplabs-deployment' created. run kubectl get deployments we can see 1 deployment created. we already know that deployment makes use of replicasets to make sure that there are always a specific no of replicas which are running.
- even in deployment.yaml file we see in spec:replicas=5.here deployment will create a rs which would have a desired state of 5.to know run " kubectl get replicaset" we will get 1 replicaset name(kplabs-deployment-9shdyujgd has a desired and current state is 5). run kubectl get pods we can see 5 pods currently running. these 5 pods are managed by repolicasets and replicasets are intern managed by deployment


deployments : ROLLOUT CHANGES : we already have 1 deployment, replicaset and 5 pods in rs. now we want to update the appl ,so now deployment will create a new rs and oit will luanch all the pods within that replicaset and that rs will be connetcted to deployment. once the pods within the rs is running perfectly then deployment will go ahead and it will remove the pods associated with the previous appl(rs).

 DEMO : in deployment.yaml at last in spec section we can see the image which the pods are launching is nginx. assume this is version1 of appl.tmrw i want to specify a custom nginx version there( in dockerhub lot of nginx versions available.) so in 2nd verison of our appl we want use 1.17.3 .

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: nginx:1.17.3      save this file.and aplly the changes within deployments.yaml file." kubectl apply -f deployments.yaml" now run kubectl get replicasets. we can see 2 replicasets which are created.( we already discussed in order for a new rollout change to happen deployment will create a 2nd replicaset and 2nd rs will have the new version of our pods running. along with that deployment also start to remove the pods which are present within the older replicaset in a rollout fashion.) 

- now run kubectl get replicaset : we will see there are zero pods which are currently present in older rs. and new rs has all the pods which are running.
 the new rs has the latest version of the nginx image which is running.
- to know more about it : run " kubectl describe deployments kplabs-deployment " this will give info about our deployments . in events section we cansee detailed info about the rollout .
 in events we ction we can see 
- " scaled up rs kplabs-deployment-1st to 5 "
- " scaled up rs kplabs-deployment-2nd to 2 "
- " scaled down rs kplabs-deployment-1st to 4 "
- " scaled up rs kplabs-deployment-2nd to 3 "
- " scaled down rs kplabs-deployment-1st to 3 "
- " scaled up rs kplabs-deployment-2nd to 4 "
- " scaled down rs kplabs-deployment-1st to 2 "
.
.
- " scaled up rs kplabs-deployment-2nd to 5 "
- " scaled down rs kplabs-deployment-1st to 0 "

- run kubectl get replicaset : 2 replicasets(1st,2nd)we know deployment maintains bot the replicasets.
- if we want to change the scaling up and scaling down of replica would happen if we change the rolling update strategy.
- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment " : it will show 2 revisions. means deployments maintains revison associated with our changes. like earlier we had simple nginx image that was revison 1. then we chnaged to nginx1.17 this is revison 2.
- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment --revison 1" : thi will show all details including image:nginx.
- run " kubectl rollout hostory deployment.v1.apps/kplabs-deployment --revison 2" : thi will show all details including image:nginx:1.17.3

This revision is a great functionality. because it happens luke we have shifted to reviso 2 and there are lot of issues whic are happening in revsion 2 then we can go ahead and even rollback to the previuous revision which was known to be working well.
IMP:
- Deployment ensures that only a certain number of Pods are down while they are being updated.
- By default, it ensures that at least 25% of the desired number of Pods are up (25% max unavailable).
Lt's say i have a deployment which has 3 pods.now i am updating appl. so deployment can depending upon strategy and my configuration deployment can bring down all 3 pods at a time of our appl so at this time our appl lead to 100% downtime.but i don't want this. i dont want deployment to immediatly terminate all my pods of running applbecause that will lead to downtime.

- so what deployment does is we have 3 pods.deployment wull terminate 1 pod, 2 pods are still running whic will serve rtraffic.Along with that deployment will go ahead and launch one more pod of my new appl.if new pod running perfectly ,remove 1 more pod from old deployment and launch new pod in new appl. till the desired statein new appl equals to current state in new appl and current state in old deployment equals to zero.
- Deployments keep the history of revision which had been made.

***********************************************************************************************************************************************************
MAX Surge and MAXUnavailable options in deployment :
- whenever we perform update in deployments there are 2 imp options : 
1.MAXSurge : Max no of pods that can be scheduled above original no of pods 

  EX: we have a deployment, and it has total 5 pods(deployment with replicas of 5). now i want to set a diff image . let's say the deployment with 5 replicas is running based on nginx image.now we set to apache image.when that update happens to the deployment, during update process a new set of pods are created on top of the existing pods.existing pods is 5, 2 more pods will be created , then at a instant of time there will be 7 pods.after 2 pods based on apache got created and running 2 older pods from nginx will be deleted simliar as part of rolling update.

2.MAXUnavailable : Max no of pods that can be unavailable during the update.
 EX : we have a deployment with replica of 2.and dependin upon the no of visitors thata re visting the website we need atleast 2 replicas to be available at a time.let's say we modified the image associated with the pod deployment and now 1 of the pod will be removed  for few secs during the update process.during that process 1 repolica would be running.so that means half of the vistors who are visting website they will either get an error or connection will become slow.
-So depending upon our use case we can go ahead and configure both of these config parameters accordingly.

Demo : create a deployment from nginx image with 3 replicas . run " kubectl create deployment demo-deployment --image=nginx replicas=3 ".run get pods we can see 3 pods will be running. to get deployment in yaml file run " kubectl get deployment demo-deployment -o yaml " ( -o yaml : output in form of yaml)
- in yaml withing strategy of rolling update: maxsurge:25% ,MaxUnavailable:25%  :means during deployment process there can be additional no of pods  at a time or less no of pods that will be running at an instant time.

- now set the image of deploymet from nginx to apache(httpd) " kubectl set image deployment demo-deployment nginx=httpd" this will trigger a new update.
- now run get pods : we can see additional to 3 pods 1 more pod will be running.it can also happen that only 2 pods will be running and 3rd will be in creation stage.

- let's say our worker node is capable of running 3 pods.it doesn't have capacity of running the 4th pod.if we see there is also a 4th pod that is been created so dependin upon the overall config the h/w that we have we can go ahead and modify the unavailable and the surge and dpending upon rhat the pod updation would take place.

EX : we have total pods in deployment is 8.
 default config of both maxsurge and maxunavaialble is 25%. now we go and set and update to deployment(like setting new image). during the update process since maxsurgeis and maxunavailable 25%(25% of 8 is 2)
- this means during update process of deployment atmost pof 10pods would be running(8 current + 2 maxsurge pods )
- atleast 6 pods(6 current pods - 2 maxunavailable pods )

DEmo : let's say worker node hardly has any resources left : it can only run 3 pods.in thisif we want to perform  deployment base update we will update the maxsurge paramater to zero. get pods : we have 3 pods
- run " kubectl edit depolyment demo-deployment " it will open in yaml file and edit mazurge to 0 and save the change and close editor.our deployment is edited now. lastime when we updated the deployment it ran an extra pod with 3 pods(4). this time chnage the image from apache to nginx.

 run " kubectl set image deployment demo-deployment nginx=nginx ". run gets we can see 3 pods.(here one old pod removed and one new pod added)at an instant of time there will be 3 pods only. in this approach for few secs only 2 pods will run and they are serving traffic.
-we can define maxsurge and unavailable in term of % or numeric.

***********************************************************************************************************************************************************
Kubernetes Secrets : 
Challenge : ex : we have an App A runs on pod and it needs to connect to a Mysql DB to start working properly.
we have appA pod, and mysql.appA needs to authenticate with mysql DB. inorder for appl to start working it has to authenticate. Generally developer hardcodes the cred's.here APP A launches from a docker image and dev would hardcode the creds like(db_user,db_pass) in docker image by itself.Since trhe cred's are hardcoded once appl starts it can directly authenticate with user name and pswrd to mysql DB.

Risks with these approach : 
- Anyone having access to the container repo can easily fetch the credentials.
- Developer needs to have creds of prod systems. if mysql is prod DB , sinvc ethe dev wouldmbe building hisown docker image, the db team will have to give 
  the dev the username and pswrd in plaitext

- Update of creds will lead to new docker image being built.: like we have a compliance policy where we have to rotate db creds every 90days,so every 90days    we have to rebuild our docker image considering that the creds are hardcoded.

- INSTEAD OF HARDCODING CREDS WE CAN STORE THEM CENTRALLY : 
 here we have a central secret store. it contains our creds.now APP A doesn't have any hardcoded creds. so whenever a APP A gets launched , our appl can fetch the creds from central secret store.once it fethes creds it can use that creds to authenticate with mysql DB.
 PATH : CENTRAL SECRET STORE - fetches creds- APP A(POD) - AUTHENTICATE WITH MYSQL DB.

ADV'S : 
- we don't store hardcode cred's in pod.
- dev will not have creds.here dev will have to write a logic to pull creds from central secret store.
- even cred's are rotated the docker image will not have to rebuild.
In central secret store db team or other team will store creds and manage the creds.

CENTRAL SECRET STORE : name itself describe it is a central place where secrets will be stored. Kubernetes secrets is central secret store.
 Various other services also provide central secret store : AWS SECRETS MANAGER, AWS SYSTEMS MANAGER PARAMETER STORE , HASHICORP VAULT all these helps to store secrets centrally and the creds in these can be fetched by APP A pod to do oprns like authentication.

K8S SECRETS : A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key.
- Allows customers to store secrets centrally to reduce risk of exposure. like we have k8s secrets and we are storing username and pswrd.
- Stored in ETCD database.

Creating k8s secrets in CLI : " kubectl create secret [TYPE][NAME][DATA] "
 here TYPE : 3 types : 
1.Generic : File, directory, literal value , ((--from will be used with all these 3 generic types)
2.Docker registry
3.TLS.

DEMO : 
run " kubectl get secret " we can see 1 secret which is default secret. 
create a secret :  " kubectl create secret generic fistsecret --from-literal=dbpass=mypassword123 " this is literal value example. run kubectl get secret we can see 1 secret(firstsecret)

- run " kubectl describe secret firstsecret " it will show details. in details it only shows dbpass. it doesn't show value associated with it.if we want to see value run " kubectl get secret firstsecret -o yaml " now secret will give dbpass value in base64 encoded format.like ( dbpass = kjshdkjfagfkjfg)
 to decode it innlinux run " echo kjshdkjfagfkjfg | base64 -d "  we will gte mypassword123. this is secret interms of literal value.

Load secret from file : in notes.txt write 'dbpassword1234'. now we will add a secret with option of file. run
 " kubectl create secret generic secondsecret --from-file=./notes.txt" run get secret we can see 2nd secret.
- Create a secret from yaml file :

apiVersion: v1
kind: Secret
metadata:
  name: thirdsecret
type: Opaque
data:
  username: dbadmin
  password: myadmin   save file as secrets.yaml

run " kubectl apply -f secrets.yaml " it will throw error : bad request because data we given usernam,pswrd are in plain text.it needs the encoded version.
now encode username and pswrd : in linux terminal run " echo -n 'dbadmin' | base64 " this will give ZGJhZG1pbg== , do same for pswrd 'myadmin' it will give bXlwYXNzd29yZDEyMw== .
apiVersion: v1
kind: Secret
metadata:
  name: thirdsecret
type: Opaque
data:
  username: ZGJhZG1pbg==
  password: bXlwYXNzd29yZDEyMw==  now run this " kubectl apply -f secrets.yaml " it will say 3rd secret is created.


MOUNTING SECRETS INSIDE PODS : 
- Once a secret is created, it is necessary to make it available to containers in a pod. there are 2 approches to achieve this 
1. Volumes(primary approach)
2.Env variables

DEMO : run kubectl get secret : we will get multiple secrets thatw e created above. run kubetl get secret firstsecrte -o yaml. in o/p we can see dbpass=base64 encoded value. now we will use volume method.

apiVersion: v1
kind: Pod
metadata:
  name: secretmount
spec:
  containers:
  - name: secretmount
    image: nginx          tille here is common
    volumeMounts:           ( here we have a vol of foo. mount path is /etc/foo , and this is a read only directory.
    - name: foo
      mountPath: "/etc/foo"       we are associating first secret within this "/etc/foo" mount path. 
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: firstsecret     here we are making firstsecret should available in this "/etc/foo" directory inside the container. 
 - save the file as secretpod.yaml : run " kubectl apply -f secretpod.yaml " o/p : pod/secretmount created. now run get pods. we can see 1 pod in creating stage.
login to pod : " kubectl exec -it secretmount bash ". inside container run " cd /etc/foo " and run ls we can see a file dbpasss.
 run cat dbpass : o/p mypassword123

2ND APPROACH : ENV VARIABLE :

apiVersion: v1
kind: Pod
metadata:
  name: secret-env
spec:
  containers:
  - name: secret-env
    image: nginx
    env:                                      this is env section
      - name: SECRET_USERNAME                name associated with env variable.
        valueFrom:
          secretKeyRef:
            name: firstsecret
            key: dbpass         whatever value associated with key of dbpass in the secret of firstsecret will be associated with SECRET_USERNAME
  restartPolicy: Never

- Save the file in secret-env.yaml . "kubectl apply -f secret-env.yaml " o/p : pod/secret-env created . cretaed 1 pod.
- login to secret-env container. so in env var approach we have a env var which has been created called as(SECRET_USERNAME) 
 inside contaienr run " echo $SECRET_USERNAME " o/p: mynewpassword123. means the secretusername env variable has value(myne..) associated with secret.	 


***********************************************************************************************************************************************************
CONFIGMAPS : 
Use Case : we have an App A container. depending upon the env the config settings associated with APP A container will need to be chjanged.
EX : we have dev and prod env's.
- we have dev pod and prod pod.
- so for dev containewr of APP A we need certain env like : app.env=dev app.mem=2048m app.properties=dev.env.url. now when this container goes to prod we have to change the values. 

- like for prod container of App A : we need  app.env=prod app.mem=8096m app.properties=prod.env.url
- usually org create 2 diff container images. inthese 2 container images althoiugh the appl is same but the config file which has the above parameters are diff. Just for that single config file they have multiple config images.this is not best practice.
- let's say tmrw in dev env we want to change memory from 2048 to  app.mem=8096m. then we have to rebuild entire contaienr image.this is very tideous task.  
 and that's why configMapas will really useful.

- ConfigMaps : ConfigMaps allows us to centrally store data.what happens now is that instead of hardcoding the above parameters within the container image, we will centrally store the data. we centrally store the data associated with dev properties , same for prod. from container image we are referrencing the central data 

- in centrally storing data we can dynamically change the values.let's say i want to add one more paramater (app.cpu=1) in config file , if our config's are stored centrally then we can easily do that. If our config files are hardcoded within the container image then if we want to change even a certain parameter 
 we will have to completly rebuikd that speciifc image.

- The method of centrally storing the data is achieved with the help of ConfigMaps within K8s.
- we have ConfigMap object.within that ConfigMaps there are 2 ConfigMaps that are available 1 is associated with dev and is associated with dev pod,2nd is associated with the prod which is mounted with the prod pod.

- if we already have all of our config's which are been stored centrally then we no longer need to have a seperate contaienr image.here we can have a single container image and depending upon which env our image is deployed(say we are depoying image in dev env)then automatically we can reference the dev config map here. if we are pushing oiur container image to prod env then we can directly push this prod properties to the container image.

- CLI Syntax for creating ConfigMap : " kubectl create configmap [NAME] [DATA-SOURCE] "  here data source can be( file,directory,literal value)
- run " kubectl get ConfigMap ": currently no resources found.

- create 1 configmap : " kubectl create configmap dev-config --from-literal=app.mem=2048m " : configmap/dev-config created
- kubectl get ConfigMap : it shows 1 config map dev-config. run " kubectl get configmap -o yaml " : inside we can see this configmap has literal(app.mem it is key) and it has a value 2048m.

2ND approach reference to a file : we have 100's of configmaps. instead pof adding via literal value we can add it via file.if we have multiple config file we can add a directory.
- i have a file dev.properties : this file has 3 key value pairs:
 app.env=dev
 app.mem=2048m
 app.properties=dev.env.ur     3 key value pairs associated with dev env. since this is a file we can directly reference this to configmap.
run " kubectl create configmap dev-prop --from-file=dev.properties ".o/p: configmap/dev-properties created . means whatever configuration which was stored in the file that config is part ofthis configmap.
- run " kubectl get configmap dev-properties -o yaml " o/p : in data section we can see all 3 values which were part of dev.properties.

Q. How we can mount configmap which has been created inside a pod.it might happen that our app will be running inside pod and app intern needs this config files to start itself. so we want whenever a pod starts all of our configurations(app.env,mem) should automatically be mounted within our pod so that appl can access it.
- various ways to mount. main way is through volumes.
file : configmap.yaml :

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod         ( cretaed a pod and name is configmap-pod and launching it from nginx container)
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:           ( we using volumes, where we have volumemounts,and we have volumes)
      - name: config-volume
        mountPath: /etc/config      ( in volumemounts we are giving mountpath
  volumes:                           (in volumes we are referencing the config-maps
    - name: config-volume
      configMap:            ( the reference of volume is config map)
        name: dev-properties   ( in our case name of configmap is dev-properties. so whatever data which is present in dev-properties will e mounted inside 
                                 etc/config directory of my pod.
  restartPolicy: Never     Save the file as configmap.yaml
run " kubectl apply -f configmap.yaml " : pod/configmap-pod created. run get pods we can see 1 pod.login to that pod using " kubectl exec -it configmap-pod bash " go to "cd /etc/config ". and run ls. we can see dev.properties. run cat dev.properties . we can see 3 properties  app.env=dev,app.mem=2048m
 app.properties=dev.env.ur " this dev.properties is the file name that we have referenced in old approach.remember we have done from file and we have referred dev.prop is mounted inside the pod under etc/config

***********************************************************************************************************************************************************
KUBERNETES SERVICE( networking aspect ) :
- whenever we create a pod we will have a corresponding ip address associated with pod.
- ex : there are 2 pods : frontend will have 10.2.0.1 ip address,backend will have 10.2.0.2 ip address. if frontend wants to communicatw with backend it can do it with the help of ip address associated with backend pod.

issue1:
- in org we will have frontend appl and backend appl.frontend appl will connect to backend appl to execute certain logic.so in order for both of them to communicate the frontend will need a ip of backennd. assume within frontend config file we sepcified backend_url=10.2.0.2 which is ip address associated with backend pod.frontend appl will refer to the given config paraeter(backen_url)to connect to backend to send the appropriate data which would be computed

 by the backend servers. asume due to some reason the backend pod has stopped working and it doesn't respond. so now if we have deployment it will llaunch a new pod and this new pod will have a diff set of ip address(10.2.0.8). the appl running in frontend and its config paramter still has the ip address associated with the old pod. now frontend appl will give error(http 500). means frontend although it is working it will give error to client eventhoiugh the working backend. 

- the config file of frentend doens't have the updated ip address associated with the new backend pod which got created. we will face these type of issue whenw e hardcode the ip or dns name associated with the pod.
Issue2 :  we have 1 frontend pod and multiple backend pod. it can happen that there are 3 backend pods.since tmrw lot of req's the backend might scale to 10.Q is how will we update the ip addresses of the backedn pods in frontend config file.if we are using deployments then replicasets can scale a lot.

- to overcome these type of issues we can use K8s Serviuces : in this we create a backend-gateway( this will connect to all the backend pods). if 5 pods in backend are reated then allk the pods associated with backend gateway. no only thing that frontend has to do is it has to send the req to backend-gateway.

- assume one pod among multiple pods in backend stopped working , we don't have to worry.becuase backend gateway will not send req to that pod.however the req will still be sent to other 2 pods which are running or if new pod gets greated then it will autoimatically will be associated with backend-gateway.

- the backend-gateway which acts as a intermediatory layer is called as Service .K8s service can acts as an abstraction which can provide a single IP address and DNS through which pods can be accessed. means backend-gateway will have an ip address or dns name.the ip address of backend can configured in configuration file of frontend appl. backend acts as a single point through which all the req's would be routed.

- like serviuce associated with 3 pods. we can perform lot of things at backend(service) level like load balancing, scaling pods without having to change the frontend



- run kubectl gfet pods: we have 4 pods.2 pods are associated with nginx webserver( both pods running as appl)
- run " kubectl get pods -o wide " it will give ip address associated with all pods. connect to 1stnginx webservr pod " kubectl exec -it 1stnginxpod curl localhost " o/p : it will say it is nginx container 1. run the same for another nginxpod using curl local host . it will say nginx container 2. 

- above we seen 2 pods of nnginx webvserver an dresponding well for curl. 1 among 4 pods is frontend(name=newcurl as frointend appl).
  frontendappl= newcurl    backend = 1stnginx, 2ndnginx (2 pods in backend)

- now login to frontend appl and do curl to 1 of the ip associated with backend appl pod " kubectl exec -it newcurl curl 10.2.0.2 " assume the ip is of backend : it will say contanienr1 ( means frontend able to communicate with backend). simliar way do the same login to frontend and do curl to 2nd backend pod it will give o/p: container2. if backends got stopped the frontend will fail. thats why we will use backend.

- run kubectl get service : i can see 1 service which is cretaed( name=servicek , it has a specific ip address , and it is conneted with all the backendpods 
- tp verify run " kubectl describe servicek " : o/p : within endpoints we can see the ip addresses of the multiple pods(1stnginxip, 2ndnginxip). means this service is conncted to the 2 pods of backend appl.

- now frontend instead of directly communicating with bckend pod it can always send a req to service ip.and service ip will deal with backend pod communication.
- connect to newcurl and do curl to service ip " kubectl exec -it newcurl curl ip_address_ofservice" will get o/p : this is nginx container1. when we run the same command it will change to container 2. again run changes to container1...

- means whenevver a frontend appl is calling a service , thenthe req is routed to pod 1,it is also been routed to pod 2.in this case if pod 1 fails then service can detect and it can only route traffic to the pod which is currently running.or if there is a new pod which is created that pod will automatically 
associated with serviece.

- service can also do load balancing so that a single pod can't have a huge load.
There are several types of Kubernetes Services which are available:
- NodePort
- ClusterIP
- LoadBalancer
- ExternalName
depending on use case we can use one among them.

***********************************************************************************************************************************************************
CREATING FIRST SERVICE AND ENDPOINT :   path : frontend(pod)- backend(service)-endpoints(pods)
- we have a frontend which is a pod
- Service(backend) is a gateway that distributes the incoming traffic b/w endpoints.
- Endpoints are the underlying PODS to which the traffic will be routed to.
DEMO :  create a frontend pod and backend pod. create servuce to route traffic coming from frontend to service endpooint.
-github link: https://github.com/zealvora/certified-kubernetes-administrator/blob/master/Domain%203%20-%20Services%20and%20Networking/serviceandendpoints.md

Create 2 backend pods :
kubectl run backend-pod-1 --image=nginx
kubectl run backend-pod-2 --image=nginx

Create frontend pod from ubuntu :
kubectl run frontend-pod --image=ubuntu --command -- sleep 3600

- now login to frontend pod and try to connect to one of the backend pod from frontend pod by doing curl.
 kubectl get pods -o wide  : we will get pods with ip's.

- now create a service and associtae the backend pods with service.
- create a file service.yaml and write inside : 

apiVersion: v1
kind: Service
metadata:
   name: kplabs-service            name of service.
spec:
   ports:
   - port: 8080                      port is where service listens to 
     targetPort: 80                  target port is where the endpoint will listen to.
- run kubectl apply -f service.yaml
- run kubectl get service : we can see kplabs service, it's ip, and its port: 8080/TCP
- run kubectl describe service kplabs-service : there we can see endpoints : none ( no endpoint connected)

PORTS: i have 2 worker nodes and one pod in each node. in one pod there is appl(nginx) is running which listens on port 80.if we want to connect to this specific appl and retriev the o/p of the appl we have to specify the ip follwed by the port 80. also we have kpolabs service and 2 endpoints (2pods) and also there is a port for service which is 8080 (the port of service). when we send the request to the ip of service with its port 8080 the service will send the traffic to the target port of the endpoint( here the target port is 80)

- now copy ip of service and connect to frontend pod.do curl on service (ip_service:8080) we won't get any respinse, because currently no endpoints that are connected to it.
- connecte endpoints to service , so that service can send the traffic to endpoint and retrieve the response and send the response back to frontend pod.

ASSOCIATE ENDPOINTS WITH SERVICE(KPLABS) :
Create a file endpoint.yaml :

apiVersion: v1
kind: Endpoints
metadata:
  name: kplabs-service
subsets:
  - addresses:
      - ip: 10.244.0.23        ip address of backend pod1.
    ports:
      - port: 80
- run kubectl apply -f endpoint.yaml.
- run kubectl describe service kplabs-service : now we can see 1 endpoint. which is endpoints:10.244.0.23
- connect to frontend pod and run curl on ip of service with service port . curl 10.24.33.2:8080  : here i will get html [page of nginx(backend appl)
- if we don't send the req to port where the serviuce is listening to 80 instead of 8080 : we won't get any response.

***********************************************************************************************************************************************************
Service Type - ClusterIP :
- Whenever service type is ClusterIP, an internal cluster IP address is assigned to the service
- since an internal cluster ip is assigned to the service , that specific service can only be reachable from within the cluster
- ClusterIP is default service type. means when we create a service and if we dont define ty of service then ClusterIP service will be cretaed.

EX ; we wlready know backend-gateway(service). and it has 3 endpoints(ip addresses of backend pods). here the service also called as ip address.- we have already done curl on ip address of service. this ip address is internal ip address of cluster.CLUSTERIP: because this ip address is among the ip addresses of cluster which is internal.
- Objects within the cluster are able to access this ip address.
- above in yaml we didn't mention type of service. so clusterip service is created. run kubectl get service . we can see kplabs-service type:ClusterIP

***********************************************************************************************************************************************************
USING SELECTORS IN SERVICE ENDPOINTS: 
- How to integarte labels and selectors while configuring k8s service : till now we manually defined the ip addresses of pods to servuce. for 1 pod is fine. but what if we have 100's of pods. if we use manual we have to add 100's of ip's to service. this way is not recommended and difficult.

- So whenever client creates a service they can define that (Add all the pods with label of app=nginx) . means when service cretaes the endpoint of that service will be associated with the ip address associated with all the pods that has specific label(app=nginx)
- so by adding only appropriate selector as part of service we can select all the pods .
DEMO : 
 create a file labels.yaml :  for deployment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3             ( 3 pods using nginx image. all 3 will have label of app:nginx)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

Create a service in service.yaml:

apiVersion: v1
kind: Service
metadata:
   name: kplabs-service-selector
spec:
   selector:
     app: nginx     ( we adding label here under selector. it matches the label that has been added to pods)
   ports:
   - port: 80
     targetPort: 80

- run kubectl apply -f labels.yaml.
- run kubectl get pods --show-labels : we can see labels associated with pods.
- run kubectl apply -f service.yaml .
- run kubectl describe service kplabs-service-selector : we can see 3 endpoints  ( 3 pods that are created)

Now suddenly lot of traffic will come to our website.we want 10 pods instead of 3 pods.so scale deployment of nginx-deployment.
run "kubectl scale deployment/nginx-deployment --replicas=10" 
- now run kubectl get pods : we can see 10 pods. run get pods --show-labels : all 10 pods will have label app=nginx
- now we want to know whether all these pods are part of service or not : 
 run " kubectl describe service kplabs-service-selector ". in endpounstwe can see 10 ip addresses.
- want to see list of ip addresses : run kubectl describe endpoints kplabs-service-selector" we can see 10 ip addresses.

- i created a new pod manually and i added same label(app=nginx) to that pod.so whenever k8s sees that a new pod is created with that specific label that is part of our given selector in creating service , the  service endpointv will automatically add this endpoint.
- kubectl run manual-pod --image=nginx   ( creating new pod name=manual pod from nginx image)
- kubectl label pods manual-pod app=nginx  ( adding label app=nginx to the pod name manual pod)

- now run get pods --show-labels. we can see all 11 pods has label app=nginx.
- run descrivbe service: w ecan see 11 endpoints associated witrh kplabs-selector.
- delete resoucres:
kubectl delete service kplabs-service-selector
kubectl delete -f demo-deployment.yaml
kubectl delete pod manual-pod

***********************************************************************************************************************************************************
SERVICE TYPE : NODE_PORT 
- NodePort exposes the service on each node's ip at a static port.
- we will be able to contact the nodeport service, from outside the cluster, by requesting <WorkerIP>:<NodePort>
- we have a service kplabs-service and it has 1endpoint(pod) in worker node1. since this ervice has clusterIP, whic ic internal Ip. so we won't be able to connect to this service from outside world.But we want outsid eworld to visit our website.Since clusterUp won't allow the external communication it.
- now i want outsiders9clients) to connect to service. this is where nodeport comes.

- now i created a service of type NodePort (service name=lplabs-service , it has 1 endpoint, the service is cretaed in worker node2). now on the worker node 
a new nodeport will be created. now anyone externally wants to service they have to connect toworker ndoe2 public ip followed by NodePort. and the req will now be routed to NodePortservice(kplabs-service)

DEMO :  create a pod, name NodePort-pod and label=publicpod from nginx image.
- kubectl run nodeport-pod --labels="type=publicpod" --image=nginx
- kubectl get pods --show-labels. we can see pod has label.

create a service of type nodeport in file Nodeport.yaml:

apiVersion: v1
kind: Service
metadata:
   name: kplabs-nodeport
spec:
   selector:
     type: publicpod
   type: NodePort           ( explicitly specifying the type of service as nodeport type )
   ports:
   - port: 80
     targetPort: 80
- kubectl apply -f nodeport.yaml. 
- kubectl get service  we can see our service and it has type: NodePort  , in ports: 80:30946/TCP ( but in cluster ip serviec we only see 80/tcp in port), but in nodeport we also see nodeport in nodeport section.

- so if we want to connect to this nodeport service we have to get the ip address of the workernode followed by the nodeport 
- get the public ip od worker node : kubectl get nodes -o wide : in exiternal_ip:12.39.30.3 this ip is associated with worker node.
- in broweser "12.39.30.3 " run this we wont get anything. here we also have to mention nodepport then only the req will transfer to the serviuce iunsid node. in browser " 12.39.30.3:30946 " we will see wlecome to nginx.
- this nginx page is coming from the pod which is nodeport-pod that is running from image of nginx

***********************************************************************************************************************************************************
K8s Networking Model: 
- Kubernetes was built to run on distributed systems where there can be hundreds of worker nodes in which Pods would be running.( k8s master and multiple worker nodes). in tjis type of arch networking is primary aspect.
- This makes networking a very important part component and with the understanding of the Kubernetes networking model, it will allow administrators to properly run, monitor as well as troubleshoot applications in K8s clusters

- Kubernetes imposes the following fundamental requirements on any networking implementation
* pods on a node can communicate with all pods on all nodes without NAT
* all Nodes can communicate with all Pods without NAT.
* the IP that a Pod sees itself as is the same IP that others see it as.

Based on the constraints set, there are four different networking challenges that need to be solved:

1. Container-to-Container Networking
2. Pod-to-Pod Networking
3. Pod-to-Service Networking
4. Internet-to-Service Networking

1. Container-to-Container Networking : 
- Container to Container networking primarily happens inside a pod. A pod can contain multiple containers.
- PODs can contain group of containers with the same IP address.
- Communication between the containers inside pods happens via localhost

EX : there are 2 containers(app,DB).the appl container can communicate with DB container via the locahost:followed by the port in which the appl is running.
- appl container communicating with db container(localhost:3306)
- if db container wants to communicate with app container and app container is running on port 80 then (localhost:80)

2. Pod-to-Pod Networking: 
- we have pod1,2. each pod has eth0(interface0) which will have pod ip.and then it also connects to one more interface(veth0), pod2(ethh0) will go to veth1.
 bothe veth0,1 will goes through a bridge and commuincation will happen.
- run kubectl get pods -o wide. we can see 2pods and they have 2 ip's. connect to one pod and run ifconfig. we can see eth0 interface and it has pod ip.
 scroll down we can also see multiple veth interfaces. there should also be a bridge for communication to happen.
- run route -n : we can see docker0 interface. this is trhe broidge used for communication.

3. Pod-to-Service Networking :
- Kubernetes Service can act as an abstraction which can provide a single IP address and DNS through which pods can be accessed.
- Endpoints track the IP address of the objects that service can send traffic to.

4. Internet-to-Service Networking
- let's i have 2 services (example and kplabs service). each service is direcyting to a single pod. now someone from internet wants to communicate with pod of example service. this can be achieved with ingress controller.
- Kubernetes Ingress is a collection of routing rules which governs how external users access the services running within the Kubernetes cluster.
- so whenever user types the example.in , then we want the traffic to go to example service, if we type kplabs.in the the traffic should route to kplabs service. these rules can be define as part of k8s ingress.
- we can have a ingress controller which is basically a LB, which intern will forward the traffic to underlying servoices.

***********************************************************************************************************************************************************
LIVENESS PROBE (health checks ):
- when we run appl for a long period of time eventually the appl will transition to a broken states and that can't recover except by being restarted.for that org's uses external tools like monit. Wheneve rmonit sees that an apll is broken or not running it will automatically restart that appl.
- Smiliar to this k8s also provides a feature of liveness probe which can detect and also provide remedy for such situations.

EX : we have an nginx and it is running for many months.And suddenly we see that nginx in broken state. when we restart it in broken state it works well.So this restart of our appl automatically when it breaks can be achieved with the help of Liveness Probe.
ppt:25 -
- we have a k8s cluster. in 1st step it verifies appl whether it is running. it found appl is working. so no action required.

- in 2nd step k8s verifies with a liveness probe , and the  appl is not working (apl inside container stopped). So now within the status we see unhealthy. when liveness marks as unhealthy it will restart the appl. once the k8s cluster verifies the appl is unhealthy , in step 3 of table we see appl is healthy.in step 2 we can see appl marked unhealthy and an action has been taken place whic is restart.
- thi sentire process of monitoring and having an action can be achieved with the help of livness probe.
- we have a file liveness.yaml : it has 2 setions 1 is creating a pod and another is creating a livnessprobe.

apiVersion: v1
kind: Pod
metadata:
  name: liveness
spec:
  containers:
  - name: liveness
    image: ubuntu
    tty: true
    livenessProbe:      (2nd section, we are telling k8s to regularly send livnessprobe to the container to verify wether the appl is runnin or not)
      exec:
        command:         (command service nginx statu, this will run by livness probe to verify appl, if appl is not running k8s will restart the appl)
        - service
        - nginx
        - status
      initialDelaySeconds: 20
      periodSeconds: 5

- create a pod from ubuntu image. run " kubectl run -it ubuntu --image=ubuntu " . this pod by default will not have nginx. run nginx status inside pod it wil say 'nginx:unrecognized service' also says ' command terminated with exit code 1' means the command that we ran gave an error.

- in the above yaml file we will get error because of command (nginx not installed), 2nd when we have nginx installed but nginx is not running.
- so manually install nginx in ubuntu pod. " apt-get update && apt-get install nginx -y " run service nginx status it will say not running. run echo status " echo $? " : o/p 3 ( non zero response means previous command is errored out). 
- run service nginx start it eill start nginx. now run status nginx : running.run echo status "echo $? " o/p:0(means everything is perfectly running)

- Run " kubectl apply -f livnessprobe.yaml " o/p:pod/livness created. run get pods we can see 1 pod.
 login to liveness container,run nginx status " kubectl exec -it liveness service nginx status " : this will give error with command terminated exitcode1.
 when the liveness porobe runs the above command'service nginx status' it will get an error over here.when it recieves error livenessporobe will restart the overall container.

- run kubectl get pods : we can see 1 pod running and also in restart:1, this will contineously doing this untill and unless it recieves an echo status of 0
- There are 3 types of probes which can be used with Liveness
- HTTP    ( if our appl is webserver.we can send a http get request and verify the response ,by http 200 or 400 
- Command   ( we taken command type in above example, where we specify the command which liveness probe have to execute to verify appl)
- TCP

***********************************************************************************************************************************************************
READINESS PROBE : 
- It can happen that an application is running but temporarily unavailable to serve traffic.
- Forexample, the application is running but it is still loading it’s large configuration files from external vendors.
- here appl is running but it is still loading the config files it is not ready to serve the traffic.in this case if we use livness probe, probe will consider this appl to be running , and it will start to serve the traffic , if it is under load balancer, we dont want this nor we don't want to kill container. 

- in these case we use READYNESS PROBE.
 EX :we have an antivirus appl and it is running but it is recieveing updates from centralaize server.
- generally in windows we have an .exe file. it can be 2 or 3 year old.when we double click on exe file and install it within our windoes this will lead to antivirus been running.but antivirus running doesn't mean that it is updated with the latest patches.so we want once antivirus is running it should fetch the latest updates with latest virus signatures and then it is considered as ready otherwise eventhougfh it is running but not considered as ready. for these case readyness will be useful.

EX : we have 2 pods under LB.1st pod is of antivirus and it has downloaded all the upodates from the cetral antivirus server.And once it has downloaded the latest updates the status will be ready. we have 2nd pod of angivirus which isjust created but it has update still in progress, eventhough antivirus service itself is running , but it is not ready since it is still updating. so this is why whenever we have LB , then we can have ability to send all thetraffic towards the appl whose status is ready.  since status of pod1 is ready, and pod 2 is not ready. so no status will reach pod2.this why readyness is usewful. but in livness probe , if we take example pof command it can check whether antivirus service is running, then liveness will consider it as ready, then LB will start to send traffic.

Syntax of readinessProbe : 
 readinessProbe:
  exec:
       command:
       - cat
       - /tmp/healthy       ( the command here is file. readinessprobe will check whether this file present or not. if it is present readiness probe success
                            if not fail. so our antivirus appl when its update completes, it should create a folder in tmp/healthy )
     initialDelaySeconds: 5
     periodSeconds: 5

Demo : i have readinessprobe.yaml file. file has 2 sections pod and readinessprobe
apiVersion: v1
kind: Pod
metadata:
  name: readiness
spec:
  containers:
  - name: readiness
    image: ubuntu
    tty: true
    readinessProbe:     ( readiness has exec. means it will run cat /tmp/healthy. the command is success if the file in directory is present)
     exec:
       command:
       - cat
       - /tmp/healthy
     initialDelaySeconds: 5
     periodSeconds: 5

-run aplly -f read.yaml : pod/readiness cretaed.
- run get pods. we see 1 readiness named pod. it is running. but ready :0/1. means not ready.
- run " kubectl exec -it readiness touch /tmp/healthy ". this will create a file in tmp directory.
- now readinessporobe will find the file and it makes ready status sucecss. 
- run get pods we can see readiness pod and ,ready:1/1.

***********************************************************************************************************************************************************
Daemonsets : 
Use case : i have 3 worker nodes(node1,2,3).i want to run a single copy of pod(Webserver) node.
common ans: create a deployment with replicaset of 3 and will launch 3 pods in 3 nodes. But in deployment it can happen that 2 or 3 copies of pod will be launched in a single node. but we want each pod should launch in each node. to achieve this we can use Daemonset.
- A DaemonSet can ensure that all Nodes run a copy of a Pod.
As nodes are added to the cluster, Pods are added to them.

- like i can have multiple copies of pods in a single node.or each podn in each node . what is advantage:
ex : i have an antivirus pod which scans every files in node in this case we need only one pod for one node.
2nd ex: whenever a new node comes up , then that new node should contain a single copy of antivirus pod. all these ex's will be achieved with the help of daemonsets.
DEMO : daemons.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kplabs-daemonset
spec:
  selector:
    matchLabels:
      name: kplabs-all-pods
  template:                    ( in template we are launching a conrainer in spec section. also every pod(container) thats get created that pod will have 				                a label of kplabs-all-pods. we also have selectore in above spec section which will have the same value as   
                                                 labels)      
    metadata:
      labels:
        name: kplabs-all-pods
    spec:
      containers:
      - name: kplabs-pods
        image: nginx

- run kubectl get nodes. i can see 2 nodes. now run apply -f daemons.yaml  :daeomonset.apps/kplabs-daemonset created.
- run get pods : we can see 2pods launched.
- run get pods -o wide : we can see in nodes section 1st pod from 1st node and 2nd from 2nd node.
 - also anytime anybnew worker node is launched in cluster.daemon set will also launch the pod in thgat node
- run kubectl get daemonset we can see 1 daemonset.desired=current=ready=2.
- run describe daemonste name_ofdaemonset

***********************************************************************************************************************************************************
Taints & Toleration :
- Taints are used to repel the pods from a specific node.
- we have node 1 ,node 2 right and left side. pod1,pod2 right and left.
- when we apply taint(boundary) on node1. so whenever taint applied on a specific node and if a pod is scheduled on that node by default it will be blocked.
  so if a scheduler tries to schedule a pod on the node which has been tainted then the effect would be blocked. node2 is not tainted. if we schedule same pod2 on node 2 it will be success. ow can we apply a pod on node which is tainted. Ans : Tolerations.

- In order to enter the taint worker node, you need a special pass.This pass is called toleration.
 same ex we have node1 which is tainted. a pod is trying to schedule itself in that tainted node.also pod got pass(specia) to enter node. since pod has special pass , the pod will be allowed to schedule in node1. pod can even scheduke on node2 because it is not tainted.

- run get nodes we have 2 nodes. run describe any one node.(kubectl describe node non=de_name): in that taints:None(no taibts applied on this node)
- to apply taint on node : run " kubectl taint nodes node_name key=value:NoSchedule" : specific node is tainted.
- run describe node : in taints section we can see taints:key=value:NoSchedule. as soon as node gets tainted it will block any pod that will launch in that node.
Syntax : 
tolerations:
 - key : "key"     ( it is giving the pass value as key)
   operator: "exists"
   effect: "NoSchedule"
- if we want to untaint any node run " kubectl taint nodes node_name key=value:NoSchedule- "

***********************************************************************************************************************************************************
Resource Requests and Limits :
- if we schedule a large appl in a node which has limited resource,then it will soonlead to out of memory and ultimately it will lead to downtime.
EX : i have a scheduler and 3 nodes available. the sizes of nodes is diff(node1,2,3= small,med,large) . i have an APPA which i want to schedule.i fi got and launch the appa, the scheduler might launch in appA in node1 which is empty.node2,3 are filled. Appa requires more resource that node 1 offers.
- so to overcome this we can manually specify the pod(appA) requires 512mb of ram or occasionally 1gb of ram. now if scheduler knows that the pod requires 512mb of ram then it won't lanch in node1. it will launch in node which has higher resources which is node3. This can be achieved with the help of Requests and Limits.

- Requests and Limits are two ways in which we can control the amount of resource that can be assigned to a pod (resource like CPU and Memory)
- Requests: Pod is Guaranteed to get a specific amnt of ram or cpu which is defined.
- Limits: Limits the amnt of resources that a specific pod can take. Makes sure that the container does not take node resources above a specific value.
DEMO : file: requestslimits.yaml : 

apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pod
spec:
  containers:
  - name: kplabs-container
    image: nginx
    resources:     ( inside this we specifying requests ,limits)
      requests:
        memory: "640Mi"
        cpu: "0.5"
      limits:
        memory: "12800Mi"
        cpu: "1"

- scheduler while scheduling any pod it only looks at requests not limits.

- runly apply -f on file.: pod created. run get nodes -o wide : we can see pod assigned to one of the node. here scheduler did was : it checks the request and launch in avaialable nodes which has resources.

- Kubernetes Scheduler decides the ideal node to run the pod depending on the requests and limits.
- If your POD requires 8GB of RAM, however, there are no nodes within your cluster which has 8GB RAM, then your pod will never get scheduled
- Guaranteed : Requests and limits are equal. Burstable : Requests and limits are not equal. Best Effort : No requests and limits have been set.

***********************************************************************************************************************************************************
Network Policies :
- in K8s pods are by default non-isolated , means they accept traffic from any source.
EX : pod 1 can communicate with pod 2.
- pod 1 in namespace Dev can communicate with Pod 3 in namsespace Staging.
- let's say we launch 100 pods. theya all will be non isolated. means all 100 pods can communicate each other. but in prod env we don't want this.
- this is similiar to firewall. we have multiple servers , we can have firewall to block specific traffic that are coming to servers.
- Similiarly we can achieve in K8s with the help of network policies.

Network Policy : It is a specification of how group of pods are allowed to communicate with each other and other network endpoints.
EX : pod1 can only communicate with pod5 in the same namespace. means pod 1 can't communicate with pod2,3,4.
  - Pod 2 can only communicate with pod 10 residing in namespace security. here network policies can even be applied to namespaces.
- No one should be able to communicate with pod3.

Use Case : Pod Compromise.
- there are 3 pods in a cluster. One pod Named AppA is compromised. we know bydefault all the pods can communicate each other.but generally in case of compromise i want to isolate the compromised pod.so that no communication should happen to that pod.

DEMo : create a deployment " kubectl run -it pod1 --image=busybox" now we are inside the pod.
- create pod2 from busybox. y default pods which are in non isolated can accept traffic from anywhere.run ifconfig in pod1. we will get ip of pod. ping this ip from pod2. communication works well.
- asume pod1 is compromised. now i want to isolate this pod. we shouldn;t delet this, if we delete all info related to forensics will be lost.
- i have a file policy.yaml of network policy type :

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy                kind is networkpolicy
metadata:  
  name: default-deny-pod
  namespace: default               namespace is default
spec:
  podSelector:                     i have pod selector and i am matching labels based on roles.
    matchLabels:                  this will apply this policy to all the pods which mach the below specific label
        run:pod1 
  policyTypes:
  - Ingress

- befor to this keep the ping ongoing in pod02 to pod1.
- apply this policy by running apply -f policy.yaml: networkpolicy has been created. now we can see ping has stopped in pod2 for pod1.

- login to pod1. and ping google.com.it works well. means policy stopped all the ingress traffic.
- in the yaml file above if we add '-Egress' at last, then it will stop outgoing traffic from pod. run google.com it wwill fail.

***********************************************************************************************************************************************************

SECURITY :-

***********************************************************************************************************************************************************
CONTAINER SECURITY SCANNING : 
- Docker Containers can have security vulnerabilities.
- If blindly pulled and if containers are running in production,and if that container has some high level vulnarabilitie whiccan be exploited, it can result in a breach. Hence it is imp to scan our containers for security vulnarabilities.
- Docker trusted registry(DTR) allows us to perform scan for the containers which we pushed to dtr. These scan can perform "on push " or manually. 
- lets say someone is pushing a container image to DTR , then dtr automatically scan that image for vulnarability, or someone already pushed we can even scan manually.
- in ppt no 2, we can see image v1 has no vulnarabilities hence it is clean.we also has ubuntu image it has been detected with lot of vulnarabilities. if we want to see the type of vulnarabilities we canm click on view details. in that we can see layer(add file or rm-rf or Mkdir layers) wise vulanrabilities associated with container. click on any layer and it will give desrcription of that vulnarability.
- Also if we wan t to scan every container in DTR console we can select security there we can enable Scanning.
- Here container scanning is a feature of DTR not a prporiatory of Dtr. if we want we can use other solns for scanning vulnarabilities.

***********************************************************************************************************************************************************
Configuring Container Scan with DTR:
- In dtr console go to system and by default in system ,security scanning is of. we can enable security scanning. again now we have 2 options Online(Automatically syncs) or offline(manually upload a file). select online and press Sync Database now. This will synce DAtabase from the online repo.
- i have one repo named admin/webserver. now push one image to this repo. " run docker pull ubuntu:16.04 " this is very old image. typically in old images there will be a lot of vulnarabilities. now tag this to our repo " docker tag ubuntu:16.04 example.com/admin/webserver:myubuntu " myubuntu is name of image and admin/webserver is repo name. now push that image to DTR " docker push example.com/admin/webserver:myubuntu "
- now in dtr go to repo.and click on tags.we can see myubuntu under image.we can also see start scan under vulnarabilities. click on start scan. after some time of loading we can see " 5 critical 31 major 18 minor " now click on details.we can see layer wise vulnarabilities. click on Add layer vulnarabilies : we can see list of vulnarabilitie sin that layer.  
- this whole process is scanning manually.
- we can even enable scanning by "on-push" enabling this in repo section under image scanning. HEre dtr automatically scan the pushed image.
- now pull latest busybox image and attatch any tag to it and push to the repo"docker push example.com/admin/webserver:mybusybox". in DTR console in repo console select tags. we can see Vulnarabilities are in pending for busybox image. here we never had to scan now option. after soime time under vulnarabilies it will show clean means no vulnarabilities associated with this image.

***********************************************************************************************************************************************************
DTR WEBHOOKS :
- You can configure DTR to automatically post-event notifications to a webhook URL of your choosing
EX(ppt.No3) : we have a DTR and we have a build server like jenkins. So now once we push the image to DTR , may be once our security scan is completed and at a later stage i also want to perform lot of things like unit test on this specific image(ubuntu).
- Dtr can send a notification to webhook url of jenkins.So once jenkins recieves the notifi it can pull image that was security scanned by DTR and then it can go ahead and do various things like unit testing and others. this is why webhook used.
- in page3 of ppt : we can see sample : in that sample once jenkins realises that a specific image named foo/bar: laytest has been security scanned, the jenkins can pull this image for further processing.
DEMo :
- the post evnt notification can be achieved at repo level in dtr. currently we have admin repo named.inside that we have tags.also we have webhooks option click on that webhook. in that webhook section we can see 'notification to recieve: Security scan comltd or Scan failed " then dtr can inform jenkins about image data and jenkins will process it further.

***********************************************************************************************************************************************************
LDAP(FEDERATION): 
- previously we spoke about deligation and how a user from one acnt can do a assume role to the 2nd aws acnt.this is about deligation 
- now we look into similiar concept FEDERATION :
Let’s assume there are 500 users within an organization. Your organization are using 3 services:-
- AWS ( Infrastructure )
- Jenkins ( CI / CD )
- HR Activator ( Payroll )
- as a solutions architect or systems administrator you have been assigned a role to give users access to all 3 services. means we need to give 3 services access to 500 users.
- simple way of doing is ,we add 500 users first in jenkins, then 500 users in AWS IAM, 500 users in Hr activator. tmrw again 10 more users joined then we have to add that 10 users in all 3 services.this is problamatic. instead of this we can have a central directory :

- LDAP(LIGHT WEIGHT DIRECTORY ACCESS CONTROL ). in LDAP all the users will be stored.So administraot has to store all the users in ldap.Once users stored in ldap depending upon the federated settings that we apllied , the user will be able to login to jenkins,aws, hr management system directly. So no need to create user in 3 all serviuces. just create a user in ldap and establish a trust relationship b/w ldap and all 3 services and then user will be able to connect to these 3 services seamlesly. so tmrw 10 users come to our org. all we need to is to add them in LDAP directory. this is very efficient way to do.
let's say tmrw any person leaves our org.our org is following pcidss standards , then we need to immediatly revoke the acces of that person. So again going to evry service and disabling the acnt needs lot of effort.Instead of that if we are using LDAp, all we have to do is to disable User in LDAp.

- There are various solutions of LDAP are  available which can store users centrally:-
* Microsoft Active Directory
* RedHat Identity Management / freeIPA

DEMO of free IPA: in ipa console we can see a list of active users.
- we can create a user group named prod.once group creates we can add partoicular users to this group to access prod env. now in the aws side we can create a role which does the fedration and we can establish a trust relationship. like only users present in prod group can login to prod role which is present in aws acnt.

***********************************************************************************************************************************************************
Linux NameSpaces :
- Docker uses a technology called namespaces to provide the isolated workspace called the container
- These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.
USE case : i have 2 namespaces(A,B). each namespace has diff components(seperate PID ,UTS components) which are available.
DEMo : create a container mybusybox from busybox image. Now from host run "ps -ef " o/p :we can see that there are lot of processes which are running.
 the first process is systemd which runs as pid1.
- now connect to contaienr and run "ps -ef " o/p : it will show only 3 processes.
o/p :
PID  user Command 
1    root   sh    
6    root   sh 
11   root   ps -ef 

- pid1 has sh command running. now howcome the contaienr is not able to see the processes which are running in the host and that is what preciesely what namespace does. EX : when we run ps -ef inside container we can see sh command has pid1. if we run the same command outside container we can see sh command has pid8329 number among all the process running there it can't have a pid1 in host. 

- So all of these isolations happens behind the scenes is the feature of Linux namespaces. there are various components to the linux namespaces. the components that we were discussing is PID. Linux namespace has various components like UTS,IPC etc. we understood that pid allows us to have a seperation in terms of process so that the container which is running it can't really see other processes which is running withing the same OS(host)

- so by any chance the contaienr is hacked(breacjed), so the attacker will not be able to know the wider poicture on what exactly running outside container.
- run " unshare -fp --mount-proc /bin/bash ". so again in order to have a namespace we don't really need docker.Docker makes use of nsamespace which is available in linux env. above '-fp' p is pid component of the namespace which linux provides.
- run 'ps -ef' o/p: 2 processes : 1.ps -ef 2. /bin/bash . this is also kind of namepsace env.this is linux feature.

EX : run hostnane in cli.o/p: docker-demo. we also have namespace of uts which allows us to set hostname.
- run " unshare --uts /bin/bash " . run hostname . as expected within this namepsace we have docker-demo hostname. however we can change hostname here.
 run " hostname mynamespace ". run hostname .o/p : mynamespace. So whatever hostanem that we have added inside a specific namespace is associated with only that namespace.it is not associated with underlying host. now from host run hostname we will get docker-demo remains same. same goes when we connect to container and run hostname we will get diff hostname'egeye". means container has diff hostname.comeout of contaienr and run hstname we will get docker-demo.

- if we are running 10 containers, 10 containers will have diff hostnames in terms of process,network,
Currently, Linux provides six different types of namespaces as follows:
- Inter-Process Communication (IPC)
- Process (pid)
- Network (net)
- User Id (user)
- Mount (mnt)
- UTS

***********************************************************************************************************************************************************
CONTROL GROUPS(cgroups) : 
- Control Groups (cgroups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes which are running in linux system.

(page.no6) : we have docker host.inside it 2 contaiers(containerA,B) are running. Now this specific docker host have 1 core CPu and 1 GB ram. both containers are green means they are running fine. Now assume Container A suddenly started to take huge amnt of resources(CPU and RAM).It might be due to DDOS or poor implementation of the appl which is running inside the container. Now other containers will not get appropriate amount of cpu and ram. this is dangerous.this is why it is imp to control the amnt of ram that we assign to container so that eventhough the container is attacked it can't really take more amnt of cpu or ram.

- ControlGroups itself is a Linux kernel feature just like linux namespace. Docker makes use of controlGroups to limit or control the amnt of cpu that can be assigned. Control group can control many things(cpu,network,disk i/o etc). However docker doesn't support each and every implementaion of what control group supports. Primary oine sthat support are cpu and ram.

DEMO : run docker ps. no containers . also run dokcer help : in memory we can see '-m'for memory limit.
- run "docker container run -dt --name mymemory -m 256m busybox sh" i alloctaed 256mb memory.
- in host run " free -h " to check how much memory that docker host has.it has 991m. now connect to container and run "free -m" we will see container also has 991m.eventhough it shows but it is not true. the containe rhas max limit is 256mb. because these are designed before cgroups are introduced.
- if we want to see exact memory run " cat /sys/fs/cgroup/memory/memory.limit_in_bytes " it will say 261m in bytes.

***********************************************************************************************************************************************************
Limiting CPUs for Containers : see ppt.page.no 7 for table.
- docker container run -dt --name constraint01 --cpus=1.5 busybox sh
- docker container run -dt --name constraint02 --cpuset-cpus=0,1 busybox sh

***********************************************************************************************************************************************************
 
STORAGE AND VOLUMES : 

***********************************************************************************************************************************************************
Docker Storage layers : 
-A Docker image is built up from a series of layers.when we create a container from image(ex:ubuntu). when we login to this container and run ls we can see multiple files which are spanned across layers are merged tgether and are shown to us as single o/p. There has to be something which would do all the merging.Those are storage drivers.

- Storage Driver makes all these things together for you .in docker There are multiple storage drivers available with different trade-offs.
- It is also important for us to understand the copy-on-write (CoW) strategy. even linux also use this cow strategy.
- Copy on write" means: everyone has a single shared copy of the same data until it's written, and then a copy is made.
EX : we have a common file and we have 2 programmes(container layer) which need to access the common file(lower layer).in this case both programmes will make use of common file. again we have programme 3(contaienr layer) which need to access the same common file. in these case we have resources which will be shared and disk space will be same.

Q. what happens when a written is happen to that common file.in this case there is a diff strategy.
- when a write operation happens within the cow strategy.in this case the common file will copied to the layer where the write is happening.lets assume write is happening on layer 3 of docker image.like it is making some changes to common file. then the common file whre the change is happening it will be copied to layer 3 and then whatever write which was supposed to happen that would happen at layer 3.
- remember once a higher layer makes changes to this common file 

-Also assume we have contrainer image. there is a file at specific layer 3.if we made any change to this file then this file will be copied to container layer completely. image the file size is 500mb.( we made change to this file) now this file will be copied to our container layer completely.means entire 500mb gets copoied to container layer.and change would be happens at this container layer.Now whenever a read oprn happens Docker storage driver will ensure that if file exists in container layer it will not read the file in below layer.it will read from container layer which is read/write layer.

DEMO : 
run docker info. we can see Storagedriver:overlay2, BAckingfilesystem=xfs.
- also run cd /var/lib/docker"  run ls -l . we can see one driver named 'overlay2'.it is a name of storage driver. 
- run docker images: no images. so now cd overlay2. we can see 2 files. l and other file
- run docker pull ubuntu in l directory. it will pull image.run images we can see 1 image. now gpo out of l dir. and run ls -l we can see multiple directories which have been created.these directories are associated with layers. layer name and directories names are different.
- ubuntu has multiple layters.and each layer can have multiple directories. Usually lowest layer will have the lowest size.
- there will be one layer which will be lowest one.and above to it multiple higher layers.the higher layer will have some kind of change on the lower layer.like i added a file in layer 2. so this 2nd layer need to identify on what is the lower layer layer which is sitting behind it.

***********************************************************************************************************************************************************
BLOCK VS OBJECT STORAGE : 
- In block storage, the data is stored in terms of blocks
- we have a hdd drive.interms of block storage whenever we install a filesystem like ntfs or efs , then the file system will divide the storage interms of blocks. here blocks will be created and storage will be stored in terms of blocks.

- Data stored in blocks are normally read or written entirely a whole block at the same time
- Most of the file systems are based on block devices( for windows ntfs or fat,for linux ext,xfs.) all these filessyetsm are block storages.
- Every block has an address and the application can be called via SCSI call via its block address
- There is no storage side meta-data associated with the block except the address.
- lets assume there is a image file is stored. there is no metadata.just by looking at block i cant determine what is the file being stored over here.

Object storage :
- Object storage is a data storage architecture that manages data as objects as opposed to blocks of storage. here no blocks of data which is been divided.even if we have 10mb of data it will be stored as object.
- An object is defined as a data (file) along with all its meta-data which is combined together as an object. aws s3 is object storage.if we clock on any bucet in aws and click on contet type . we can see type of metadata for that onject.this meta data tells all about what the object(mp3,png,txt) us all about.

- This object is given an ID which is calculated from the content of the object (from the data and
metadata ). The application can then call the object with the unique object ID.
- refer ppt.page.no:4 for diff b/w object and block storage.








